{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f40938bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import random\n",
    "import math\n",
    "import tensorflow as tf\n",
    "from skmultiflow.drift_detection import PageHinkley, ADWIN\n",
    "from skmultiflow.data import DataStream\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense , Dropout , Lambda, Flatten\n",
    "from tensorflow.keras.optimizers import Adam ,RMSprop\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import  backend as K\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from scipy.stats import spearmanr\n",
    "from scipy.stats import entropy\n",
    "from tensorflow import keras\n",
    "from sklearn import preprocessing\n",
    "import tensorflow_privacy\n",
    "from tensorflow_privacy.privacy.analysis import compute_dp_sgd_privacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f30db0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_initial_model(inp, out): #get initial model\n",
    "    model = tf.keras.Sequential([\n",
    "        Dense(10, input_dim=inp, kernel_initializer='normal', activation='relu'),\n",
    "        Dense(out, activation='softmax')\n",
    "        ])\n",
    "    #optimizer = keras.optimizers.Adam(learning_rate=0.0001)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f2f9ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_initial_model_2(inp, out):\n",
    "    model = tf.keras.Sequential([\n",
    "        Dense(10, input_dim=inp, kernel_initializer='normal', activation='relu'),\n",
    "        Dense(20, activation='relu'),\n",
    "        Dense(10, activation='relu'),\n",
    "        Dense(out, activation='softmax')\n",
    "        ])\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12b8a088",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DP_get_initial_model(inp, out): #get initial model\n",
    "    model = tf.keras.Sequential([\n",
    "        Dense(10, input_dim=inp, kernel_initializer='normal', activation='relu'),\n",
    "        Dense(out, activation='softmax')\n",
    "        ])\n",
    "    #optimizer = keras.optimizers.Adam(learning_rate=0.0001)\n",
    "    #model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7dcea984",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DP_get_initial_model_2(inp, out): #get initial model\n",
    "    model = tf.keras.Sequential([\n",
    "        Dense(10, input_dim=inp, kernel_initializer='normal', activation='relu'),\n",
    "        Dense(20, activation='relu'),\n",
    "        Dense(10, activation='relu'),\n",
    "        Dense(out, activation='softmax')\n",
    "        ])\n",
    "    #optimizer = keras.optimizers.Adam(learning_rate=0.0001)\n",
    "    #model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6dfc529",
   "metadata": {},
   "outputs": [],
   "source": [
    "def node_comparison1(node1, node2, epsilon=0.05): #this function is to see if the nodes are atmost epsilon distance apart\n",
    "  for x, y in zip(node1,node2):\n",
    "    #print(x,y)\n",
    "    if isinstance(x, list):\n",
    "        if((np.linalg.norm(np.array(x)-np.array(y))/len(x))<=epsilon):\n",
    "            continue\n",
    "        else:\n",
    "            return False\n",
    "    else:\n",
    "        if(math.sqrt((x-y)*(x-y))<=epsilon):\n",
    "            continue\n",
    "        else:\n",
    "            return False\n",
    "  return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4359843e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def node_comparison(node1, node2):\n",
    "  for x, y in zip(node1,node2):\n",
    "    #print(x,y)\n",
    "    if isinstance(x, list):\n",
    "        if(sorted(x)==sorted(y)):\n",
    "            continue\n",
    "        else:\n",
    "            return False\n",
    "    else:\n",
    "        if(x==y):\n",
    "            continue\n",
    "        else:\n",
    "            return False\n",
    "  return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6429ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_comparison(layer1, layer2): #compare same layers in two different DNNs\n",
    "  for node1 in layer1:\n",
    "    present=False\n",
    "    for node2 in layer2:\n",
    "      if (node_comparison1(node1, node2)):\n",
    "        present=True\n",
    "    if present==False:\n",
    "      return False\n",
    "  return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "539a1951",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_models(Model_weights1, Model_weights2): # compare two deep neural networks based on their weights\n",
    "  for i in range(0,len(Model_weights1), 2):\n",
    "    layer1=[]\n",
    "    layer2=[]\n",
    "    for j in range(len(Model_weights1[i+1].T)):\n",
    "      Node1=[]\n",
    "      Node2=[]\n",
    "      Node1.append(list(Model_weights1[i].T[j]))\n",
    "      Node1.append(Model_weights1[i+1][j])\n",
    "      if (i+2<len(Model_weights1)):\n",
    "        Node1.append(list(Model_weights1[i+2][j]))\n",
    "      Node2.append(list(Model_weights2[i].T[j]))\n",
    "      Node2.append(Model_weights2[i+1][j])\n",
    "      if (i+2<len(Model_weights2)):\n",
    "        Node2.append(list(Model_weights2[i+2][j]))\n",
    "      layer1.append(Node1)\n",
    "      layer2.append(Node2)\n",
    "    if (layer_comparison(layer1, layer2)):\n",
    "      continue\n",
    "    else:\n",
    "      return False\n",
    "  return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b328da9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function is used to average out all the models in the epsilon range\n",
    "#the problem is different here than compared with model comparison. Shape not important.\n",
    "def get_avg_weights(models_weights, inp_shape, out_shape):\n",
    "    avg_sum=get_initial_model(inp_shape, out_shape).get_weights()\n",
    "    #print(avg_sum)\n",
    "    for i in range(0,len(avg_sum),2):\n",
    "        if (i+2<=len(avg_sum)):\n",
    "            for j in range(len(avg_sum[i])):\n",
    "                for k in range(len(avg_sum[i][j])):\n",
    "                    avg_sum[i][j][k]=0\n",
    "            for j in range(len(avg_sum[i+1])):\n",
    "                avg_sum[i+1][j]=0\n",
    "    #print(avg_sum)\n",
    "    \n",
    "    for i in range(len(models_weights)):\n",
    "        for j in range(0, len(avg_sum),2):\n",
    "            #print(isinstance(avg_sum[j], np.ndarray))\n",
    "            #if(isinstance(avg_sum[j][0], np.ndarray)):\n",
    "            for k in range(len(avg_sum[j])):\n",
    "                avg_sum[j][k]=[avg_sum[j][k][l]+models_weights[i][j][k][l] for l in range(len(avg_sum[j][k]))]\n",
    "            #print(isinstance(avg_sum[j], np.ndarray))\n",
    "            #else: gayab kr diya\n",
    "            print('andr aara h')\n",
    "            for k in range(len(avg_sum[j+1])):\n",
    "                avg_sum[j+1][k]=avg_sum[j+1][k]+models_weights[i][j+1][k]\n",
    "    \n",
    "    mean_size=len(models_weights)\n",
    "    for i in range(0,len(avg_sum),2):\n",
    "        if (i+2<=len(avg_sum)):\n",
    "            for j in range(len(avg_sum[i])):\n",
    "                #print(\"yhn tk\")\n",
    "                avg_sum[i][j]=[avg_sum[i][j][k]/mean_size for k in range(len(avg_sum[i][j]))]\n",
    "            for j in range(len(avg_sum[i+1])):\n",
    "                avg_sum[i+1][j]=avg_sum[i+1][j]/mean_size\n",
    "    print(\"Done\")\n",
    "    return avg_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b47adf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function is used to average out all the models in the epsilon range\n",
    "#the problem is different here than compared with model comparison. Shape not important.\n",
    "def get_avg_weights_2(models_weights, inp_shape, out_shape):\n",
    "    avg_sum=get_initial_model_2(inp_shape, out_shape).get_weights()\n",
    "    #print(avg_sum)\n",
    "    for i in range(0,len(avg_sum),2):\n",
    "        if (i+2<=len(avg_sum)):\n",
    "            for j in range(len(avg_sum[i])):\n",
    "                for k in range(len(avg_sum[i][j])):\n",
    "                    avg_sum[i][j][k]=0\n",
    "            for j in range(len(avg_sum[i+1])):\n",
    "                avg_sum[i+1][j]=0\n",
    "    #print(avg_sum)\n",
    "    for i in range(len(models_weights)):\n",
    "        for j in range(0, len(avg_sum),2):\n",
    "            #print(isinstance(avg_sum[j], np.ndarray))\n",
    "            #if(isinstance(avg_sum[j][0], np.ndarray)):\n",
    "            for k in range(len(avg_sum[j])):\n",
    "                avg_sum[j][k]=[avg_sum[j][k][l]+models_weights[i][j][k][l] for l in range(len(avg_sum[j][k]))]\n",
    "            #print(isinstance(avg_sum[j], np.ndarray))\n",
    "            \n",
    "            for k in range(len(avg_sum[j+1])):\n",
    "                avg_sum[j+1][k]=avg_sum[j+1][k]+models_weights[i][j+1][k]\n",
    "    print(\"yhn tk\")\n",
    "    mean_size=len(models_weights)\n",
    "    print(mean_size)\n",
    "    for i in range(0,len(avg_sum),2):\n",
    "        if (i+2<=len(avg_sum)):\n",
    "            for j in range(len(avg_sum[i])):\n",
    "                #print(\"yhn tk\")\n",
    "                avg_sum[i][j]=[avg_sum[i][j][k]/mean_size for k in range(len(avg_sum[i][j]))]\n",
    "            for j in range(len(avg_sum[i+1])):\n",
    "                avg_sum[i+1][j]=avg_sum[i+1][j]/mean_size\n",
    "    print(\"Done\")\n",
    "    return avg_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42178cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom functions for f1, precision and recall\n",
    "\n",
    "from keras import backend as K\n",
    "from sklearn import preprocessing\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29e440f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to check if the two samples are different:\n",
    "def return_samples(Positive, Negative, data, N):\n",
    "    datasets=[]\n",
    "    positiveN=int((Positive.shape[0]/data.shape[0])*N)\n",
    "    negativeN=int(N-positiveN)\n",
    "    print(negativeN)\n",
    "    while (Positive.empty==False and Negative.empty==False):\n",
    "        df1=Positive.sample(min(positiveN, len(Positive)))\n",
    "        Positive.drop(df1.index, inplace=True)\n",
    "        \"\"\"\n",
    "        drop_df1=np.random.choice(df1.index,(int)(min(positiveN, math.ceil(len(Positive))/2)), replace=False)\n",
    "        if len(Positive)<positiveN:\n",
    "            Positive.drop(df1.index, inplace=True)\n",
    "        else:\n",
    "            Positive.drop(drop_df1, inplace=True)\n",
    "        #print(len(Positive))\n",
    "        \"\"\"\n",
    "        df2=Negative.sample(min(negativeN, len(Negative)))\n",
    "        Negative.drop(df2.index, inplace=True)\n",
    "        \"\"\"\n",
    "        drop_df2 = np.random.choice(df2.index,(int)(min(negativeN, math.ceil(len(Negative))/2)))\n",
    "        if len(Negative)<negativeN:\n",
    "            Negative.drop(df2.index, inplace = True)\n",
    "        else:\n",
    "            Negative.drop(drop_df2, inplace=True)\n",
    "        \"\"\"\n",
    "        dataset=df1.append(df2, ignore_index=True)\n",
    "        dataset.sample(frac = 1)\n",
    "        dataset.sample(frac = 1)\n",
    "        dataset.sample(frac = 1)\n",
    "        datasets.append(dataset)\n",
    "    print(\"returned datasets\")\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fbc74a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate sampele which are plausible deniable\n",
    "#there exists a data lets say data\n",
    "def generate_samples(data, num_samples, N):\n",
    "    samples=[]\n",
    "    intersection=[]\n",
    "    while num_samples>0:    \n",
    "        Positive = data[data[target_variable]==0]\n",
    "        Negative = data[data[target_variable]==1]\n",
    "        positiveN=int((Positive.shape[0]/data.shape[0])*N)\n",
    "        negativeN=int(N-positiveN)\n",
    "        df1=Positive.sample(min(positiveN, len(Positive)))\n",
    "        #Positive.drop(df1.index, inplace=True)\n",
    "        df2=Negative.sample(min(negativeN, len(Negative)))\n",
    "        #Negative.drop(df2.index, inplace=True)\n",
    "        sample=df1.append(df2, ignore_index=False)\n",
    "        samples.append(sample)\n",
    "        num_samples-=1\n",
    "\n",
    "    intersection=list(set(samples[0].index).intersection(samples[1].index))\n",
    "    for i in range(2,len(samples)):\n",
    "        intersection=list(set(samples[i].index).intersection(intersection))\n",
    "    if intersection:\n",
    "        rnum = random.randint(0, len(samples)-1)\n",
    "        samples[rnum].drop(intersection)\n",
    "        print(\"some intersection\")\n",
    "    print(\"finished\")\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba5025d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate sampele which are plausible deniable multiclass\n",
    "#there exists a data lets say data\n",
    "def generate_samples_multi(data, num_samples, N):\n",
    "    samples=[]\n",
    "    intersection=[]\n",
    "    labels = [0,1,2,3,4,5,6,7,8,9]\n",
    "    len_clas=[]\n",
    "    data_cls=[]\n",
    "    target_variable='out'\n",
    "    print((data[data[target_variable]==labels[9]]))\n",
    "    \n",
    "    for i in range(len(labels)):\n",
    "        data_cls.append(data[data[target_variable]==labels[i]])\n",
    "        len_clas.append(min(int((data_cls[i].shape[0]/5)), 1))\n",
    "    print(len(data_cls))\n",
    "    while num_samples>0:  \n",
    "        for i in range(len(labels)):\n",
    "            if i==0:\n",
    "                df=data_cls[i].sample(min(len(data_cls[i]), len_clas[i]))\n",
    "            else:\n",
    "                df1=data_cls[i].sample(min(len(data_cls[i]), len_clas[i]))\n",
    "                df=df.append(df1, ignore_index = True)\n",
    "        df=df.sample(frac=1)\n",
    "        df=df.sample(frac=1)\n",
    "        df=df.sample(frac=1)\n",
    "        samples.append(df)\n",
    "        num_samples-=1\n",
    "    intersection=list(set(samples[0].index).intersection(samples[1].index))\n",
    "    for i in range(2,len(samples)):\n",
    "        intersection=list(set(samples[i].index).intersection(intersection))\n",
    "    if intersection:\n",
    "        rnum = 0 #here intentionally done 0 fas poker has enough number of 0 class\n",
    "        samples[rnum].drop(intersection)\n",
    "        print(\"some intersection\")\n",
    "    print(\"finished\")\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bafc1536",
   "metadata": {},
   "outputs": [],
   "source": [
    "#do we need to find all the models or only the mean recommended models?\n",
    "def epsilon_mean_recommendation(add_weights,data):\n",
    "    mean_model_weights=[]\n",
    "    for i in range(len(add_weights)):\n",
    "        mean_model_weights.append(get_avg_weights(add_weights[i],data.shape[1]-1, 2))\n",
    "    mean_models=[]\n",
    "    mean_model_train_metrics=[]\n",
    "    mean_model_loss=[]\n",
    "    mean_model_acc=[]\n",
    "    mean_model_test_metrics=[]\n",
    "    mean_model_test_loss=[]\n",
    "    mean_model_test_acc=[]\n",
    "    y = to_categorical(data[target_variable])\n",
    "    X = data.drop(columns=target_variable)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "    for i in range(len(add_weights)):\n",
    "        init_model=get_initial_model(X_test.shape[1], 2)\n",
    "        init_model.set_weights(mean_model_weights[i])\n",
    "        mean_model_train_metrics.append(init_model.evaluate(X_train, y_train))\n",
    "        mean_model_loss.append(mean_model_train_metrics[i][0])\n",
    "        mean_model_acc.append(mean_model_train_metrics[i][1])\n",
    "        mean_model_test_metrics.append(init_model.evaluate(X_test, y_test))\n",
    "        mean_model_test_loss.append(mean_model_test_metrics[i][0])\n",
    "        mean_model_test_acc.append(mean_model_test_metrics[i][1])\n",
    "    return mean_model_weights, mean_model_acc, mean_model_loss, mean_model_test_acc, mean_model_test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "96227a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#do we need to find all the models or only the mean recommended models?\n",
    "def epsilon_mean_recommendation_2(add_weights,data):\n",
    "    mean_model_weights=[]\n",
    "    for i in range(len(add_weights)):\n",
    "        mean_model_weights.append(get_avg_weights_2(add_weights[i],data.shape[1]-1, 2))\n",
    "    mean_models=[]\n",
    "    mean_model_train_metrics=[]\n",
    "    mean_model_loss=[]\n",
    "    mean_model_acc=[]\n",
    "    mean_model_test_metrics=[]\n",
    "    mean_model_test_loss=[]\n",
    "    mean_model_test_acc=[]\n",
    "    y = to_categorical(data[target_variable])\n",
    "    X = data.drop(columns=target_variable)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "    for i in range(len(add_weights)):\n",
    "        init_model=get_initial_model_2(X_test.shape[1], 2)\n",
    "        init_model.set_weights(mean_model_weights[i])\n",
    "        mean_model_train_metrics.append(init_model.evaluate(X_train, y_train))\n",
    "        mean_model_loss.append(mean_model_train_metrics[i][0])\n",
    "        mean_model_acc.append(mean_model_train_metrics[i][1])\n",
    "        mean_model_test_metrics.append(init_model.evaluate(X_test, y_test))\n",
    "        mean_model_test_loss.append(mean_model_test_metrics[i][0])\n",
    "        mean_model_test_acc.append(mean_model_test_metrics[i][1])\n",
    "    return mean_model_weights, mean_model_acc, mean_model_loss, mean_model_test_acc, mean_model_test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "039769ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drift_detection(models, incoming_data):\n",
    "    predictions = []\n",
    "    prediction_probe=[]\n",
    "    \n",
    "    \n",
    "    y_test=to_categorical(incoming_data[target_variable])\n",
    "    #print(y_test)\n",
    "    X_test=incoming_data.drop(columns=target_variable)\n",
    "    \n",
    "    init_model=get_initial_model(incoming_data.shape[1]-1, 2)\n",
    "    \n",
    "    for i in range(len(models)):\n",
    "        init_model.set_weights(models[i])\n",
    "        predictions.append(init_model.predict(X_test))\n",
    "    prediction_probe = np.mean(predictions, axis = 0)\n",
    "    entro = entropy(prediction_probe, base=2, axis=1)\n",
    "    \n",
    "    entropy_list = entro.tolist()\n",
    "    \n",
    "    return prediction_probe, entropy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5686cca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drift_detection_2(models, incoming_data):\n",
    "    predictions = []\n",
    "    prediction_probe=[]\n",
    "    \n",
    "    \n",
    "    y_test=to_categorical(incoming_data[target_variable])\n",
    "    #print(y_test)\n",
    "    X_test=incoming_data.drop(columns=target_variable)\n",
    "    \n",
    "    init_model=get_initial_model_2(incoming_data.shape[1]-1, 2)\n",
    "    \n",
    "    for i in range(len(models)):\n",
    "        init_model.set_weights(models[i])\n",
    "        predictions.append(init_model.predict(X_test))\n",
    "    prediction_probe = np.mean(predictions, axis = 0)\n",
    "    entro = entropy(prediction_probe, base=2, axis=1)\n",
    "    \n",
    "    entropy_list = entro.tolist()\n",
    "    \n",
    "    return prediction_probe, entropy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "14302953",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_train_data(train_data, incoming_data):\n",
    "    train_data.drop(index=train_data.index[:len(incoming_data)], inplace=True)\n",
    "    train_data=train_data.append(incoming_data, ignore_index=True)\n",
    "    return train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5f0c6ba1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of           0         1         2         3         4         5         6   \\\n",
      "0        0.0  0.035371  0.655669  0.838999  0.022344  0.077551  0.247887   \n",
      "1        1.0  0.069615  0.515424  0.146879  0.002377  0.417663  0.982085   \n",
      "2        1.0  0.009358  0.468216  0.295368  0.000709  0.108141  0.278343   \n",
      "3        1.0  0.006226  0.267992  0.699770  0.000625  0.716446  0.304672   \n",
      "4        1.0  0.051980  0.336027  0.305084  0.035597  0.331627  0.679500   \n",
      "...      ...       ...       ...       ...       ...       ...       ...   \n",
      "4999995  1.0  0.029482  0.271408  0.071337  0.007655  0.619779  0.975001   \n",
      "4999996  0.0  0.034323  0.533305  0.914125  0.013858  0.414505  0.286360   \n",
      "4999997  0.0  0.028845  0.837689  0.148726  0.023524  0.911611  0.691276   \n",
      "4999998  1.0  0.075342  0.301903  0.338566  0.016093  0.332685  0.088204   \n",
      "4999999  0.0  0.024958  0.661996  0.158108  0.018859  0.423049  0.571100   \n",
      "\n",
      "               7         8         9         10        11        12        13  \\\n",
      "0        0.026936  0.447579  0.034638  0.442559  0.079515  0.054968  0.060739   \n",
      "1        0.164947  0.146485  0.000554  0.550812  0.037421  0.035075  0.071314   \n",
      "2        0.057890  0.643387  0.078304  0.437752  0.012446  0.058177  0.235614   \n",
      "3        0.096485  0.940121  0.130258  0.422328  0.014517  0.062736  0.234824   \n",
      "4        0.051608  0.387962  0.025194  0.486047  0.043828  0.059829  0.107965   \n",
      "...           ...       ...       ...       ...       ...       ...       ...   \n",
      "4999995  0.087505  0.577908  0.043833  0.409393  0.030063  0.104133  0.248476   \n",
      "4999996  0.013793  0.286855  0.000058  0.456274  0.025738  0.033669  0.089266   \n",
      "4999997  0.023287  0.351211  0.030115  0.450576  0.026802  0.044693  0.115347   \n",
      "4999998  0.126275  0.249660  0.090518  0.527455  0.046377  0.058853  0.101617   \n",
      "4999999  0.053161  0.785971  0.070164  0.427907  0.028205  0.069333  0.172888   \n",
      "\n",
      "               14        15        16        17        18  \n",
      "0        0.055381  0.090192  0.063425  0.859364  0.040714  \n",
      "1        0.000000  0.019932  0.012871  0.830512  0.377584  \n",
      "2        0.097858  0.027274  0.099808  0.713377  0.180910  \n",
      "3        0.075022  0.034740  0.109615  0.937547  0.090719  \n",
      "4        0.000000  0.049980  0.002497  0.725566  0.094859  \n",
      "...           ...       ...       ...       ...       ...  \n",
      "4999995  0.162731  0.048254  0.169271  0.872816  0.364599  \n",
      "4999996  0.000145  0.034160  0.025415  0.278622  0.239953  \n",
      "4999997  0.036398  0.038420  0.039053  0.760586  0.026692  \n",
      "4999998  0.000000  0.035718  0.043023  0.752552  0.093689  \n",
      "4999999  0.072432  0.042804  0.082584  0.967020  0.187496  \n",
      "\n",
      "[5000000 rows x 19 columns]>\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"SUSY.csv\",sep=',', header=None)\n",
    "dataset.round(3)\n",
    "#print(dataset['label'])\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "dataset = pd.DataFrame(scaler.fit_transform(dataset), columns=dataset.columns)\n",
    "target_variable=0\n",
    "print(dataset.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e7d95eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished\n"
     ]
    }
   ],
   "source": [
    "from skmultiflow.data import DataStream\n",
    "\n",
    "stream = DataStream(dataset)\n",
    "\n",
    "two_percent = int(stream.n_remaining_samples()*0.02)\n",
    "#five_percent = int(stream.n_remaining_samples()*0.05)\n",
    "initial_data = stream.next_sample(int(stream.n_remaining_samples()*0.1))\n",
    "\n",
    "data_init=pd.DataFrame(initial_data[0], columns=dataset.columns[:-1])\n",
    "data_init[19]=initial_data[1]\n",
    "\n",
    "epsilon = 0.01\n",
    "N = int(data_init.shape[0]*0.10)\n",
    "samples = generate_samples(data_init, 50, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6ca22d42",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 1s 849us/step - loss: 0.5520 - accuracy: 0.7265 - val_loss: 0.8368 - val_accuracy: 0.4431\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 1s 771us/step - loss: 0.4831 - accuracy: 0.7819 - val_loss: 0.7622 - val_accuracy: 0.5215\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 1s 751us/step - loss: 0.4633 - accuracy: 0.7947 - val_loss: 0.7991 - val_accuracy: 0.5173\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 1s 770us/step - loss: 0.4501 - accuracy: 0.8015 - val_loss: 0.8537 - val_accuracy: 0.5010\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 1s 796us/step - loss: 0.4403 - accuracy: 0.8074 - val_loss: 0.8987 - val_accuracy: 0.4889\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 1s 767us/step - loss: 0.4333 - accuracy: 0.8116 - val_loss: 0.7425 - val_accuracy: 0.5670\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 1s 725us/step - loss: 0.4271 - accuracy: 0.8145 - val_loss: 0.7995 - val_accuracy: 0.5401\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 1s 744us/step - loss: 0.4228 - accuracy: 0.8160 - val_loss: 0.7549 - val_accuracy: 0.5690\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 1s 726us/step - loss: 0.4186 - accuracy: 0.8192 - val_loss: 0.7500 - val_accuracy: 0.5758\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 1s 764us/step - loss: 0.4159 - accuracy: 0.8206 - val_loss: 0.6867 - val_accuracy: 0.6085\n",
      "0.8206250071525574\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 1s 807us/step - loss: 0.5625 - accuracy: 0.7197 - val_loss: 0.8658 - val_accuracy: 0.4149\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 1s 742us/step - loss: 0.4845 - accuracy: 0.7795 - val_loss: 0.8577 - val_accuracy: 0.4827\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 1s 725us/step - loss: 0.4637 - accuracy: 0.7916 - val_loss: 0.8614 - val_accuracy: 0.4986\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 1s 724us/step - loss: 0.4512 - accuracy: 0.8007 - val_loss: 0.8910 - val_accuracy: 0.4974\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 1s 732us/step - loss: 0.4423 - accuracy: 0.8054 - val_loss: 0.8629 - val_accuracy: 0.5141\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 1s 729us/step - loss: 0.4356 - accuracy: 0.8091 - val_loss: 0.7174 - val_accuracy: 0.5901\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 1s 734us/step - loss: 0.4307 - accuracy: 0.8125 - val_loss: 0.7711 - val_accuracy: 0.5596\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 1s 735us/step - loss: 0.4276 - accuracy: 0.8156 - val_loss: 0.8820 - val_accuracy: 0.5223\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 1s 722us/step - loss: 0.4250 - accuracy: 0.8163 - val_loss: 0.7756 - val_accuracy: 0.5682\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 1s 724us/step - loss: 0.4227 - accuracy: 0.8185 - val_loss: 0.7291 - val_accuracy: 0.5949\n",
      "0.8185499906539917\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 1s 797us/step - loss: 0.5648 - accuracy: 0.7177 - val_loss: 0.9327 - val_accuracy: 0.3661\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 1s 726us/step - loss: 0.4893 - accuracy: 0.7783 - val_loss: 0.7952 - val_accuracy: 0.5073\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 1s 731us/step - loss: 0.4643 - accuracy: 0.7931 - val_loss: 0.8669 - val_accuracy: 0.4918\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 1s 728us/step - loss: 0.4491 - accuracy: 0.8020 - val_loss: 0.8248 - val_accuracy: 0.5275\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 1s 732us/step - loss: 0.4389 - accuracy: 0.8079 - val_loss: 0.8540 - val_accuracy: 0.5188\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 1s 734us/step - loss: 0.4321 - accuracy: 0.8109 - val_loss: 0.8298 - val_accuracy: 0.5306\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 1s 740us/step - loss: 0.4278 - accuracy: 0.8133 - val_loss: 0.6849 - val_accuracy: 0.6041\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 1s 723us/step - loss: 0.4242 - accuracy: 0.8156 - val_loss: 0.7181 - val_accuracy: 0.5872\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 1s 721us/step - loss: 0.4219 - accuracy: 0.8172 - val_loss: 0.7340 - val_accuracy: 0.5804\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 1s 732us/step - loss: 0.4189 - accuracy: 0.8176 - val_loss: 0.6682 - val_accuracy: 0.6172\n",
      "0.8175749778747559\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 1s 796us/step - loss: 0.5656 - accuracy: 0.7144 - val_loss: 0.7855 - val_accuracy: 0.4611\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 1s 726us/step - loss: 0.4916 - accuracy: 0.7744 - val_loss: 0.8488 - val_accuracy: 0.4723\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 1s 725us/step - loss: 0.4708 - accuracy: 0.7884 - val_loss: 0.8592 - val_accuracy: 0.4880\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 1s 730us/step - loss: 0.4580 - accuracy: 0.7950 - val_loss: 0.8039 - val_accuracy: 0.5355\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 1s 723us/step - loss: 0.4485 - accuracy: 0.8007 - val_loss: 0.7984 - val_accuracy: 0.5443\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 1s 724us/step - loss: 0.4406 - accuracy: 0.8066 - val_loss: 0.7388 - val_accuracy: 0.5752\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 1s 723us/step - loss: 0.4350 - accuracy: 0.8095 - val_loss: 0.7151 - val_accuracy: 0.5891\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 1s 731us/step - loss: 0.4309 - accuracy: 0.8118 - val_loss: 0.7819 - val_accuracy: 0.5571\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 1s 695us/step - loss: 0.4274 - accuracy: 0.8133 - val_loss: 0.7111 - val_accuracy: 0.5913\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 1s 701us/step - loss: 0.4254 - accuracy: 0.8148 - val_loss: 0.7515 - val_accuracy: 0.5719\n",
      "0.8148499727249146\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 1s 789us/step - loss: 0.5641 - accuracy: 0.7166 - val_loss: 0.9009 - val_accuracy: 0.3901\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 1s 737us/step - loss: 0.4865 - accuracy: 0.7788 - val_loss: 0.8798 - val_accuracy: 0.4656\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 1s 731us/step - loss: 0.4654 - accuracy: 0.7925 - val_loss: 0.8464 - val_accuracy: 0.5016\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 1s 717us/step - loss: 0.4529 - accuracy: 0.8000 - val_loss: 0.8197 - val_accuracy: 0.5261\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 1s 720us/step - loss: 0.4436 - accuracy: 0.8056 - val_loss: 0.7655 - val_accuracy: 0.5566\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 1s 740us/step - loss: 0.4368 - accuracy: 0.8095 - val_loss: 0.8119 - val_accuracy: 0.5377\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 1s 725us/step - loss: 0.4319 - accuracy: 0.8119 - val_loss: 0.7983 - val_accuracy: 0.5481\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 1s 762us/step - loss: 0.4285 - accuracy: 0.8133 - val_loss: 0.8061 - val_accuracy: 0.5437\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 1s 727us/step - loss: 0.4257 - accuracy: 0.8158 - val_loss: 0.7733 - val_accuracy: 0.5662\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 1s 726us/step - loss: 0.4238 - accuracy: 0.8170 - val_loss: 0.7524 - val_accuracy: 0.5762\n",
      "0.8169999718666077\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 1s 796us/step - loss: 0.5615 - accuracy: 0.7192 - val_loss: 0.8245 - val_accuracy: 0.4451\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 1s 732us/step - loss: 0.4854 - accuracy: 0.7785 - val_loss: 0.7575 - val_accuracy: 0.5398\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 1s 730us/step - loss: 0.4632 - accuracy: 0.7922 - val_loss: 0.8140 - val_accuracy: 0.5245\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 1s 726us/step - loss: 0.4503 - accuracy: 0.7993 - val_loss: 0.7413 - val_accuracy: 0.5665\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 1s 724us/step - loss: 0.4404 - accuracy: 0.8056 - val_loss: 0.7927 - val_accuracy: 0.5507\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 1s 716us/step - loss: 0.4327 - accuracy: 0.8109 - val_loss: 0.8371 - val_accuracy: 0.5351\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 1s 737us/step - loss: 0.4273 - accuracy: 0.8124 - val_loss: 0.7774 - val_accuracy: 0.5583\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 1s 731us/step - loss: 0.4234 - accuracy: 0.8164 - val_loss: 0.6617 - val_accuracy: 0.6173\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 1s 727us/step - loss: 0.4206 - accuracy: 0.8167 - val_loss: 0.7302 - val_accuracy: 0.5874\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 1s 722us/step - loss: 0.4185 - accuracy: 0.8187 - val_loss: 0.7358 - val_accuracy: 0.5900\n",
      "0.8187249898910522\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 1s 796us/step - loss: 0.5593 - accuracy: 0.7209 - val_loss: 0.9426 - val_accuracy: 0.3763\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 1s 740us/step - loss: 0.4841 - accuracy: 0.7799 - val_loss: 0.7904 - val_accuracy: 0.5074\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 1s 720us/step - loss: 0.4639 - accuracy: 0.7925 - val_loss: 0.8024 - val_accuracy: 0.5205\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 1s 721us/step - loss: 0.4513 - accuracy: 0.8003 - val_loss: 0.8305 - val_accuracy: 0.5156\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 1s 720us/step - loss: 0.4413 - accuracy: 0.8058 - val_loss: 0.8296 - val_accuracy: 0.5200\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 1s 720us/step - loss: 0.4338 - accuracy: 0.8106 - val_loss: 0.7923 - val_accuracy: 0.5405\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 1s 739us/step - loss: 0.4281 - accuracy: 0.8137 - val_loss: 0.6718 - val_accuracy: 0.6067\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 1s 728us/step - loss: 0.4238 - accuracy: 0.8167 - val_loss: 0.7580 - val_accuracy: 0.5675\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 1s 728us/step - loss: 0.4202 - accuracy: 0.8176 - val_loss: 0.7778 - val_accuracy: 0.5586\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 1s 730us/step - loss: 0.4186 - accuracy: 0.8189 - val_loss: 0.7886 - val_accuracy: 0.5582\n",
      "0.8188999891281128\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 1s 788us/step - loss: 0.5713 - accuracy: 0.7118 - val_loss: 0.9347 - val_accuracy: 0.3671\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 1s 739us/step - loss: 0.4822 - accuracy: 0.7819 - val_loss: 0.6865 - val_accuracy: 0.5688\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 1s 731us/step - loss: 0.4594 - accuracy: 0.7948 - val_loss: 0.8222 - val_accuracy: 0.5114\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 1s 727us/step - loss: 0.4462 - accuracy: 0.8016 - val_loss: 0.7760 - val_accuracy: 0.5472\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 1s 724us/step - loss: 0.4373 - accuracy: 0.8077 - val_loss: 0.8762 - val_accuracy: 0.5095\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 1s 727us/step - loss: 0.4309 - accuracy: 0.8099 - val_loss: 0.7383 - val_accuracy: 0.5722\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 1s 728us/step - loss: 0.4262 - accuracy: 0.8142 - val_loss: 0.7616 - val_accuracy: 0.5689\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 1s 735us/step - loss: 0.4222 - accuracy: 0.8174 - val_loss: 0.7536 - val_accuracy: 0.5744\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 1s 748us/step - loss: 0.4191 - accuracy: 0.8174 - val_loss: 0.6944 - val_accuracy: 0.6044\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 1s 697us/step - loss: 0.4158 - accuracy: 0.8207 - val_loss: 0.8771 - val_accuracy: 0.5242\n",
      "0.8206999897956848\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 1s 792us/step - loss: 0.5632 - accuracy: 0.7193 - val_loss: 0.9013 - val_accuracy: 0.3972\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 1s 721us/step - loss: 0.4828 - accuracy: 0.7795 - val_loss: 0.9291 - val_accuracy: 0.4465\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 1s 734us/step - loss: 0.4629 - accuracy: 0.7916 - val_loss: 0.9251 - val_accuracy: 0.4699\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 1s 739us/step - loss: 0.4503 - accuracy: 0.7991 - val_loss: 0.6788 - val_accuracy: 0.5996\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 1s 712us/step - loss: 0.4415 - accuracy: 0.8037 - val_loss: 0.7290 - val_accuracy: 0.5759\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 1s 724us/step - loss: 0.4350 - accuracy: 0.8089 - val_loss: 0.6991 - val_accuracy: 0.5904\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 1s 730us/step - loss: 0.4299 - accuracy: 0.8115 - val_loss: 0.7580 - val_accuracy: 0.5597\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 1s 720us/step - loss: 0.4263 - accuracy: 0.8133 - val_loss: 0.7701 - val_accuracy: 0.5620\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 1s 728us/step - loss: 0.4236 - accuracy: 0.8139 - val_loss: 0.7925 - val_accuracy: 0.5557\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 1s 729us/step - loss: 0.4216 - accuracy: 0.8159 - val_loss: 0.8175 - val_accuracy: 0.5491\n",
      "0.815850019454956\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 1s 793us/step - loss: 0.5583 - accuracy: 0.7265 - val_loss: 0.9306 - val_accuracy: 0.3744\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 1s 730us/step - loss: 0.4861 - accuracy: 0.7800 - val_loss: 0.8039 - val_accuracy: 0.4916\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 1s 731us/step - loss: 0.4661 - accuracy: 0.7927 - val_loss: 0.9236 - val_accuracy: 0.4440\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 1s 735us/step - loss: 0.4533 - accuracy: 0.8002 - val_loss: 0.8369 - val_accuracy: 0.4927\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 1s 725us/step - loss: 0.4432 - accuracy: 0.8054 - val_loss: 0.7900 - val_accuracy: 0.5225\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 1s 722us/step - loss: 0.4359 - accuracy: 0.8098 - val_loss: 0.6907 - val_accuracy: 0.5805\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 1s 737us/step - loss: 0.4306 - accuracy: 0.8118 - val_loss: 0.7374 - val_accuracy: 0.5588\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 1s 715us/step - loss: 0.4268 - accuracy: 0.8137 - val_loss: 0.7654 - val_accuracy: 0.5522\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 1s 734us/step - loss: 0.4234 - accuracy: 0.8151 - val_loss: 0.6660 - val_accuracy: 0.6099\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 1s 744us/step - loss: 0.4211 - accuracy: 0.8174 - val_loss: 0.7154 - val_accuracy: 0.5883\n",
      "0.8173999786376953\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 1s 796us/step - loss: 0.5624 - accuracy: 0.7175 - val_loss: 0.8894 - val_accuracy: 0.4046\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 1s 727us/step - loss: 0.4846 - accuracy: 0.7796 - val_loss: 0.7178 - val_accuracy: 0.5595\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 1s 730us/step - loss: 0.4647 - accuracy: 0.7909 - val_loss: 0.8366 - val_accuracy: 0.5090\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 1s 735us/step - loss: 0.4526 - accuracy: 0.7982 - val_loss: 0.9020 - val_accuracy: 0.4959\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 1s 723us/step - loss: 0.4432 - accuracy: 0.8052 - val_loss: 0.7631 - val_accuracy: 0.5559\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 1s 732us/step - loss: 0.4363 - accuracy: 0.8097 - val_loss: 0.8072 - val_accuracy: 0.5393\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 1s 734us/step - loss: 0.4316 - accuracy: 0.8135 - val_loss: 0.7677 - val_accuracy: 0.5645\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 1s 736us/step - loss: 0.4276 - accuracy: 0.8145 - val_loss: 0.7984 - val_accuracy: 0.5525\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 1s 740us/step - loss: 0.4247 - accuracy: 0.8163 - val_loss: 0.6357 - val_accuracy: 0.6321\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 1s 737us/step - loss: 0.4227 - accuracy: 0.8181 - val_loss: 0.7834 - val_accuracy: 0.5641\n",
      "0.8181250095367432\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250/1250 [==============================] - 1s 790us/step - loss: 0.5680 - accuracy: 0.7140 - val_loss: 0.8588 - val_accuracy: 0.4056\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 1s 749us/step - loss: 0.4943 - accuracy: 0.7725 - val_loss: 0.8305 - val_accuracy: 0.4846\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 1s 732us/step - loss: 0.4739 - accuracy: 0.7837 - val_loss: 0.8763 - val_accuracy: 0.4811\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 1s 735us/step - loss: 0.4606 - accuracy: 0.7929 - val_loss: 0.8608 - val_accuracy: 0.5060\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 1s 726us/step - loss: 0.4502 - accuracy: 0.7991 - val_loss: 0.8933 - val_accuracy: 0.4992\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 1s 731us/step - loss: 0.4422 - accuracy: 0.8044 - val_loss: 0.7523 - val_accuracy: 0.5696\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 1s 729us/step - loss: 0.4350 - accuracy: 0.8083 - val_loss: 0.7499 - val_accuracy: 0.5730\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 1s 724us/step - loss: 0.4281 - accuracy: 0.8130 - val_loss: 0.7998 - val_accuracy: 0.5523\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 1s 732us/step - loss: 0.4234 - accuracy: 0.8157 - val_loss: 0.6929 - val_accuracy: 0.6074\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 1s 743us/step - loss: 0.4202 - accuracy: 0.8173 - val_loss: 0.7768 - val_accuracy: 0.5641\n",
      "0.817300021648407\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 1s 792us/step - loss: 0.5515 - accuracy: 0.7263 - val_loss: 0.8307 - val_accuracy: 0.4476\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 1s 696us/step - loss: 0.4794 - accuracy: 0.7814 - val_loss: 0.7867 - val_accuracy: 0.5129\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 1s 719us/step - loss: 0.4584 - accuracy: 0.7939 - val_loss: 0.7891 - val_accuracy: 0.5294\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 1s 718us/step - loss: 0.4451 - accuracy: 0.8010 - val_loss: 0.7763 - val_accuracy: 0.5477\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 1s 743us/step - loss: 0.4351 - accuracy: 0.8073 - val_loss: 0.7829 - val_accuracy: 0.5515\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 1s 720us/step - loss: 0.4286 - accuracy: 0.8110 - val_loss: 0.8077 - val_accuracy: 0.5441\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 1s 716us/step - loss: 0.4241 - accuracy: 0.8141 - val_loss: 0.6935 - val_accuracy: 0.5991\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 1s 734us/step - loss: 0.4204 - accuracy: 0.8165 - val_loss: 0.7520 - val_accuracy: 0.5783\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 1s 728us/step - loss: 0.4179 - accuracy: 0.8178 - val_loss: 0.7159 - val_accuracy: 0.5984\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 1s 727us/step - loss: 0.4154 - accuracy: 0.8201 - val_loss: 0.7250 - val_accuracy: 0.5968\n",
      "0.8200749754905701\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 1s 788us/step - loss: 0.5636 - accuracy: 0.7200 - val_loss: 0.8938 - val_accuracy: 0.3848\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 1s 735us/step - loss: 0.4868 - accuracy: 0.7789 - val_loss: 0.8736 - val_accuracy: 0.4615\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 1s 731us/step - loss: 0.4641 - accuracy: 0.7925 - val_loss: 0.7611 - val_accuracy: 0.5407\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 1s 738us/step - loss: 0.4507 - accuracy: 0.8015 - val_loss: 0.7876 - val_accuracy: 0.5393\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 1s 746us/step - loss: 0.4409 - accuracy: 0.8069 - val_loss: 0.7756 - val_accuracy: 0.5462\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 1s 730us/step - loss: 0.4333 - accuracy: 0.8109 - val_loss: 0.7854 - val_accuracy: 0.5481\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 1s 720us/step - loss: 0.4283 - accuracy: 0.8146 - val_loss: 0.7752 - val_accuracy: 0.5550\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 1s 732us/step - loss: 0.4239 - accuracy: 0.8169 - val_loss: 0.8478 - val_accuracy: 0.5255\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 1s 738us/step - loss: 0.4209 - accuracy: 0.8178 - val_loss: 0.8245 - val_accuracy: 0.5384\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 1s 741us/step - loss: 0.4184 - accuracy: 0.8195 - val_loss: 0.9241 - val_accuracy: 0.5012\n",
      "0.8194500207901001\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 1s 803us/step - loss: 0.5527 - accuracy: 0.7277 - val_loss: 0.8366 - val_accuracy: 0.4464\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 1s 730us/step - loss: 0.4783 - accuracy: 0.7859 - val_loss: 0.8754 - val_accuracy: 0.4709\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 1s 740us/step - loss: 0.4578 - accuracy: 0.7979 - val_loss: 0.8670 - val_accuracy: 0.4948\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 1s 728us/step - loss: 0.4455 - accuracy: 0.8051 - val_loss: 0.7948 - val_accuracy: 0.5409\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 1s 740us/step - loss: 0.4363 - accuracy: 0.8084 - val_loss: 0.8378 - val_accuracy: 0.5243\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 1s 738us/step - loss: 0.4306 - accuracy: 0.8133 - val_loss: 0.6880 - val_accuracy: 0.6017\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 1s 736us/step - loss: 0.4266 - accuracy: 0.8140 - val_loss: 0.7076 - val_accuracy: 0.5951\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 1s 733us/step - loss: 0.4233 - accuracy: 0.8156 - val_loss: 0.8848 - val_accuracy: 0.5272\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 1s 735us/step - loss: 0.4211 - accuracy: 0.8173 - val_loss: 0.7510 - val_accuracy: 0.5853\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 1s 743us/step - loss: 0.4192 - accuracy: 0.8184 - val_loss: 0.7464 - val_accuracy: 0.5885\n",
      "0.8184000253677368\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 1s 808us/step - loss: 0.5636 - accuracy: 0.7178 - val_loss: 0.9414 - val_accuracy: 0.3676\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 1s 811us/step - loss: 0.4847 - accuracy: 0.7788 - val_loss: 0.8538 - val_accuracy: 0.4785\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 1s 877us/step - loss: 0.4619 - accuracy: 0.7928 - val_loss: 0.8572 - val_accuracy: 0.5002\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 1s 808us/step - loss: 0.4480 - accuracy: 0.8016 - val_loss: 0.8245 - val_accuracy: 0.5286\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 1s 837us/step - loss: 0.4384 - accuracy: 0.8079 - val_loss: 0.8465 - val_accuracy: 0.5236\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 1s 877us/step - loss: 0.4324 - accuracy: 0.8116 - val_loss: 0.6816 - val_accuracy: 0.6095\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 1s 802us/step - loss: 0.4282 - accuracy: 0.8148 - val_loss: 0.6847 - val_accuracy: 0.6118\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 1s 815us/step - loss: 0.4252 - accuracy: 0.8163 - val_loss: 0.9542 - val_accuracy: 0.4961\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 1s 764us/step - loss: 0.4229 - accuracy: 0.8176 - val_loss: 0.7211 - val_accuracy: 0.5983\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 1s 759us/step - loss: 0.4207 - accuracy: 0.8181 - val_loss: 0.7729 - val_accuracy: 0.5791\n",
      "0.8180500268936157\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 1s 867us/step - loss: 0.5604 - accuracy: 0.7207 - val_loss: 0.8983 - val_accuracy: 0.4049\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 1s 777us/step - loss: 0.4802 - accuracy: 0.7840 - val_loss: 0.7471 - val_accuracy: 0.5509\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 1s 710us/step - loss: 0.4604 - accuracy: 0.7949 - val_loss: 0.8945 - val_accuracy: 0.4921\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 1s 742us/step - loss: 0.4486 - accuracy: 0.8036 - val_loss: 0.7887 - val_accuracy: 0.5432\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 1s 729us/step - loss: 0.4399 - accuracy: 0.8080 - val_loss: 0.7179 - val_accuracy: 0.5854\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 1s 711us/step - loss: 0.4333 - accuracy: 0.8120 - val_loss: 0.7084 - val_accuracy: 0.5959\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 1s 747us/step - loss: 0.4284 - accuracy: 0.8144 - val_loss: 0.7446 - val_accuracy: 0.5759\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 1s 718us/step - loss: 0.4252 - accuracy: 0.8153 - val_loss: 0.7568 - val_accuracy: 0.5744\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 1s 714us/step - loss: 0.4227 - accuracy: 0.8163 - val_loss: 0.8084 - val_accuracy: 0.5525\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 1s 718us/step - loss: 0.4200 - accuracy: 0.8185 - val_loss: 0.8357 - val_accuracy: 0.5458\n",
      "0.8185250163078308\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 1s 780us/step - loss: 0.5631 - accuracy: 0.7195 - val_loss: 0.8740 - val_accuracy: 0.4058\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 1s 742us/step - loss: 0.4877 - accuracy: 0.7768 - val_loss: 0.8226 - val_accuracy: 0.4890\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 1s 726us/step - loss: 0.4673 - accuracy: 0.7885 - val_loss: 0.7972 - val_accuracy: 0.5244\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 1s 732us/step - loss: 0.4552 - accuracy: 0.7957 - val_loss: 0.7705 - val_accuracy: 0.5536\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 1s 715us/step - loss: 0.4454 - accuracy: 0.8021 - val_loss: 0.7602 - val_accuracy: 0.5683\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 1s 734us/step - loss: 0.4380 - accuracy: 0.8071 - val_loss: 0.7886 - val_accuracy: 0.5579\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 1s 733us/step - loss: 0.4318 - accuracy: 0.8103 - val_loss: 0.8011 - val_accuracy: 0.5539\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 1s 715us/step - loss: 0.4279 - accuracy: 0.8135 - val_loss: 0.6750 - val_accuracy: 0.6165\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 1s 730us/step - loss: 0.4246 - accuracy: 0.8147 - val_loss: 0.8666 - val_accuracy: 0.5276\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 1s 720us/step - loss: 0.4229 - accuracy: 0.8152 - val_loss: 0.8422 - val_accuracy: 0.5403\n",
      "0.8151999711990356\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 1s 784us/step - loss: 0.5645 - accuracy: 0.7161 - val_loss: 0.9500 - val_accuracy: 0.3659\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 1s 725us/step - loss: 0.4856 - accuracy: 0.7778 - val_loss: 0.8479 - val_accuracy: 0.4812\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 1s 739us/step - loss: 0.4650 - accuracy: 0.7892 - val_loss: 0.7897 - val_accuracy: 0.5361\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 1s 726us/step - loss: 0.4526 - accuracy: 0.7985 - val_loss: 0.9185 - val_accuracy: 0.4859\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 1s 727us/step - loss: 0.4430 - accuracy: 0.8042 - val_loss: 0.7685 - val_accuracy: 0.5544\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 1s 720us/step - loss: 0.4363 - accuracy: 0.8080 - val_loss: 0.7874 - val_accuracy: 0.5479\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 1s 724us/step - loss: 0.4311 - accuracy: 0.8115 - val_loss: 0.7885 - val_accuracy: 0.5484\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 1s 745us/step - loss: 0.4280 - accuracy: 0.8132 - val_loss: 0.7659 - val_accuracy: 0.5636\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 1s 734us/step - loss: 0.4256 - accuracy: 0.8149 - val_loss: 0.8213 - val_accuracy: 0.5436\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 1s 727us/step - loss: 0.4237 - accuracy: 0.8159 - val_loss: 0.6358 - val_accuracy: 0.6309\n",
      "0.8159499764442444\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 1s 795us/step - loss: 0.5656 - accuracy: 0.7153 - val_loss: 0.8303 - val_accuracy: 0.4179\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 1s 730us/step - loss: 0.4912 - accuracy: 0.7737 - val_loss: 0.8530 - val_accuracy: 0.4653\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 1s 736us/step - loss: 0.4707 - accuracy: 0.7862 - val_loss: 0.7288 - val_accuracy: 0.5572\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 1s 729us/step - loss: 0.4584 - accuracy: 0.7940 - val_loss: 0.8469 - val_accuracy: 0.5057\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 1s 727us/step - loss: 0.4487 - accuracy: 0.8002 - val_loss: 0.8001 - val_accuracy: 0.5386\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 1s 723us/step - loss: 0.4408 - accuracy: 0.8054 - val_loss: 0.7438 - val_accuracy: 0.5727\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 1s 739us/step - loss: 0.4349 - accuracy: 0.8087 - val_loss: 0.7346 - val_accuracy: 0.5756\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 1s 735us/step - loss: 0.4304 - accuracy: 0.8122 - val_loss: 0.7751 - val_accuracy: 0.5570\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 1s 740us/step - loss: 0.4267 - accuracy: 0.8136 - val_loss: 0.7944 - val_accuracy: 0.5531\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 1s 733us/step - loss: 0.4239 - accuracy: 0.8137 - val_loss: 0.7458 - val_accuracy: 0.5755\n",
      "0.8137000203132629\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 1s 798us/step - loss: 0.5547 - accuracy: 0.7262 - val_loss: 0.8401 - val_accuracy: 0.4391\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 1s 754us/step - loss: 0.4826 - accuracy: 0.7817 - val_loss: 0.7221 - val_accuracy: 0.5488\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 1s 740us/step - loss: 0.4615 - accuracy: 0.7940 - val_loss: 0.7494 - val_accuracy: 0.5482\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 1s 760us/step - loss: 0.4484 - accuracy: 0.8025 - val_loss: 0.7895 - val_accuracy: 0.5369\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 1s 714us/step - loss: 0.4389 - accuracy: 0.8072 - val_loss: 0.9300 - val_accuracy: 0.4912\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 1s 729us/step - loss: 0.4328 - accuracy: 0.8101 - val_loss: 0.8219 - val_accuracy: 0.5363\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 1s 720us/step - loss: 0.4282 - accuracy: 0.8133 - val_loss: 0.7319 - val_accuracy: 0.5788\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 1s 710us/step - loss: 0.4252 - accuracy: 0.8149 - val_loss: 0.7631 - val_accuracy: 0.5682\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 1s 732us/step - loss: 0.4232 - accuracy: 0.8149 - val_loss: 0.8301 - val_accuracy: 0.5445\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 1s 723us/step - loss: 0.4213 - accuracy: 0.8169 - val_loss: 0.6734 - val_accuracy: 0.6212\n",
      "0.8168749809265137\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 1s 778us/step - loss: 0.5603 - accuracy: 0.7205 - val_loss: 0.9241 - val_accuracy: 0.3788\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 1s 747us/step - loss: 0.4840 - accuracy: 0.7799 - val_loss: 0.8665 - val_accuracy: 0.4570\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 1s 768us/step - loss: 0.4619 - accuracy: 0.7950 - val_loss: 0.8403 - val_accuracy: 0.4988\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 1s 758us/step - loss: 0.4461 - accuracy: 0.8038 - val_loss: 0.7599 - val_accuracy: 0.5527\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 1s 724us/step - loss: 0.4354 - accuracy: 0.8106 - val_loss: 0.7808 - val_accuracy: 0.5498\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 1s 746us/step - loss: 0.4285 - accuracy: 0.8143 - val_loss: 0.8485 - val_accuracy: 0.5225\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 1s 742us/step - loss: 0.4239 - accuracy: 0.8158 - val_loss: 0.7094 - val_accuracy: 0.5944\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 1s 747us/step - loss: 0.4214 - accuracy: 0.8177 - val_loss: 0.7480 - val_accuracy: 0.5778\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 1s 725us/step - loss: 0.4187 - accuracy: 0.8190 - val_loss: 0.7613 - val_accuracy: 0.5725\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 1s 754us/step - loss: 0.4166 - accuracy: 0.8200 - val_loss: 0.7729 - val_accuracy: 0.5741\n",
      "0.8199999928474426\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250/1250 [==============================] - 1s 793us/step - loss: 0.5617 - accuracy: 0.7220 - val_loss: 0.8169 - val_accuracy: 0.4506\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 1s 720us/step - loss: 0.4860 - accuracy: 0.7796 - val_loss: 0.8259 - val_accuracy: 0.4948\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 1s 730us/step - loss: 0.4638 - accuracy: 0.7930 - val_loss: 0.7622 - val_accuracy: 0.5532\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 1s 727us/step - loss: 0.4484 - accuracy: 0.8015 - val_loss: 0.8684 - val_accuracy: 0.5078\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 1s 725us/step - loss: 0.4374 - accuracy: 0.8084 - val_loss: 0.7389 - val_accuracy: 0.5758\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 1s 724us/step - loss: 0.4299 - accuracy: 0.8133 - val_loss: 0.7796 - val_accuracy: 0.5612\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 1s 763us/step - loss: 0.4252 - accuracy: 0.8160 - val_loss: 0.7925 - val_accuracy: 0.5564\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 1s 741us/step - loss: 0.4217 - accuracy: 0.8168 - val_loss: 0.8523 - val_accuracy: 0.5332\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 1s 742us/step - loss: 0.4194 - accuracy: 0.8182 - val_loss: 0.8942 - val_accuracy: 0.5214\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 1s 733us/step - loss: 0.4176 - accuracy: 0.8191 - val_loss: 0.7736 - val_accuracy: 0.5711\n",
      "0.8190749883651733\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 1s 797us/step - loss: 0.5609 - accuracy: 0.7212 - val_loss: 0.8618 - val_accuracy: 0.4279\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 1s 724us/step - loss: 0.4807 - accuracy: 0.7845 - val_loss: 0.8370 - val_accuracy: 0.4851\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 1s 727us/step - loss: 0.4607 - accuracy: 0.7970 - val_loss: 0.9093 - val_accuracy: 0.4601\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 1s 716us/step - loss: 0.4490 - accuracy: 0.8046 - val_loss: 0.7581 - val_accuracy: 0.5446\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 1s 735us/step - loss: 0.4397 - accuracy: 0.8095 - val_loss: 0.7934 - val_accuracy: 0.5286\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 1s 728us/step - loss: 0.4337 - accuracy: 0.8122 - val_loss: 0.7378 - val_accuracy: 0.5612\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 1s 758us/step - loss: 0.4293 - accuracy: 0.8155 - val_loss: 0.7399 - val_accuracy: 0.5678\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 1s 756us/step - loss: 0.4260 - accuracy: 0.8161 - val_loss: 0.6746 - val_accuracy: 0.6063\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 1s 730us/step - loss: 0.4235 - accuracy: 0.8180 - val_loss: 0.7359 - val_accuracy: 0.5750\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 1s 734us/step - loss: 0.4213 - accuracy: 0.8183 - val_loss: 0.6526 - val_accuracy: 0.6197\n",
      "0.8182500004768372\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 1s 788us/step - loss: 0.5642 - accuracy: 0.7165 - val_loss: 0.9294 - val_accuracy: 0.3705\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 1s 721us/step - loss: 0.4887 - accuracy: 0.7772 - val_loss: 0.7664 - val_accuracy: 0.5233\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 1s 728us/step - loss: 0.4670 - accuracy: 0.7903 - val_loss: 0.8621 - val_accuracy: 0.4922\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 1s 735us/step - loss: 0.4532 - accuracy: 0.7987 - val_loss: 0.7745 - val_accuracy: 0.5464\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 1s 748us/step - loss: 0.4428 - accuracy: 0.8058 - val_loss: 0.7078 - val_accuracy: 0.5884\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 1s 740us/step - loss: 0.4356 - accuracy: 0.8104 - val_loss: 0.7632 - val_accuracy: 0.5623\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 1s 738us/step - loss: 0.4307 - accuracy: 0.8134 - val_loss: 0.7238 - val_accuracy: 0.5826\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 1s 707us/step - loss: 0.4269 - accuracy: 0.8155 - val_loss: 0.7901 - val_accuracy: 0.5558\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 1s 705us/step - loss: 0.4239 - accuracy: 0.8173 - val_loss: 0.7811 - val_accuracy: 0.5639\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 1s 740us/step - loss: 0.4218 - accuracy: 0.8180 - val_loss: 0.7739 - val_accuracy: 0.5684\n",
      "0.8179749846458435\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 1s 809us/step - loss: 0.5651 - accuracy: 0.7162 - val_loss: 1.0062 - val_accuracy: 0.3256\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 1s 738us/step - loss: 0.4911 - accuracy: 0.7752 - val_loss: 0.8773 - val_accuracy: 0.4592\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 1s 728us/step - loss: 0.4697 - accuracy: 0.7881 - val_loss: 0.8544 - val_accuracy: 0.4933\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 1s 738us/step - loss: 0.4572 - accuracy: 0.7962 - val_loss: 0.7300 - val_accuracy: 0.5688\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 1s 741us/step - loss: 0.4471 - accuracy: 0.8016 - val_loss: 0.7688 - val_accuracy: 0.5554\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 1s 741us/step - loss: 0.4396 - accuracy: 0.8066 - val_loss: 0.8301 - val_accuracy: 0.5276\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 1s 729us/step - loss: 0.4340 - accuracy: 0.8107 - val_loss: 0.8592 - val_accuracy: 0.5204\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 1s 730us/step - loss: 0.4300 - accuracy: 0.8126 - val_loss: 0.8753 - val_accuracy: 0.5149\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 1s 738us/step - loss: 0.4268 - accuracy: 0.8147 - val_loss: 0.8390 - val_accuracy: 0.5350\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 1s 744us/step - loss: 0.4241 - accuracy: 0.8159 - val_loss: 0.8757 - val_accuracy: 0.5248\n",
      "0.8158749938011169\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 1s 810us/step - loss: 0.5480 - accuracy: 0.7312 - val_loss: 0.9614 - val_accuracy: 0.3757\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 1s 741us/step - loss: 0.4780 - accuracy: 0.7850 - val_loss: 0.8708 - val_accuracy: 0.4666\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 1s 732us/step - loss: 0.4567 - accuracy: 0.7988 - val_loss: 0.8140 - val_accuracy: 0.5008\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 1s 731us/step - loss: 0.4430 - accuracy: 0.8059 - val_loss: 0.7185 - val_accuracy: 0.5662\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 1s 735us/step - loss: 0.4332 - accuracy: 0.8093 - val_loss: 0.8589 - val_accuracy: 0.5032\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 1s 746us/step - loss: 0.4274 - accuracy: 0.8127 - val_loss: 0.7393 - val_accuracy: 0.5660\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 1s 743us/step - loss: 0.4232 - accuracy: 0.8157 - val_loss: 0.7033 - val_accuracy: 0.5871\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 1s 748us/step - loss: 0.4201 - accuracy: 0.8171 - val_loss: 0.7954 - val_accuracy: 0.5517\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 1s 738us/step - loss: 0.4177 - accuracy: 0.8182 - val_loss: 0.7188 - val_accuracy: 0.5899\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 1s 732us/step - loss: 0.4159 - accuracy: 0.8196 - val_loss: 0.8854 - val_accuracy: 0.5155\n",
      "0.8195750117301941\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 1s 813us/step - loss: 0.5654 - accuracy: 0.7156 - val_loss: 0.7748 - val_accuracy: 0.4716\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 1s 740us/step - loss: 0.4894 - accuracy: 0.7757 - val_loss: 0.8788 - val_accuracy: 0.4582\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 1s 735us/step - loss: 0.4673 - accuracy: 0.7916 - val_loss: 0.8550 - val_accuracy: 0.5004\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 1s 744us/step - loss: 0.4535 - accuracy: 0.7992 - val_loss: 0.7586 - val_accuracy: 0.5576\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 1s 747us/step - loss: 0.4427 - accuracy: 0.8042 - val_loss: 0.7275 - val_accuracy: 0.5809\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 1s 751us/step - loss: 0.4344 - accuracy: 0.8100 - val_loss: 0.8068 - val_accuracy: 0.5371\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 1s 740us/step - loss: 0.4290 - accuracy: 0.8121 - val_loss: 0.7143 - val_accuracy: 0.5916\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 1s 779us/step - loss: 0.4250 - accuracy: 0.8143 - val_loss: 0.6867 - val_accuracy: 0.6092\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 1s 777us/step - loss: 0.4226 - accuracy: 0.8154 - val_loss: 0.7002 - val_accuracy: 0.6018\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 1s 774us/step - loss: 0.4205 - accuracy: 0.8178 - val_loss: 0.7668 - val_accuracy: 0.5737\n",
      "0.817799985408783\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 0.5526 - accuracy: 0.7281 - val_loss: 0.9002 - val_accuracy: 0.4089\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 1s 699us/step - loss: 0.4788 - accuracy: 0.7834 - val_loss: 0.8469 - val_accuracy: 0.4762\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 1s 721us/step - loss: 0.4567 - accuracy: 0.7966 - val_loss: 0.8610 - val_accuracy: 0.4940\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 1s 712us/step - loss: 0.4428 - accuracy: 0.8042 - val_loss: 0.7989 - val_accuracy: 0.5328\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 1s 707us/step - loss: 0.4331 - accuracy: 0.8115 - val_loss: 0.7442 - val_accuracy: 0.5656\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 1s 824us/step - loss: 0.4265 - accuracy: 0.8143 - val_loss: 0.6744 - val_accuracy: 0.6085\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 1s 708us/step - loss: 0.4221 - accuracy: 0.8187 - val_loss: 0.8431 - val_accuracy: 0.5324\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 1s 695us/step - loss: 0.4192 - accuracy: 0.8193 - val_loss: 0.6938 - val_accuracy: 0.6012\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 1s 698us/step - loss: 0.4170 - accuracy: 0.8205 - val_loss: 0.8154 - val_accuracy: 0.5518\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 1s 694us/step - loss: 0.4148 - accuracy: 0.8219 - val_loss: 0.6886 - val_accuracy: 0.6093\n",
      "0.8218500018119812\n",
      "if any\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 2s 783us/step - loss: 0.5604 - accuracy: 0.7205 - val_loss: 0.8840 - val_accuracy: 0.4084\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 1s 776us/step - loss: 0.4865 - accuracy: 0.7804 - val_loss: 0.7965 - val_accuracy: 0.4964\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 1s 932us/step - loss: 0.4660 - accuracy: 0.7937 - val_loss: 0.8487 - val_accuracy: 0.4851\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 1s 709us/step - loss: 0.4515 - accuracy: 0.8018 - val_loss: 0.8057 - val_accuracy: 0.5230\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 1s 716us/step - loss: 0.4411 - accuracy: 0.8088 - val_loss: 0.7268 - val_accuracy: 0.5682\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 1s 720us/step - loss: 0.4336 - accuracy: 0.8130 - val_loss: 0.7910 - val_accuracy: 0.5451\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 1s 706us/step - loss: 0.4283 - accuracy: 0.8170 - val_loss: 0.7634 - val_accuracy: 0.5585\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 1s 765us/step - loss: 0.4240 - accuracy: 0.8178 - val_loss: 0.6561 - val_accuracy: 0.6183\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 1s 711us/step - loss: 0.4207 - accuracy: 0.8199 - val_loss: 0.8627 - val_accuracy: 0.5236\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 1s 692us/step - loss: 0.4181 - accuracy: 0.8218 - val_loss: 0.7753 - val_accuracy: 0.5634\n",
      "0.8218250274658203\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 1s 752us/step - loss: 0.5653 - accuracy: 0.7161 - val_loss: 0.9417 - val_accuracy: 0.3564\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 1s 721us/step - loss: 0.4893 - accuracy: 0.7750 - val_loss: 0.8860 - val_accuracy: 0.4513\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 1s 733us/step - loss: 0.4665 - accuracy: 0.7909 - val_loss: 0.9251 - val_accuracy: 0.4573\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 1s 825us/step - loss: 0.4534 - accuracy: 0.7991 - val_loss: 0.8163 - val_accuracy: 0.5257\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 1s 725us/step - loss: 0.4435 - accuracy: 0.8063 - val_loss: 0.7811 - val_accuracy: 0.5499\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 1s 703us/step - loss: 0.4359 - accuracy: 0.8099 - val_loss: 0.8594 - val_accuracy: 0.5198\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 1s 809us/step - loss: 0.4302 - accuracy: 0.8131 - val_loss: 0.7848 - val_accuracy: 0.5512\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 1s 894us/step - loss: 0.4260 - accuracy: 0.8156 - val_loss: 0.8639 - val_accuracy: 0.5201\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 1s 815us/step - loss: 0.4216 - accuracy: 0.8181 - val_loss: 0.7420 - val_accuracy: 0.5783\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 1s 806us/step - loss: 0.4179 - accuracy: 0.8196 - val_loss: 0.8230 - val_accuracy: 0.5407\n",
      "0.819599986076355\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 1s 900us/step - loss: 0.5595 - accuracy: 0.7226 - val_loss: 0.9645 - val_accuracy: 0.3616\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 1s 779us/step - loss: 0.4822 - accuracy: 0.7826 - val_loss: 0.8449 - val_accuracy: 0.4766\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 1s 807us/step - loss: 0.4609 - accuracy: 0.7952 - val_loss: 0.7721 - val_accuracy: 0.5360\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 1s 789us/step - loss: 0.4479 - accuracy: 0.8023 - val_loss: 0.7723 - val_accuracy: 0.5460\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 1s 783us/step - loss: 0.4380 - accuracy: 0.8086 - val_loss: 0.7422 - val_accuracy: 0.5662\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 1s 778us/step - loss: 0.4314 - accuracy: 0.8129 - val_loss: 0.7967 - val_accuracy: 0.5441\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 1s 760us/step - loss: 0.4273 - accuracy: 0.8148 - val_loss: 0.7634 - val_accuracy: 0.5620\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 1s 794us/step - loss: 0.4241 - accuracy: 0.8167 - val_loss: 0.8564 - val_accuracy: 0.5256\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 1s 807us/step - loss: 0.4209 - accuracy: 0.8183 - val_loss: 0.7214 - val_accuracy: 0.5899\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 1s 776us/step - loss: 0.4190 - accuracy: 0.8205 - val_loss: 0.7367 - val_accuracy: 0.5804\n",
      "0.8205000162124634\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 1s 819us/step - loss: 0.5640 - accuracy: 0.7186 - val_loss: 0.8826 - val_accuracy: 0.4057\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 1s 746us/step - loss: 0.4861 - accuracy: 0.7786 - val_loss: 0.7967 - val_accuracy: 0.5139\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 1s 771us/step - loss: 0.4645 - accuracy: 0.7922 - val_loss: 0.8363 - val_accuracy: 0.5106\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 1s 789us/step - loss: 0.4516 - accuracy: 0.8000 - val_loss: 0.7493 - val_accuracy: 0.5593\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 1s 759us/step - loss: 0.4419 - accuracy: 0.8057 - val_loss: 0.8103 - val_accuracy: 0.5338\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 1s 775us/step - loss: 0.4353 - accuracy: 0.8116 - val_loss: 0.7196 - val_accuracy: 0.5799\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 1s 777us/step - loss: 0.4302 - accuracy: 0.8136 - val_loss: 0.8267 - val_accuracy: 0.5347\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 1s 776us/step - loss: 0.4268 - accuracy: 0.8160 - val_loss: 0.7201 - val_accuracy: 0.5886\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 1s 800us/step - loss: 0.4239 - accuracy: 0.8176 - val_loss: 0.8331 - val_accuracy: 0.5379\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 1s 761us/step - loss: 0.4215 - accuracy: 0.8184 - val_loss: 0.6874 - val_accuracy: 0.6111\n",
      "0.8183500170707703\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250/1250 [==============================] - 1s 856us/step - loss: 0.5661 - accuracy: 0.7156 - val_loss: 0.9151 - val_accuracy: 0.3799\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 1s 746us/step - loss: 0.4880 - accuracy: 0.7778 - val_loss: 0.7864 - val_accuracy: 0.5144\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 1s 753us/step - loss: 0.4680 - accuracy: 0.7906 - val_loss: 0.7382 - val_accuracy: 0.5567\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 1s 741us/step - loss: 0.4561 - accuracy: 0.7986 - val_loss: 0.7386 - val_accuracy: 0.5631\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 1s 752us/step - loss: 0.4473 - accuracy: 0.8039 - val_loss: 0.7590 - val_accuracy: 0.5561\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 1s 798us/step - loss: 0.4408 - accuracy: 0.8077 - val_loss: 0.8073 - val_accuracy: 0.5363\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 1s 892us/step - loss: 0.4359 - accuracy: 0.8103 - val_loss: 0.7528 - val_accuracy: 0.5630\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 1s 765us/step - loss: 0.4327 - accuracy: 0.8122 - val_loss: 0.7466 - val_accuracy: 0.5724\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 1s 783us/step - loss: 0.4298 - accuracy: 0.8143 - val_loss: 0.8157 - val_accuracy: 0.5458\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 1s 765us/step - loss: 0.4276 - accuracy: 0.8147 - val_loss: 0.9272 - val_accuracy: 0.5051\n",
      "0.8147000074386597\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 1s 821us/step - loss: 0.5527 - accuracy: 0.7262 - val_loss: 0.8084 - val_accuracy: 0.4663\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 1s 767us/step - loss: 0.4795 - accuracy: 0.7828 - val_loss: 0.7972 - val_accuracy: 0.5058\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 1s 763us/step - loss: 0.4588 - accuracy: 0.7940 - val_loss: 0.8439 - val_accuracy: 0.5004\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 1s 795us/step - loss: 0.4449 - accuracy: 0.8014 - val_loss: 0.9170 - val_accuracy: 0.4783\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 1s 762us/step - loss: 0.4358 - accuracy: 0.8084 - val_loss: 0.8845 - val_accuracy: 0.5022\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 1s 782us/step - loss: 0.4293 - accuracy: 0.8123 - val_loss: 0.8359 - val_accuracy: 0.5346\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 1s 763us/step - loss: 0.4253 - accuracy: 0.8148 - val_loss: 0.7982 - val_accuracy: 0.5525\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 1s 767us/step - loss: 0.4223 - accuracy: 0.8164 - val_loss: 0.8566 - val_accuracy: 0.5330\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 1s 755us/step - loss: 0.4195 - accuracy: 0.8180 - val_loss: 0.7368 - val_accuracy: 0.5898\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 1s 764us/step - loss: 0.4182 - accuracy: 0.8179 - val_loss: 0.7373 - val_accuracy: 0.5900\n",
      "0.8179000020027161\n",
      "if any\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 1s 827us/step - loss: 0.5602 - accuracy: 0.7217 - val_loss: 0.8748 - val_accuracy: 0.4217\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 1s 783us/step - loss: 0.4858 - accuracy: 0.7789 - val_loss: 0.7869 - val_accuracy: 0.5133\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 1s 764us/step - loss: 0.4651 - accuracy: 0.7926 - val_loss: 0.8360 - val_accuracy: 0.5038\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 1s 760us/step - loss: 0.4513 - accuracy: 0.8020 - val_loss: 0.7839 - val_accuracy: 0.5411\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 1s 758us/step - loss: 0.4416 - accuracy: 0.8062 - val_loss: 0.6728 - val_accuracy: 0.6068\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 1s 763us/step - loss: 0.4354 - accuracy: 0.8106 - val_loss: 0.7534 - val_accuracy: 0.5613\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 1s 764us/step - loss: 0.4312 - accuracy: 0.8127 - val_loss: 0.7161 - val_accuracy: 0.5861\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 1s 771us/step - loss: 0.4280 - accuracy: 0.8148 - val_loss: 0.7822 - val_accuracy: 0.5625\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 1s 785us/step - loss: 0.4260 - accuracy: 0.8160 - val_loss: 0.7845 - val_accuracy: 0.5646\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 1s 815us/step - loss: 0.4235 - accuracy: 0.8176 - val_loss: 0.7549 - val_accuracy: 0.5789\n",
      "0.8175749778747559\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 1s 828us/step - loss: 0.5613 - accuracy: 0.7197 - val_loss: 0.7212 - val_accuracy: 0.5263\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 1s 772us/step - loss: 0.4790 - accuracy: 0.7847 - val_loss: 0.9312 - val_accuracy: 0.4455\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 1s 766us/step - loss: 0.4595 - accuracy: 0.7977 - val_loss: 0.9360 - val_accuracy: 0.4603\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 1s 776us/step - loss: 0.4466 - accuracy: 0.8051 - val_loss: 0.8972 - val_accuracy: 0.4897\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 1s 767us/step - loss: 0.4378 - accuracy: 0.8102 - val_loss: 0.8464 - val_accuracy: 0.5147\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 1s 787us/step - loss: 0.4309 - accuracy: 0.8144 - val_loss: 0.7126 - val_accuracy: 0.5818\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 1s 795us/step - loss: 0.4265 - accuracy: 0.8150 - val_loss: 0.7317 - val_accuracy: 0.5717\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 1s 772us/step - loss: 0.4234 - accuracy: 0.8173 - val_loss: 0.7180 - val_accuracy: 0.5805\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 1s 774us/step - loss: 0.4207 - accuracy: 0.8198 - val_loss: 0.6885 - val_accuracy: 0.6026\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 1s 770us/step - loss: 0.4192 - accuracy: 0.8190 - val_loss: 0.7541 - val_accuracy: 0.5753\n",
      "0.8190000057220459\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 1s 820us/step - loss: 0.5601 - accuracy: 0.7191 - val_loss: 0.9449 - val_accuracy: 0.3830\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 1s 784us/step - loss: 0.4802 - accuracy: 0.7816 - val_loss: 0.8374 - val_accuracy: 0.4957\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 1s 770us/step - loss: 0.4603 - accuracy: 0.7950 - val_loss: 0.8017 - val_accuracy: 0.5319\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 1s 772us/step - loss: 0.4483 - accuracy: 0.8026 - val_loss: 0.8057 - val_accuracy: 0.5427\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 1s 766us/step - loss: 0.4390 - accuracy: 0.8083 - val_loss: 0.7313 - val_accuracy: 0.5818\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 1s 765us/step - loss: 0.4326 - accuracy: 0.8122 - val_loss: 0.7621 - val_accuracy: 0.5727\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 1s 771us/step - loss: 0.4276 - accuracy: 0.8142 - val_loss: 0.7677 - val_accuracy: 0.5647\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 1s 769us/step - loss: 0.4244 - accuracy: 0.8173 - val_loss: 0.7995 - val_accuracy: 0.5565\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 1s 808us/step - loss: 0.4217 - accuracy: 0.8180 - val_loss: 0.7823 - val_accuracy: 0.5629\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 1s 830us/step - loss: 0.4192 - accuracy: 0.8190 - val_loss: 0.7548 - val_accuracy: 0.5756\n",
      "0.8189749717712402\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 1s 790us/step - loss: 0.5494 - accuracy: 0.7291 - val_loss: 0.7596 - val_accuracy: 0.4977\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 1s 701us/step - loss: 0.4775 - accuracy: 0.7849 - val_loss: 0.8560 - val_accuracy: 0.4752\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 1s 724us/step - loss: 0.4572 - accuracy: 0.7977 - val_loss: 0.8189 - val_accuracy: 0.5038\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 1s 777us/step - loss: 0.4437 - accuracy: 0.8062 - val_loss: 0.7833 - val_accuracy: 0.5350\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 1s 785us/step - loss: 0.4340 - accuracy: 0.8116 - val_loss: 0.8317 - val_accuracy: 0.5193\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 1s 782us/step - loss: 0.4279 - accuracy: 0.8144 - val_loss: 0.8363 - val_accuracy: 0.5224\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 1s 735us/step - loss: 0.4234 - accuracy: 0.8175 - val_loss: 0.7242 - val_accuracy: 0.5756\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 1s 743us/step - loss: 0.4207 - accuracy: 0.8178 - val_loss: 0.6614 - val_accuracy: 0.6145\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 1s 755us/step - loss: 0.4181 - accuracy: 0.8194 - val_loss: 0.7549 - val_accuracy: 0.5705\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 1s 788us/step - loss: 0.4159 - accuracy: 0.8193 - val_loss: 0.6929 - val_accuracy: 0.6079\n",
      "0.8193249702453613\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 1s 789us/step - loss: 0.5548 - accuracy: 0.7247 - val_loss: 0.8723 - val_accuracy: 0.4210\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 1s 730us/step - loss: 0.4793 - accuracy: 0.7822 - val_loss: 0.8545 - val_accuracy: 0.4751\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 1s 726us/step - loss: 0.4570 - accuracy: 0.7971 - val_loss: 0.8501 - val_accuracy: 0.5024\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 1s 761us/step - loss: 0.4429 - accuracy: 0.8051 - val_loss: 0.8111 - val_accuracy: 0.5306\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 1s 733us/step - loss: 0.4319 - accuracy: 0.8115 - val_loss: 0.7344 - val_accuracy: 0.5792\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 1s 720us/step - loss: 0.4245 - accuracy: 0.8160 - val_loss: 0.9041 - val_accuracy: 0.5118\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 1s 732us/step - loss: 0.4197 - accuracy: 0.8179 - val_loss: 0.7775 - val_accuracy: 0.5662\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 1s 732us/step - loss: 0.4161 - accuracy: 0.8206 - val_loss: 0.7284 - val_accuracy: 0.5889\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 1s 725us/step - loss: 0.4132 - accuracy: 0.8220 - val_loss: 0.7465 - val_accuracy: 0.5915\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 1s 738us/step - loss: 0.4112 - accuracy: 0.8228 - val_loss: 0.6345 - val_accuracy: 0.6414\n",
      "0.822825014591217\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 1s 810us/step - loss: 0.5559 - accuracy: 0.7256 - val_loss: 0.9168 - val_accuracy: 0.3928\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 1s 753us/step - loss: 0.4806 - accuracy: 0.7822 - val_loss: 0.8741 - val_accuracy: 0.4608\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 1s 766us/step - loss: 0.4603 - accuracy: 0.7961 - val_loss: 0.7133 - val_accuracy: 0.5660\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 1s 726us/step - loss: 0.4468 - accuracy: 0.8042 - val_loss: 0.7801 - val_accuracy: 0.5380\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 1s 739us/step - loss: 0.4376 - accuracy: 0.8089 - val_loss: 0.7491 - val_accuracy: 0.5584\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 1s 737us/step - loss: 0.4313 - accuracy: 0.8122 - val_loss: 0.7344 - val_accuracy: 0.5710\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 1s 734us/step - loss: 0.4266 - accuracy: 0.8150 - val_loss: 0.7581 - val_accuracy: 0.5642\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 1s 728us/step - loss: 0.4234 - accuracy: 0.8176 - val_loss: 0.7790 - val_accuracy: 0.5572\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 1s 747us/step - loss: 0.4214 - accuracy: 0.8187 - val_loss: 0.7512 - val_accuracy: 0.5723\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 1s 728us/step - loss: 0.4192 - accuracy: 0.8188 - val_loss: 0.7636 - val_accuracy: 0.5715\n",
      "0.8187749981880188\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 1s 797us/step - loss: 0.5538 - accuracy: 0.7267 - val_loss: 0.8516 - val_accuracy: 0.4361\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 1s 755us/step - loss: 0.4810 - accuracy: 0.7822 - val_loss: 0.8418 - val_accuracy: 0.4822\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 1s 713us/step - loss: 0.4609 - accuracy: 0.7948 - val_loss: 0.8451 - val_accuracy: 0.4964\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 1s 726us/step - loss: 0.4478 - accuracy: 0.8025 - val_loss: 0.7925 - val_accuracy: 0.5336\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 1s 727us/step - loss: 0.4382 - accuracy: 0.8079 - val_loss: 0.7854 - val_accuracy: 0.5460\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 1s 724us/step - loss: 0.4315 - accuracy: 0.8122 - val_loss: 0.7622 - val_accuracy: 0.5615\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 1s 724us/step - loss: 0.4267 - accuracy: 0.8162 - val_loss: 0.7542 - val_accuracy: 0.5702\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 1s 725us/step - loss: 0.4236 - accuracy: 0.8163 - val_loss: 0.7247 - val_accuracy: 0.5890\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 1s 727us/step - loss: 0.4215 - accuracy: 0.8178 - val_loss: 0.7350 - val_accuracy: 0.5894\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 1s 740us/step - loss: 0.4194 - accuracy: 0.8189 - val_loss: 0.8246 - val_accuracy: 0.5546\n",
      "0.8188750147819519\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 1s 833us/step - loss: 0.5624 - accuracy: 0.7189 - val_loss: 1.0003 - val_accuracy: 0.3445\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 1s 732us/step - loss: 0.4844 - accuracy: 0.7790 - val_loss: 0.7876 - val_accuracy: 0.5212\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 1s 734us/step - loss: 0.4641 - accuracy: 0.7935 - val_loss: 0.8053 - val_accuracy: 0.5264\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 1s 754us/step - loss: 0.4513 - accuracy: 0.8002 - val_loss: 0.8306 - val_accuracy: 0.5207\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 1s 735us/step - loss: 0.4420 - accuracy: 0.8054 - val_loss: 0.7940 - val_accuracy: 0.5475\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 1s 736us/step - loss: 0.4353 - accuracy: 0.8091 - val_loss: 0.7928 - val_accuracy: 0.5535\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 1s 729us/step - loss: 0.4299 - accuracy: 0.8138 - val_loss: 0.7979 - val_accuracy: 0.5557\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 1s 730us/step - loss: 0.4259 - accuracy: 0.8156 - val_loss: 0.7692 - val_accuracy: 0.5685\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 1s 730us/step - loss: 0.4228 - accuracy: 0.8176 - val_loss: 0.7942 - val_accuracy: 0.5583\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 1s 737us/step - loss: 0.4206 - accuracy: 0.8193 - val_loss: 0.7459 - val_accuracy: 0.5836\n",
      "0.8192750215530396\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 1s 791us/step - loss: 0.5598 - accuracy: 0.7211 - val_loss: 0.9085 - val_accuracy: 0.3938\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 1s 727us/step - loss: 0.4839 - accuracy: 0.7792 - val_loss: 0.9368 - val_accuracy: 0.4262\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 1s 737us/step - loss: 0.4638 - accuracy: 0.7913 - val_loss: 0.7438 - val_accuracy: 0.5420\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 1s 732us/step - loss: 0.4504 - accuracy: 0.8009 - val_loss: 0.8714 - val_accuracy: 0.4933\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 1s 729us/step - loss: 0.4396 - accuracy: 0.8078 - val_loss: 0.7930 - val_accuracy: 0.5345\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 1s 756us/step - loss: 0.4327 - accuracy: 0.8132 - val_loss: 0.6947 - val_accuracy: 0.5919\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 1s 743us/step - loss: 0.4280 - accuracy: 0.8142 - val_loss: 0.7729 - val_accuracy: 0.5540\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 1s 729us/step - loss: 0.4249 - accuracy: 0.8155 - val_loss: 0.7210 - val_accuracy: 0.5815\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 1s 759us/step - loss: 0.4222 - accuracy: 0.8166 - val_loss: 0.7456 - val_accuracy: 0.5732\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 1s 745us/step - loss: 0.4205 - accuracy: 0.8183 - val_loss: 0.7922 - val_accuracy: 0.5575\n",
      "0.8183249831199646\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250/1250 [==============================] - 1s 788us/step - loss: 0.5653 - accuracy: 0.7162 - val_loss: 0.8378 - val_accuracy: 0.4217\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 1s 718us/step - loss: 0.4851 - accuracy: 0.7789 - val_loss: 0.8050 - val_accuracy: 0.5060\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 1s 721us/step - loss: 0.4640 - accuracy: 0.7933 - val_loss: 0.8637 - val_accuracy: 0.4931\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 1s 724us/step - loss: 0.4520 - accuracy: 0.7997 - val_loss: 0.8556 - val_accuracy: 0.5104\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 1s 713us/step - loss: 0.4431 - accuracy: 0.8040 - val_loss: 0.7200 - val_accuracy: 0.5816\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 1s 721us/step - loss: 0.4368 - accuracy: 0.8082 - val_loss: 0.7681 - val_accuracy: 0.5590\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 1s 720us/step - loss: 0.4317 - accuracy: 0.8127 - val_loss: 0.7262 - val_accuracy: 0.5792\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 1s 737us/step - loss: 0.4280 - accuracy: 0.8148 - val_loss: 0.7837 - val_accuracy: 0.5542\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 1s 750us/step - loss: 0.4251 - accuracy: 0.8153 - val_loss: 0.7841 - val_accuracy: 0.5605\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 1s 719us/step - loss: 0.4230 - accuracy: 0.8175 - val_loss: 0.8471 - val_accuracy: 0.5394\n",
      "0.8174750208854675\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 1s 789us/step - loss: 0.5633 - accuracy: 0.7159 - val_loss: 0.8952 - val_accuracy: 0.3917\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 1s 746us/step - loss: 0.4858 - accuracy: 0.7784 - val_loss: 0.8297 - val_accuracy: 0.4922\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 1s 724us/step - loss: 0.4635 - accuracy: 0.7929 - val_loss: 0.8286 - val_accuracy: 0.5162\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 1s 731us/step - loss: 0.4509 - accuracy: 0.7990 - val_loss: 0.7343 - val_accuracy: 0.5757\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 1s 716us/step - loss: 0.4415 - accuracy: 0.8045 - val_loss: 0.7492 - val_accuracy: 0.5741\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 1s 724us/step - loss: 0.4342 - accuracy: 0.8101 - val_loss: 0.8369 - val_accuracy: 0.5368\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 1s 727us/step - loss: 0.4292 - accuracy: 0.8135 - val_loss: 0.7404 - val_accuracy: 0.5830\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 1s 744us/step - loss: 0.4254 - accuracy: 0.8161 - val_loss: 0.6748 - val_accuracy: 0.6180\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 1s 724us/step - loss: 0.4228 - accuracy: 0.8174 - val_loss: 0.7042 - val_accuracy: 0.6069\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 1s 736us/step - loss: 0.4203 - accuracy: 0.8189 - val_loss: 0.6787 - val_accuracy: 0.6190\n",
      "0.8188750147819519\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 1s 789us/step - loss: 0.5617 - accuracy: 0.7190 - val_loss: 0.8558 - val_accuracy: 0.4304\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 1s 722us/step - loss: 0.4841 - accuracy: 0.7798 - val_loss: 0.8655 - val_accuracy: 0.4706\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 1s 725us/step - loss: 0.4633 - accuracy: 0.7930 - val_loss: 0.8110 - val_accuracy: 0.5191\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 1s 725us/step - loss: 0.4498 - accuracy: 0.8010 - val_loss: 0.8576 - val_accuracy: 0.5076\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 1s 721us/step - loss: 0.4406 - accuracy: 0.8054 - val_loss: 0.8750 - val_accuracy: 0.5059\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 1s 725us/step - loss: 0.4346 - accuracy: 0.8092 - val_loss: 0.6761 - val_accuracy: 0.6062\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 1s 719us/step - loss: 0.4300 - accuracy: 0.8126 - val_loss: 0.6951 - val_accuracy: 0.5965\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 1s 763us/step - loss: 0.4270 - accuracy: 0.8138 - val_loss: 0.6427 - val_accuracy: 0.6267\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 1s 738us/step - loss: 0.4242 - accuracy: 0.8152 - val_loss: 0.7300 - val_accuracy: 0.5842\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 1s 731us/step - loss: 0.4222 - accuracy: 0.8173 - val_loss: 0.8114 - val_accuracy: 0.5542\n",
      "0.8172500133514404\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 1s 804us/step - loss: 0.5560 - accuracy: 0.7244 - val_loss: 0.8282 - val_accuracy: 0.4459\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 1s 725us/step - loss: 0.4795 - accuracy: 0.7836 - val_loss: 0.8457 - val_accuracy: 0.4781\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 1s 724us/step - loss: 0.4586 - accuracy: 0.7974 - val_loss: 0.8228 - val_accuracy: 0.5042\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 1s 731us/step - loss: 0.4459 - accuracy: 0.8039 - val_loss: 0.7943 - val_accuracy: 0.5314\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 1s 730us/step - loss: 0.4370 - accuracy: 0.8090 - val_loss: 0.8392 - val_accuracy: 0.5212\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 1s 726us/step - loss: 0.4305 - accuracy: 0.8133 - val_loss: 0.7923 - val_accuracy: 0.5430\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 1s 766us/step - loss: 0.4262 - accuracy: 0.8148 - val_loss: 0.7836 - val_accuracy: 0.5513\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 1s 745us/step - loss: 0.4240 - accuracy: 0.8168 - val_loss: 0.8743 - val_accuracy: 0.5180\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 1s 750us/step - loss: 0.4212 - accuracy: 0.8176 - val_loss: 0.6872 - val_accuracy: 0.6039\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 1s 738us/step - loss: 0.4195 - accuracy: 0.8182 - val_loss: 0.7449 - val_accuracy: 0.5771\n",
      "0.8181999921798706\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 1s 791us/step - loss: 0.5661 - accuracy: 0.7144 - val_loss: 0.9144 - val_accuracy: 0.3829\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 1s 726us/step - loss: 0.4862 - accuracy: 0.7788 - val_loss: 0.7578 - val_accuracy: 0.5242\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 1s 729us/step - loss: 0.4638 - accuracy: 0.7924 - val_loss: 0.8263 - val_accuracy: 0.5009\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 1s 748us/step - loss: 0.4498 - accuracy: 0.8007 - val_loss: 0.8182 - val_accuracy: 0.5167\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 1s 723us/step - loss: 0.4403 - accuracy: 0.8060 - val_loss: 0.7471 - val_accuracy: 0.5604\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 1s 750us/step - loss: 0.4345 - accuracy: 0.8092 - val_loss: 0.7611 - val_accuracy: 0.5581\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 1s 738us/step - loss: 0.4304 - accuracy: 0.8117 - val_loss: 0.7670 - val_accuracy: 0.5640\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 1s 736us/step - loss: 0.4275 - accuracy: 0.8148 - val_loss: 0.7609 - val_accuracy: 0.5733\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 1s 741us/step - loss: 0.4251 - accuracy: 0.8158 - val_loss: 0.7965 - val_accuracy: 0.5581\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 1s 806us/step - loss: 0.4230 - accuracy: 0.8173 - val_loss: 0.7668 - val_accuracy: 0.5768\n",
      "0.817300021648407\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 1s 826us/step - loss: 0.5682 - accuracy: 0.7151 - val_loss: 0.9129 - val_accuracy: 0.3748\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 1s 791us/step - loss: 0.4855 - accuracy: 0.7782 - val_loss: 0.7501 - val_accuracy: 0.5389\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 1s 791us/step - loss: 0.4649 - accuracy: 0.7904 - val_loss: 0.8079 - val_accuracy: 0.5191\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 1s 754us/step - loss: 0.4529 - accuracy: 0.7984 - val_loss: 0.7466 - val_accuracy: 0.5596\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 1s 776us/step - loss: 0.4441 - accuracy: 0.8028 - val_loss: 0.7332 - val_accuracy: 0.5748\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 1s 749us/step - loss: 0.4375 - accuracy: 0.8063 - val_loss: 0.7662 - val_accuracy: 0.5516\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 1s 727us/step - loss: 0.4325 - accuracy: 0.8105 - val_loss: 0.7300 - val_accuracy: 0.5816\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 1s 757us/step - loss: 0.4291 - accuracy: 0.8120 - val_loss: 0.7324 - val_accuracy: 0.5797\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 1s 758us/step - loss: 0.4263 - accuracy: 0.8137 - val_loss: 0.7239 - val_accuracy: 0.5885\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 1s 789us/step - loss: 0.4241 - accuracy: 0.8155 - val_loss: 0.7596 - val_accuracy: 0.5705\n",
      "0.8154500126838684\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "Done\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "Done\n",
      "andr aara h\n",
      "andr aara h\n",
      "Done\n",
      "andr aara h\n",
      "andr aara h\n",
      "Done\n",
      "andr aara h\n",
      "andr aara h\n",
      "Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ayushkv/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:45: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10938/10938 [==============================] - 6s 537us/step - loss: 0.4708 - accuracy: 0.7798\n",
      "4688/4688 [==============================] - 3s 553us/step - loss: 0.4711 - accuracy: 0.7801\n",
      "10938/10938 [==============================] - 6s 574us/step - loss: 0.4813 - accuracy: 0.7742\n",
      "4688/4688 [==============================] - 3s 580us/step - loss: 0.4817 - accuracy: 0.7737\n",
      "10938/10938 [==============================] - 6s 543us/step - loss: 0.4897 - accuracy: 0.7678\n",
      "4688/4688 [==============================] - 3s 549us/step - loss: 0.4900 - accuracy: 0.7675\n",
      "10938/10938 [==============================] - 6s 544us/step - loss: 0.4858 - accuracy: 0.7715\n",
      "4688/4688 [==============================] - 3s 544us/step - loss: 0.4863 - accuracy: 0.7711\n",
      "10938/10938 [==============================] - 7s 586us/step - loss: 0.4876 - accuracy: 0.7701\n",
      "4688/4688 [==============================] - 3s 549us/step - loss: 0.4879 - accuracy: 0.7699\n",
      "15625/15625 [==============================] - 7s 445us/step\n",
      "15625/15625 [==============================] - 8s 485us/step\n",
      "15625/15625 [==============================] - 7s 432us/step\n",
      "15625/15625 [==============================] - 7s 425us/step\n",
      "15625/15625 [==============================] - 7s 431us/step\n"
     ]
    }
   ],
   "source": [
    "# adding dense layer\n",
    "# adding dense layer\n",
    "initial_model= get_initial_model(dataset.shape[1]-1, 2)\n",
    "Models=[]\n",
    "val_acc=[]\n",
    "train_acc=[]\n",
    "test_acc=[]\n",
    "val_loss=[]\n",
    "train_loss=[]\n",
    "ind=0\n",
    "add_weights=[]\n",
    "true_values=[]\n",
    "while ind<len(samples):\n",
    "  \n",
    "  train_data=samples[ind]\n",
    "  ind=ind+1\n",
    "  \n",
    "  \n",
    "  ann_model=get_initial_model(dataset.shape[1]-1, 2) #same intial weights\n",
    "  ann_model.set_weights(initial_model.get_weights())\n",
    "  X_train=train_data.drop(columns=[target_variable])\n",
    "  \n",
    "\n",
    "  y_train=to_categorical(train_data[target_variable])\n",
    "    \n",
    "  ann_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) # metrics=['accuracy']\n",
    "  history = ann_model.fit(X_train, y_train, epochs=10, validation_split=0.2, verbose=1)\n",
    "  print(history.history['accuracy'][-1])\n",
    "\n",
    "  present=False\n",
    "  for i in range(len(Models)):\n",
    "    if (check_models(Models[i][0], ann_model.get_weights())):\n",
    "      print(\"if any\")\n",
    "      Models[i][1]=Models[i][1]+1\n",
    "      add_weights[i].append(ann_model.get_weights())\n",
    "      present=True\n",
    "      break;\n",
    "  if present==False:\n",
    "    add_weights.append([ann_model.get_weights()])\n",
    "    Models.append([ann_model.get_weights(), 1])\n",
    "\n",
    "len(Models)   \n",
    "#here use only top 5-10 integrally private models.\n",
    "#add_weights=add_weights[top_5]\n",
    "A=np.argsort(np.array(Models).T[1])[::-1][:5]\n",
    "recommended_models=[]\n",
    "for i in range(len(A)):\n",
    "    recommended_models.append(add_weights[A[i]])\n",
    "#add_weights=add_weights[A]\n",
    "\n",
    "# Now trying to generate Streaming settings for the dataset\n",
    "# lets find the outputs from all the \n",
    "mean_model_weights, mean_model_acc, mean_model_loss, mean_model_test_acc, mean_model_test_loss = epsilon_mean_recommendation(recommended_models, data_init)\n",
    "\n",
    "y_pred_total=[]\n",
    "y_pred_uncertainty_total=[]\n",
    "\n",
    "y_pred, y_pred_uncertainty = drift_detection(mean_model_weights, data_init.copy())\n",
    "\n",
    "y_pred_total.append(y_pred)\n",
    "y_pred_uncertainty_total.append(y_pred_uncertainty)\n",
    "\n",
    "true_values.append(data_init.copy()[target_variable])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "787c37f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15625/15625 [==============================] - 7s 450us/step\n",
      "15625/15625 [==============================] - 7s 428us/step\n",
      "15625/15625 [==============================] - 7s 426us/step\n",
      "15625/15625 [==============================] - 7s 437us/step\n",
      "15625/15625 [==============================] - 7s 448us/step\n",
      "3125/3125 [==============================] - 1s 439us/step\n",
      "3125/3125 [==============================] - 1s 442us/step\n",
      "3125/3125 [==============================] - 1s 431us/step\n",
      "3125/3125 [==============================] - 1s 426us/step\n",
      "3125/3125 [==============================] - 1s 428us/step\n",
      "100000\n",
      "One lap done.\n",
      "15625/15625 [==============================] - 7s 458us/step\n",
      "15625/15625 [==============================] - 7s 478us/step\n",
      "15625/15625 [==============================] - 7s 453us/step\n",
      "15625/15625 [==============================] - 7s 455us/step\n",
      "15625/15625 [==============================] - 7s 479us/step\n",
      "3125/3125 [==============================] - 2s 502us/step\n",
      "3125/3125 [==============================] - 1s 462us/step\n",
      "3125/3125 [==============================] - 1s 469us/step\n",
      "3125/3125 [==============================] - 2s 507us/step\n",
      "3125/3125 [==============================] - 2s 485us/step\n",
      "100000\n",
      "One lap done.\n",
      "15625/15625 [==============================] - 7s 466us/step\n",
      "15625/15625 [==============================] - 8s 483us/step\n",
      "15625/15625 [==============================] - 7s 467us/step\n",
      "15625/15625 [==============================] - 8s 480us/step\n",
      "15625/15625 [==============================] - 7s 456us/step\n",
      "3125/3125 [==============================] - 1s 457us/step\n",
      "3125/3125 [==============================] - 1s 453us/step\n",
      "3125/3125 [==============================] - 1s 427us/step\n",
      "3125/3125 [==============================] - 1s 436us/step\n",
      "3125/3125 [==============================] - 1s 445us/step\n",
      "100000\n",
      "One lap done.\n",
      "15625/15625 [==============================] - 7s 448us/step\n",
      "15625/15625 [==============================] - 7s 454us/step\n",
      "15625/15625 [==============================] - 7s 446us/step\n",
      "15625/15625 [==============================] - 7s 474us/step\n",
      "15625/15625 [==============================] - 7s 451us/step\n",
      "3125/3125 [==============================] - 1s 459us/step\n",
      "3125/3125 [==============================] - 1s 443us/step\n",
      "3125/3125 [==============================] - 2s 483us/step\n",
      "3125/3125 [==============================] - 2s 495us/step\n",
      "3125/3125 [==============================] - 2s 486us/step\n",
      "100000\n",
      "One lap done.\n",
      "15625/15625 [==============================] - 7s 473us/step\n",
      "15625/15625 [==============================] - 7s 468us/step\n",
      "15625/15625 [==============================] - 7s 442us/step\n",
      "15625/15625 [==============================] - 7s 462us/step\n",
      "15625/15625 [==============================] - 7s 471us/step\n",
      "3125/3125 [==============================] - 2s 476us/step\n",
      "3125/3125 [==============================] - 1s 455us/step\n",
      "3125/3125 [==============================] - 1s 426us/step\n",
      "3125/3125 [==============================] - 1s 466us/step\n",
      "3125/3125 [==============================] - 1s 459us/step\n",
      "100000\n",
      "One lap done.\n",
      "15625/15625 [==============================] - 7s 474us/step\n",
      "15625/15625 [==============================] - 8s 483us/step\n",
      "15625/15625 [==============================] - 8s 489us/step\n",
      "15625/15625 [==============================] - 7s 454us/step\n",
      "15625/15625 [==============================] - 7s 468us/step\n",
      "3125/3125 [==============================] - 1s 448us/step\n",
      "3125/3125 [==============================] - 1s 445us/step\n",
      "3125/3125 [==============================] - 1s 447us/step\n",
      "3125/3125 [==============================] - 1s 438us/step\n",
      "3125/3125 [==============================] - 1s 422us/step\n",
      "100000\n",
      "One lap done.\n",
      "15625/15625 [==============================] - 8s 487us/step\n",
      "15625/15625 [==============================] - 7s 464us/step\n",
      "15625/15625 [==============================] - 8s 502us/step\n",
      "15625/15625 [==============================] - 8s 484us/step\n",
      "15625/15625 [==============================] - 7s 464us/step\n",
      "3125/3125 [==============================] - 1s 468us/step\n",
      "3125/3125 [==============================] - 1s 453us/step\n",
      "3125/3125 [==============================] - 1s 451us/step\n",
      "3125/3125 [==============================] - 2s 482us/step\n",
      "3125/3125 [==============================] - 1s 474us/step\n",
      "100000\n",
      "One lap done.\n",
      "15625/15625 [==============================] - 7s 476us/step\n",
      "15625/15625 [==============================] - 8s 485us/step\n",
      "15625/15625 [==============================] - 7s 449us/step\n",
      "15625/15625 [==============================] - 7s 436us/step\n",
      "15625/15625 [==============================] - 7s 448us/step\n",
      "3125/3125 [==============================] - 1s 444us/step\n",
      "3125/3125 [==============================] - 1s 451us/step\n",
      "3125/3125 [==============================] - 1s 447us/step\n",
      "3125/3125 [==============================] - 1s 446us/step\n",
      "3125/3125 [==============================] - 1s 446us/step\n",
      "100000\n",
      "One lap done.\n",
      "15625/15625 [==============================] - 7s 438us/step\n",
      "15625/15625 [==============================] - 7s 448us/step\n",
      "15625/15625 [==============================] - 7s 445us/step\n",
      "15625/15625 [==============================] - 7s 443us/step\n",
      "15625/15625 [==============================] - 7s 444us/step\n",
      "3125/3125 [==============================] - 1s 445us/step\n",
      "3125/3125 [==============================] - 1s 443us/step\n",
      "3125/3125 [==============================] - 1s 437us/step\n",
      "3125/3125 [==============================] - 1s 444us/step\n",
      "3125/3125 [==============================] - 1s 445us/step\n",
      "100000\n",
      "One lap done.\n",
      "15625/15625 [==============================] - 7s 443us/step\n",
      "15625/15625 [==============================] - 7s 446us/step\n",
      "15625/15625 [==============================] - 7s 456us/step\n",
      "15625/15625 [==============================] - 7s 438us/step\n",
      "15625/15625 [==============================] - 7s 443us/step\n",
      "3125/3125 [==============================] - 1s 442us/step\n",
      "3125/3125 [==============================] - 1s 451us/step\n",
      "3125/3125 [==============================] - 1s 449us/step\n",
      "3125/3125 [==============================] - 1s 445us/step\n",
      "3125/3125 [==============================] - 1s 451us/step\n",
      "100000\n",
      "One lap done.\n",
      "15625/15625 [==============================] - 7s 445us/step\n",
      "15625/15625 [==============================] - 7s 443us/step\n",
      "15625/15625 [==============================] - 7s 450us/step\n",
      "15625/15625 [==============================] - 7s 447us/step\n",
      "15625/15625 [==============================] - 7s 447us/step\n",
      "3125/3125 [==============================] - 1s 438us/step\n",
      "3125/3125 [==============================] - 1s 445us/step\n",
      "3125/3125 [==============================] - 1s 445us/step\n",
      "3125/3125 [==============================] - 1s 439us/step\n",
      "3125/3125 [==============================] - 1s 449us/step\n",
      "100000\n",
      "One lap done.\n",
      "15625/15625 [==============================] - 7s 444us/step\n",
      "15625/15625 [==============================] - 7s 456us/step\n",
      "15625/15625 [==============================] - 7s 448us/step\n",
      "15625/15625 [==============================] - 7s 443us/step\n",
      "15625/15625 [==============================] - 7s 448us/step\n",
      "3125/3125 [==============================] - 1s 441us/step\n",
      "3125/3125 [==============================] - 1s 457us/step\n",
      "3125/3125 [==============================] - 1s 440us/step\n",
      "3125/3125 [==============================] - 1s 452us/step\n",
      "3125/3125 [==============================] - 1s 451us/step\n",
      "100000\n",
      "One lap done.\n",
      "15625/15625 [==============================] - 7s 449us/step\n",
      "15625/15625 [==============================] - 7s 442us/step\n",
      "15625/15625 [==============================] - 7s 443us/step\n",
      "15625/15625 [==============================] - 7s 445us/step\n",
      "15625/15625 [==============================] - 7s 442us/step\n",
      "3125/3125 [==============================] - 1s 442us/step\n",
      "3125/3125 [==============================] - 1s 446us/step\n",
      "3125/3125 [==============================] - 1s 444us/step\n",
      "3125/3125 [==============================] - 1s 444us/step\n",
      "3125/3125 [==============================] - 1s 442us/step\n",
      "100000\n",
      "One lap done.\n",
      "15625/15625 [==============================] - 7s 442us/step\n",
      "15625/15625 [==============================] - 7s 446us/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15625/15625 [==============================] - 7s 441us/step\n",
      "15625/15625 [==============================] - 7s 430us/step\n",
      "15625/15625 [==============================] - 7s 438us/step\n",
      "3125/3125 [==============================] - 1s 437us/step\n",
      "3125/3125 [==============================] - 1s 434us/step\n",
      "3125/3125 [==============================] - 1s 448us/step\n",
      "3125/3125 [==============================] - 1s 440us/step\n",
      "3125/3125 [==============================] - 1s 435us/step\n",
      "100000\n",
      "One lap done.\n",
      "15625/15625 [==============================] - 7s 434us/step\n",
      "15625/15625 [==============================] - 7s 437us/step\n",
      "15625/15625 [==============================] - 7s 438us/step\n",
      "15625/15625 [==============================] - 7s 446us/step\n",
      "15625/15625 [==============================] - 7s 440us/step\n",
      "3125/3125 [==============================] - 1s 446us/step\n",
      "3125/3125 [==============================] - 1s 417us/step\n",
      "3125/3125 [==============================] - 1s 436us/step\n",
      "3125/3125 [==============================] - 1s 444us/step\n",
      "3125/3125 [==============================] - 1s 433us/step\n",
      "100000\n",
      "One lap done.\n",
      "15625/15625 [==============================] - 7s 433us/step\n",
      "15625/15625 [==============================] - 7s 438us/step\n",
      "15625/15625 [==============================] - 7s 439us/step\n",
      "15625/15625 [==============================] - 7s 416us/step\n",
      "15625/15625 [==============================] - 6s 413us/step\n",
      "3125/3125 [==============================] - 1s 413us/step\n",
      "3125/3125 [==============================] - 1s 412us/step\n",
      "3125/3125 [==============================] - 1s 416us/step\n",
      "3125/3125 [==============================] - 1s 443us/step\n",
      "3125/3125 [==============================] - 1s 425us/step\n",
      "100000\n",
      "One lap done.\n",
      "15625/15625 [==============================] - 6s 370us/step\n",
      "15625/15625 [==============================] - 6s 372us/step\n",
      "15625/15625 [==============================] - 6s 380us/step\n",
      "15625/15625 [==============================] - 6s 371us/step\n",
      "15625/15625 [==============================] - 6s 370us/step\n",
      "3125/3125 [==============================] - 1s 405us/step\n",
      "3125/3125 [==============================] - 1s 408us/step\n",
      "3125/3125 [==============================] - 1s 404us/step\n",
      "3125/3125 [==============================] - 1s 403us/step\n",
      "3125/3125 [==============================] - 1s 408us/step\n",
      "100000\n",
      "One lap done.\n",
      "15625/15625 [==============================] - 6s 413us/step\n",
      "15625/15625 [==============================] - 7s 423us/step\n",
      "15625/15625 [==============================] - 7s 447us/step\n",
      "15625/15625 [==============================] - 7s 419us/step\n",
      "15625/15625 [==============================] - 7s 423us/step\n",
      "3125/3125 [==============================] - 1s 417us/step\n",
      "3125/3125 [==============================] - 1s 419us/step\n",
      "3125/3125 [==============================] - 1s 422us/step\n",
      "3125/3125 [==============================] - 1s 420us/step\n",
      "3125/3125 [==============================] - 1s 417us/step\n",
      "100000\n",
      "One lap done.\n",
      "15625/15625 [==============================] - 7s 414us/step\n",
      "15625/15625 [==============================] - 6s 415us/step\n",
      "15625/15625 [==============================] - 7s 417us/step\n",
      "15625/15625 [==============================] - 7s 419us/step\n",
      "15625/15625 [==============================] - 7s 417us/step\n",
      "3125/3125 [==============================] - 1s 412us/step\n",
      "3125/3125 [==============================] - 1s 421us/step\n",
      "3125/3125 [==============================] - 1s 415us/step\n",
      "3125/3125 [==============================] - 1s 417us/step\n",
      "3125/3125 [==============================] - 1s 417us/step\n",
      "100000\n",
      "One lap done.\n",
      "15625/15625 [==============================] - 7s 415us/step\n",
      "15625/15625 [==============================] - 7s 419us/step\n",
      "15625/15625 [==============================] - 7s 418us/step\n",
      "15625/15625 [==============================] - 7s 429us/step\n",
      "15625/15625 [==============================] - 7s 417us/step\n",
      "3125/3125 [==============================] - 1s 414us/step\n",
      "3125/3125 [==============================] - 1s 415us/step\n",
      "3125/3125 [==============================] - 1s 416us/step\n",
      "3125/3125 [==============================] - 1s 419us/step\n",
      "3125/3125 [==============================] - 1s 416us/step\n",
      "100000\n",
      "One lap done.\n",
      "15625/15625 [==============================] - 6s 411us/step\n",
      "15625/15625 [==============================] - 7s 432us/step\n",
      "15625/15625 [==============================] - 7s 422us/step\n",
      "15625/15625 [==============================] - 7s 444us/step\n",
      "15625/15625 [==============================] - 7s 426us/step\n",
      "3125/3125 [==============================] - 1s 419us/step\n",
      "3125/3125 [==============================] - 1s 418us/step\n",
      "3125/3125 [==============================] - 1s 416us/step\n",
      "3125/3125 [==============================] - 1s 418us/step\n",
      "3125/3125 [==============================] - 1s 433us/step\n",
      "100000\n",
      "One lap done.\n",
      "15625/15625 [==============================] - 7s 420us/step\n",
      "15625/15625 [==============================] - 7s 419us/step\n",
      "15625/15625 [==============================] - 7s 427us/step\n",
      "15625/15625 [==============================] - 7s 420us/step\n",
      "15625/15625 [==============================] - 7s 418us/step\n",
      "3125/3125 [==============================] - 1s 419us/step\n",
      "3125/3125 [==============================] - 1s 420us/step\n",
      "3125/3125 [==============================] - 1s 424us/step\n",
      "3125/3125 [==============================] - 1s 419us/step\n",
      "3125/3125 [==============================] - 1s 415us/step\n",
      "100000\n",
      "One lap done.\n",
      "15625/15625 [==============================] - 6s 414us/step\n",
      "15625/15625 [==============================] - 7s 421us/step\n",
      "15625/15625 [==============================] - 7s 418us/step\n",
      "15625/15625 [==============================] - 7s 418us/step\n",
      "15625/15625 [==============================] - 7s 419us/step\n",
      "3125/3125 [==============================] - 1s 415us/step\n",
      "3125/3125 [==============================] - 1s 418us/step\n",
      "3125/3125 [==============================] - 1s 418us/step\n",
      "3125/3125 [==============================] - 1s 417us/step\n",
      "3125/3125 [==============================] - 1s 421us/step\n",
      "100000\n",
      "One lap done.\n",
      "15625/15625 [==============================] - 6s 413us/step\n",
      "15625/15625 [==============================] - 7s 429us/step\n",
      "15625/15625 [==============================] - 7s 418us/step\n",
      "15625/15625 [==============================] - 7s 419us/step\n",
      "15625/15625 [==============================] - 7s 418us/step\n",
      "3125/3125 [==============================] - 1s 463us/step\n",
      "3125/3125 [==============================] - 1s 416us/step\n",
      "3125/3125 [==============================] - 1s 422us/step\n",
      "3125/3125 [==============================] - 1s 417us/step\n",
      "3125/3125 [==============================] - 1s 415us/step\n",
      "100000\n",
      "One lap done.\n",
      "15625/15625 [==============================] - 6s 370us/step\n",
      "15625/15625 [==============================] - 6s 374us/step\n",
      "15625/15625 [==============================] - 6s 372us/step\n",
      "15625/15625 [==============================] - 6s 376us/step\n",
      "15625/15625 [==============================] - 6s 372us/step\n",
      "3125/3125 [==============================] - 1s 417us/step\n",
      "3125/3125 [==============================] - 1s 419us/step\n",
      "3125/3125 [==============================] - 1s 416us/step\n",
      "3125/3125 [==============================] - 1s 419us/step\n",
      "3125/3125 [==============================] - 1s 423us/step\n",
      "100000\n",
      "One lap done.\n",
      "15625/15625 [==============================] - 6s 414us/step\n",
      "15625/15625 [==============================] - 6s 412us/step\n",
      "15625/15625 [==============================] - 7s 418us/step\n",
      "15625/15625 [==============================] - 7s 417us/step\n",
      "15625/15625 [==============================] - 7s 419us/step\n",
      "3125/3125 [==============================] - 1s 417us/step\n",
      "3125/3125 [==============================] - 1s 422us/step\n",
      "3125/3125 [==============================] - 1s 421us/step\n",
      "3125/3125 [==============================] - 1s 419us/step\n",
      "3125/3125 [==============================] - 1s 416us/step\n",
      "100000\n",
      "One lap done.\n",
      "15625/15625 [==============================] - 6s 372us/step\n",
      "15625/15625 [==============================] - 6s 374us/step\n",
      "15625/15625 [==============================] - 6s 374us/step\n",
      "15625/15625 [==============================] - 6s 375us/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15625/15625 [==============================] - 6s 370us/step\n",
      "3125/3125 [==============================] - 1s 413us/step\n",
      "3125/3125 [==============================] - 1s 417us/step\n",
      "3125/3125 [==============================] - 1s 415us/step\n",
      "3125/3125 [==============================] - 1s 415us/step\n",
      "3125/3125 [==============================] - 1s 418us/step\n",
      "100000\n",
      "One lap done.\n",
      "15625/15625 [==============================] - 6s 411us/step\n",
      "15625/15625 [==============================] - 6s 413us/step\n",
      "15625/15625 [==============================] - 6s 415us/step\n",
      "15625/15625 [==============================] - 7s 426us/step\n",
      "15625/15625 [==============================] - 6s 414us/step\n",
      "3125/3125 [==============================] - 1s 412us/step\n",
      "3125/3125 [==============================] - 1s 413us/step\n",
      "3125/3125 [==============================] - 1s 415us/step\n",
      "3125/3125 [==============================] - 1s 416us/step\n",
      "3125/3125 [==============================] - 1s 417us/step\n",
      "100000\n",
      "One lap done.\n",
      "15625/15625 [==============================] - 6s 411us/step\n",
      "15625/15625 [==============================] - 7s 424us/step\n",
      "15625/15625 [==============================] - 6s 415us/step\n",
      "15625/15625 [==============================] - 7s 417us/step\n",
      "15625/15625 [==============================] - 7s 416us/step\n",
      "3125/3125 [==============================] - 1s 416us/step\n",
      "3125/3125 [==============================] - 1s 416us/step\n",
      "3125/3125 [==============================] - 1s 420us/step\n",
      "3125/3125 [==============================] - 1s 416us/step\n",
      "3125/3125 [==============================] - 1s 415us/step\n",
      "100000\n",
      "One lap done.\n",
      "15625/15625 [==============================] - 6s 368us/step\n",
      "15625/15625 [==============================] - 6s 372us/step\n",
      "15625/15625 [==============================] - 6s 372us/step\n",
      "15625/15625 [==============================] - 6s 372us/step\n",
      "15625/15625 [==============================] - 6s 371us/step\n",
      "3125/3125 [==============================] - 1s 411us/step\n",
      "3125/3125 [==============================] - 1s 415us/step\n",
      "3125/3125 [==============================] - 1s 416us/step\n",
      "3125/3125 [==============================] - 1s 422us/step\n",
      "3125/3125 [==============================] - 1s 416us/step\n",
      "100000\n",
      "One lap done.\n",
      "15625/15625 [==============================] - 6s 378us/step\n",
      "15625/15625 [==============================] - 6s 371us/step\n",
      "15625/15625 [==============================] - 6s 374us/step\n",
      "15625/15625 [==============================] - 6s 371us/step\n",
      "15625/15625 [==============================] - 6s 371us/step\n",
      "3125/3125 [==============================] - 1s 416us/step\n",
      "3125/3125 [==============================] - 1s 412us/step\n",
      "3125/3125 [==============================] - 1s 429us/step\n",
      "3125/3125 [==============================] - 1s 418us/step\n",
      "3125/3125 [==============================] - 1s 419us/step\n",
      "100000\n",
      "One lap done.\n",
      "15625/15625 [==============================] - 6s 371us/step\n",
      "15625/15625 [==============================] - 6s 371us/step\n",
      "15625/15625 [==============================] - 6s 372us/step\n",
      "15625/15625 [==============================] - 6s 372us/step\n",
      "15625/15625 [==============================] - 6s 374us/step\n",
      "3125/3125 [==============================] - 1s 416us/step\n",
      "3125/3125 [==============================] - 1s 419us/step\n",
      "3125/3125 [==============================] - 1s 417us/step\n",
      "3125/3125 [==============================] - 1s 417us/step\n",
      "3125/3125 [==============================] - 1s 427us/step\n",
      "100000\n",
      "One lap done.\n",
      "15625/15625 [==============================] - 7s 423us/step\n",
      "15625/15625 [==============================] - 7s 425us/step\n",
      "15625/15625 [==============================] - 7s 416us/step\n",
      "15625/15625 [==============================] - 7s 427us/step\n",
      "15625/15625 [==============================] - 7s 418us/step\n",
      "3125/3125 [==============================] - 1s 416us/step\n",
      "3125/3125 [==============================] - 1s 415us/step\n",
      "3125/3125 [==============================] - 1s 419us/step\n",
      "3125/3125 [==============================] - 1s 424us/step\n",
      "3125/3125 [==============================] - 1s 420us/step\n",
      "100000\n",
      "One lap done.\n",
      "15625/15625 [==============================] - 6s 413us/step\n",
      "15625/15625 [==============================] - 7s 419us/step\n",
      "15625/15625 [==============================] - 7s 416us/step\n",
      "15625/15625 [==============================] - 7s 418us/step\n",
      "15625/15625 [==============================] - 7s 421us/step\n",
      "3125/3125 [==============================] - 1s 417us/step\n",
      "3125/3125 [==============================] - 1s 415us/step\n",
      "3125/3125 [==============================] - 1s 420us/step\n",
      "3125/3125 [==============================] - 1s 419us/step\n",
      "3125/3125 [==============================] - 1s 415us/step\n",
      "100000\n",
      "One lap done.\n",
      "15625/15625 [==============================] - 7s 415us/step\n",
      "15625/15625 [==============================] - 7s 420us/step\n",
      "15625/15625 [==============================] - 7s 420us/step\n",
      "15625/15625 [==============================] - 7s 421us/step\n",
      "15625/15625 [==============================] - 7s 421us/step\n",
      "3125/3125 [==============================] - 1s 410us/step\n",
      "3125/3125 [==============================] - 1s 418us/step\n",
      "3125/3125 [==============================] - 1s 427us/step\n",
      "3125/3125 [==============================] - 1s 419us/step\n",
      "3125/3125 [==============================] - 1s 418us/step\n",
      "100000\n",
      "One lap done.\n",
      "15625/15625 [==============================] - 7s 415us/step\n",
      "15625/15625 [==============================] - 7s 418us/step\n",
      "15625/15625 [==============================] - 6s 415us/step\n",
      "15625/15625 [==============================] - 7s 417us/step\n",
      "15625/15625 [==============================] - 7s 417us/step\n",
      "3125/3125 [==============================] - 1s 417us/step\n",
      "3125/3125 [==============================] - 1s 417us/step\n",
      "3125/3125 [==============================] - 1s 418us/step\n",
      "3125/3125 [==============================] - 1s 420us/step\n",
      "3125/3125 [==============================] - 1s 420us/step\n",
      "100000\n",
      "One lap done.\n",
      "15625/15625 [==============================] - 7s 422us/step\n",
      "15625/15625 [==============================] - 7s 419us/step\n",
      "15625/15625 [==============================] - 7s 430us/step\n",
      "15625/15625 [==============================] - 7s 419us/step\n",
      "15625/15625 [==============================] - 7s 420us/step\n",
      "3125/3125 [==============================] - 1s 416us/step\n",
      "3125/3125 [==============================] - 1s 431us/step\n",
      "3125/3125 [==============================] - 1s 420us/step\n",
      "3125/3125 [==============================] - 1s 422us/step\n",
      "3125/3125 [==============================] - 1s 419us/step\n",
      "100000\n",
      "One lap done.\n",
      "15625/15625 [==============================] - 6s 381us/step\n",
      "15625/15625 [==============================] - 6s 373us/step\n",
      "15625/15625 [==============================] - 6s 373us/step\n",
      "15625/15625 [==============================] - 6s 373us/step\n",
      "15625/15625 [==============================] - 6s 373us/step\n",
      "3125/3125 [==============================] - 1s 413us/step\n",
      "3125/3125 [==============================] - 1s 416us/step\n",
      "3125/3125 [==============================] - 1s 414us/step\n",
      "3125/3125 [==============================] - 1s 422us/step\n",
      "3125/3125 [==============================] - 1s 416us/step\n",
      "100000\n",
      "One lap done.\n",
      "15625/15625 [==============================] - 6s 368us/step\n",
      "15625/15625 [==============================] - 6s 378us/step\n",
      "15625/15625 [==============================] - 6s 373us/step\n",
      "15625/15625 [==============================] - 6s 372us/step\n",
      "15625/15625 [==============================] - 6s 372us/step\n",
      "3125/3125 [==============================] - 1s 421us/step\n",
      "3125/3125 [==============================] - 1s 419us/step\n",
      "3125/3125 [==============================] - 1s 419us/step\n",
      "3125/3125 [==============================] - 1s 419us/step\n",
      "3125/3125 [==============================] - 1s 423us/step\n",
      "100000\n",
      "One lap done.\n",
      "15625/15625 [==============================] - 7s 415us/step\n",
      "15625/15625 [==============================] - 7s 420us/step\n",
      "15625/15625 [==============================] - 7s 417us/step\n",
      "15625/15625 [==============================] - 7s 439us/step\n",
      "15625/15625 [==============================] - 7s 421us/step\n",
      "3125/3125 [==============================] - 1s 416us/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3125/3125 [==============================] - 1s 416us/step\n",
      "3125/3125 [==============================] - 1s 416us/step\n",
      "3125/3125 [==============================] - 1s 411us/step\n",
      "3125/3125 [==============================] - 1s 419us/step\n",
      "100000\n",
      "One lap done.\n",
      "15625/15625 [==============================] - 6s 367us/step\n",
      "15625/15625 [==============================] - 6s 383us/step\n",
      "15625/15625 [==============================] - 6s 370us/step\n",
      "15625/15625 [==============================] - 6s 371us/step\n",
      "15625/15625 [==============================] - 6s 380us/step\n",
      "3125/3125 [==============================] - 1s 413us/step\n",
      "3125/3125 [==============================] - 1s 415us/step\n",
      "3125/3125 [==============================] - 1s 415us/step\n",
      "3125/3125 [==============================] - 1s 415us/step\n",
      "3125/3125 [==============================] - 1s 415us/step\n",
      "100000\n",
      "One lap done.\n",
      "15625/15625 [==============================] - 6s 367us/step\n",
      "15625/15625 [==============================] - 6s 371us/step\n",
      "15625/15625 [==============================] - 6s 371us/step\n",
      "15625/15625 [==============================] - 6s 373us/step\n",
      "15625/15625 [==============================] - 6s 371us/step\n",
      "3125/3125 [==============================] - 1s 408us/step\n",
      "3125/3125 [==============================] - 1s 413us/step\n",
      "3125/3125 [==============================] - 1s 421us/step\n",
      "3125/3125 [==============================] - 1s 416us/step\n",
      "3125/3125 [==============================] - 1s 418us/step\n",
      "100000\n",
      "One lap done.\n",
      "15625/15625 [==============================] - 6s 410us/step\n",
      "15625/15625 [==============================] - 7s 417us/step\n",
      "15625/15625 [==============================] - 7s 415us/step\n",
      "15625/15625 [==============================] - 7s 416us/step\n",
      "15625/15625 [==============================] - 7s 425us/step\n",
      "3125/3125 [==============================] - 1s 424us/step\n",
      "3125/3125 [==============================] - 1s 415us/step\n",
      "3125/3125 [==============================] - 1s 415us/step\n",
      "3125/3125 [==============================] - 1s 418us/step\n",
      "3125/3125 [==============================] - 1s 420us/step\n",
      "100000\n",
      "One lap done.\n",
      "15625/15625 [==============================] - 6s 410us/step\n",
      "15625/15625 [==============================] - 6s 413us/step\n",
      "15625/15625 [==============================] - 7s 433us/step\n",
      "15625/15625 [==============================] - 6s 415us/step\n",
      "15625/15625 [==============================] - 7s 417us/step\n",
      "3125/3125 [==============================] - 1s 416us/step\n",
      "3125/3125 [==============================] - 1s 417us/step\n",
      "3125/3125 [==============================] - 1s 425us/step\n",
      "3125/3125 [==============================] - 1s 420us/step\n",
      "3125/3125 [==============================] - 1s 417us/step\n",
      "100000\n",
      "One lap done.\n",
      "15625/15625 [==============================] - 6s 367us/step\n",
      "15625/15625 [==============================] - 6s 372us/step\n",
      "15625/15625 [==============================] - 6s 372us/step\n",
      "15625/15625 [==============================] - 6s 372us/step\n",
      "15625/15625 [==============================] - 6s 372us/step\n",
      "3125/3125 [==============================] - 1s 418us/step\n",
      "3125/3125 [==============================] - 1s 416us/step\n",
      "3125/3125 [==============================] - 1s 415us/step\n",
      "3125/3125 [==============================] - 1s 424us/step\n",
      "3125/3125 [==============================] - 1s 415us/step\n",
      "100000\n",
      "One lap done.\n",
      "0\n",
      "0.7729512\n",
      "0.5590794162128297\n",
      "0.7020658799397226\n",
      "0.7582201762640243\n"
     ]
    }
   ],
   "source": [
    "stream_mean_results=[]\n",
    "current_window = data_init.copy()\n",
    "delta=0.01\n",
    "adwin = ADWIN(delta)\n",
    "detected = False\n",
    "retraining_count=0\n",
    "data_window=data_init.copy()\n",
    "\n",
    "\n",
    "\n",
    "num_models=[]\n",
    "while stream.n_remaining_samples()>1:\n",
    "    \n",
    "    y_pred, y_pred_uncertainty = drift_detection(mean_model_weights, data_window.copy())\n",
    "    for i in range(len(y_pred_uncertainty)):\n",
    "        adwin.add_element(y_pred_uncertainty[i])\n",
    "        \n",
    "    incoming_data = stream.next_sample(two_percent)\n",
    "    \n",
    "    data_incoming = pd.DataFrame(incoming_data[0], columns=dataset.columns[:-1])\n",
    "    data_incoming[19]=incoming_data[1]\n",
    "    true_values.append(data_incoming[target_variable])\n",
    "    \n",
    "    y_pred, y_pred_uncertainty = drift_detection(mean_model_weights, data_incoming.copy()) \n",
    "    \n",
    "    y_pred_total.append(y_pred)\n",
    "    y_pred_uncertainty_total.append(y_pred_uncertainty)\n",
    "    print(len(y_pred_uncertainty))\n",
    "    \n",
    "    detected=False\n",
    "    for i in range(len(data_incoming[target_variable])):\n",
    "        adwin.add_element(y_pred_uncertainty[i])\n",
    "        if adwin.detected_change():\n",
    "            detected = True\n",
    "            break;\n",
    "    \n",
    "    if detected:\n",
    "        print(\"drift has been detecte models must be retrained\")\n",
    "        \n",
    "        data_window = update_train_data(data_window, data_incoming)\n",
    "        \n",
    "        retraining_count+=1\n",
    "        \n",
    "        Models=[]\n",
    "        val_acc=[]\n",
    "        train_acc=[]\n",
    "        test_acc=[]\n",
    "        val_loss=[]\n",
    "        train_loss=[]\n",
    "        ind=0\n",
    "        add_weights=[]\n",
    "        \n",
    "        samples = generate_samples(data_window, 50, N)\n",
    "        \n",
    "        while ind<len(samples):\n",
    "            \n",
    "            train_data=samples[ind]\n",
    "            ind+=1\n",
    "            \n",
    "            ann_model = get_initial_model(dataset.shape[1]-1, 2) #same intial weights\n",
    "            ann_model.set_weights(initial_model.get_weights())\n",
    "            X_train=train_data.drop(columns=[target_variable])\n",
    "            print(X_train.shape[1])\n",
    "\n",
    "            y_train=to_categorical(train_data[target_variable])\n",
    "\n",
    "            ann_model.compile(loss='categorical_crossentropy', optimizer='accuracy', metrics=[f1_m]) # metrics=['accuracy']\n",
    "            history = ann_model.fit(X_train, y_train, epochs=10, validation_split=0.2, verbose=1)\n",
    "            print(history.history['f1_m'][-1])\n",
    "\n",
    "\n",
    "\n",
    "            present=False\n",
    "            for i in range(len(Models)):\n",
    "                if (check_models(Models[i][0], ann_model.get_weights())):\n",
    "                  print(\"if any\")\n",
    "                  Models[i][1]=Models[i][1]+1\n",
    "                  add_weights[i].append(ann_model.get_weights())\n",
    "                  val_acc[i].append(history.history['val_f1_m'])\n",
    "                  train_acc[i].append(history.history['f1_m'])\n",
    "                  #test_acc[i].append(f1_m(y_test, pred_test).numpy())\n",
    "                  val_loss[i].append(history.history['val_loss'])\n",
    "                  train_loss[i].append(history.history['loss'])\n",
    "                  present=True\n",
    "                  break;\n",
    "            if present==False:\n",
    "                add_weights.append([ann_model.get_weights()])\n",
    "                Models.append([ann_model.get_weights(), 1])\n",
    "                val_acc.append([history.history['val_f1_m']])\n",
    "                train_acc.append([history.history['f1_m']])\n",
    "                #test_acc.append([f1_m(y_test, pred_test).numpy()])\n",
    "                val_loss.append([history.history['val_loss']])\n",
    "                train_loss.append([history.history['loss']])\n",
    "        \n",
    "        A=np.argsort(np.array(Models).T[1])[::-1][:5]\n",
    "        \n",
    "        num_models.append(len(A))\n",
    "        recommended_models=[]\n",
    "        for i in range(len(A)):\n",
    "            recommended_models.append(add_weights[A[i]])\n",
    "            \n",
    "        mean_model_weights, mean_model_acc, mean_model_loss, mean_model_test_acc, mean_model_test_loss = epsilon_mean_recommendation(recommended_models, data_window)\n",
    "    else:\n",
    "        print(\"One lap done.\")\n",
    "        continue\n",
    "    print(\"one thing\")\n",
    "print(retraining_count)\n",
    "\n",
    "predictions=[]\n",
    "for i in y_pred_total:\n",
    "    predictions.append(list(np.argmax(i,axis=1)))\n",
    "predictions = list(np.concatenate(predictions))\n",
    "y_pred_total[0], len(predictions)\n",
    "\n",
    "true_values = list(np.concatenate(true_values))\n",
    "\n",
    "\n",
    "from sklearn.metrics import log_loss, mean_squared_error, accuracy_score, mean_absolute_error, f1_score, matthews_corrcoef, mean_squared_error, mean_squared_log_error, roc_auc_score\n",
    "\n",
    "# this is for classification\n",
    "acc_score = accuracy_score(true_values, predictions)\n",
    "print(acc_score)\n",
    "mcc = matthews_corrcoef(true_values, predictions)\n",
    "f1_score = f1_score(true_values, predictions)\n",
    "auc_score = roc_auc_score(true_values, predictions)\n",
    "\n",
    "print(mcc)\n",
    "print(f1_score)\n",
    "print(auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "04f4a537",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " ...]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6ad5b94c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished\n"
     ]
    }
   ],
   "source": [
    "from skmultiflow.data import DataStream\n",
    "\n",
    "stream = DataStream(dataset)\n",
    "\n",
    "two_percent = int(stream.n_remaining_samples()*0.02)\n",
    "#five_percent = int(stream.n_remaining_samples()*0.05)\n",
    "initial_data = stream.next_sample(int(stream.n_remaining_samples()*0.1))\n",
    "\n",
    "data_init=pd.DataFrame(initial_data[0], columns=dataset.columns[:-1])\n",
    "data_init[19]=initial_data[1]\n",
    "\n",
    "epsilon = 0.01\n",
    "N = int(data_init.shape[0]*0.10)\n",
    "samples = generate_samples(data_init, 50, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "39f6ef25",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 1s 906us/step - loss: 0.5081 - accuracy: 0.7606 - val_loss: 0.6709 - val_accuracy: 0.6078\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 1s 833us/step - loss: 0.4399 - accuracy: 0.8076 - val_loss: 0.7513 - val_accuracy: 0.5776\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 1s 821us/step - loss: 0.4264 - accuracy: 0.8144 - val_loss: 0.7927 - val_accuracy: 0.5645\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 1s 813us/step - loss: 0.4177 - accuracy: 0.8200 - val_loss: 0.9232 - val_accuracy: 0.5086\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 1s 828us/step - loss: 0.4142 - accuracy: 0.8213 - val_loss: 0.6507 - val_accuracy: 0.6396\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 1s 823us/step - loss: 0.4113 - accuracy: 0.8209 - val_loss: 0.7473 - val_accuracy: 0.5946\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 1s 814us/step - loss: 0.4079 - accuracy: 0.8253 - val_loss: 0.6861 - val_accuracy: 0.6199\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 1s 838us/step - loss: 0.4067 - accuracy: 0.8242 - val_loss: 0.7392 - val_accuracy: 0.5944\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 1s 826us/step - loss: 0.4061 - accuracy: 0.8250 - val_loss: 0.7873 - val_accuracy: 0.5763\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 1s 818us/step - loss: 0.4043 - accuracy: 0.8257 - val_loss: 0.7730 - val_accuracy: 0.5868\n",
      "0.8256750106811523\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 1s 948us/step - loss: 0.5092 - accuracy: 0.7642 - val_loss: 0.7404 - val_accuracy: 0.5797\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 1s 836us/step - loss: 0.4401 - accuracy: 0.8089 - val_loss: 0.5481 - val_accuracy: 0.6792\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 1s 822us/step - loss: 0.4215 - accuracy: 0.8195 - val_loss: 0.7616 - val_accuracy: 0.5814\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 1s 824us/step - loss: 0.4163 - accuracy: 0.8224 - val_loss: 0.7897 - val_accuracy: 0.5695\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 1s 831us/step - loss: 0.4142 - accuracy: 0.8221 - val_loss: 0.6641 - val_accuracy: 0.6293\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 1s 830us/step - loss: 0.4117 - accuracy: 0.8232 - val_loss: 0.6131 - val_accuracy: 0.6558\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 1s 825us/step - loss: 0.4112 - accuracy: 0.8242 - val_loss: 0.6960 - val_accuracy: 0.6192\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 1s 829us/step - loss: 0.4097 - accuracy: 0.8249 - val_loss: 0.7325 - val_accuracy: 0.6027\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 1s 831us/step - loss: 0.4096 - accuracy: 0.8243 - val_loss: 0.6746 - val_accuracy: 0.6275\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 1s 836us/step - loss: 0.4071 - accuracy: 0.8257 - val_loss: 0.9253 - val_accuracy: 0.5323\n",
      "0.8257250189781189\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 1s 891us/step - loss: 0.5138 - accuracy: 0.7585 - val_loss: 0.9811 - val_accuracy: 0.4830\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 1s 833us/step - loss: 0.4430 - accuracy: 0.8073 - val_loss: 0.7049 - val_accuracy: 0.6045\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 1s 850us/step - loss: 0.4253 - accuracy: 0.8181 - val_loss: 0.7152 - val_accuracy: 0.6039\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 1s 861us/step - loss: 0.4182 - accuracy: 0.8205 - val_loss: 0.8434 - val_accuracy: 0.5579\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 1s 831us/step - loss: 0.4122 - accuracy: 0.8247 - val_loss: 0.7122 - val_accuracy: 0.6113\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 1s 835us/step - loss: 0.4094 - accuracy: 0.8256 - val_loss: 0.6349 - val_accuracy: 0.6525\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 1s 832us/step - loss: 0.4073 - accuracy: 0.8251 - val_loss: 0.6033 - val_accuracy: 0.6744\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 1s 831us/step - loss: 0.4045 - accuracy: 0.8266 - val_loss: 0.8309 - val_accuracy: 0.5593\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 1s 844us/step - loss: 0.4028 - accuracy: 0.8270 - val_loss: 0.5790 - val_accuracy: 0.6919\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 1s 837us/step - loss: 0.4019 - accuracy: 0.8282 - val_loss: 0.8210 - val_accuracy: 0.5770\n",
      "0.8281999826431274\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 1s 900us/step - loss: 0.5058 - accuracy: 0.7649 - val_loss: 0.7052 - val_accuracy: 0.6040\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 1s 828us/step - loss: 0.4344 - accuracy: 0.8134 - val_loss: 0.8343 - val_accuracy: 0.5364\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 1s 831us/step - loss: 0.4225 - accuracy: 0.8178 - val_loss: 0.7683 - val_accuracy: 0.5731\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 1s 839us/step - loss: 0.4163 - accuracy: 0.8218 - val_loss: 0.6354 - val_accuracy: 0.6511\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 1s 831us/step - loss: 0.4122 - accuracy: 0.8249 - val_loss: 0.9749 - val_accuracy: 0.5038\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 1s 816us/step - loss: 0.4097 - accuracy: 0.8235 - val_loss: 0.7314 - val_accuracy: 0.6000\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 1s 811us/step - loss: 0.4065 - accuracy: 0.8254 - val_loss: 0.7351 - val_accuracy: 0.6060\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 1s 842us/step - loss: 0.4053 - accuracy: 0.8270 - val_loss: 0.7457 - val_accuracy: 0.6132\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 1s 822us/step - loss: 0.4027 - accuracy: 0.8273 - val_loss: 0.7220 - val_accuracy: 0.6216\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 1s 830us/step - loss: 0.4024 - accuracy: 0.8271 - val_loss: 0.7058 - val_accuracy: 0.6249\n",
      "0.8270999789237976\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 1s 884us/step - loss: 0.5144 - accuracy: 0.7591 - val_loss: 0.5387 - val_accuracy: 0.6962\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 1s 819us/step - loss: 0.4432 - accuracy: 0.8063 - val_loss: 0.9252 - val_accuracy: 0.4943\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 1s 804us/step - loss: 0.4275 - accuracy: 0.8156 - val_loss: 0.7251 - val_accuracy: 0.5862\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 1s 800us/step - loss: 0.4214 - accuracy: 0.8198 - val_loss: 0.8932 - val_accuracy: 0.5293\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 1s 813us/step - loss: 0.4180 - accuracy: 0.8202 - val_loss: 0.6576 - val_accuracy: 0.6261\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 1s 820us/step - loss: 0.4173 - accuracy: 0.8222 - val_loss: 0.8545 - val_accuracy: 0.5481\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 1s 814us/step - loss: 0.4154 - accuracy: 0.8215 - val_loss: 0.7273 - val_accuracy: 0.6028\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 1s 812us/step - loss: 0.4129 - accuracy: 0.8232 - val_loss: 0.5721 - val_accuracy: 0.6761\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 1s 809us/step - loss: 0.4110 - accuracy: 0.8229 - val_loss: 0.7228 - val_accuracy: 0.5911\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 1s 820us/step - loss: 0.4113 - accuracy: 0.8230 - val_loss: 0.7181 - val_accuracy: 0.5948\n",
      "0.8229749798774719\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 1s 899us/step - loss: 0.5052 - accuracy: 0.7642 - val_loss: 0.6407 - val_accuracy: 0.6258\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 1s 850us/step - loss: 0.4358 - accuracy: 0.8115 - val_loss: 0.7149 - val_accuracy: 0.5868\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 1s 820us/step - loss: 0.4221 - accuracy: 0.8198 - val_loss: 0.8097 - val_accuracy: 0.5528\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 1s 822us/step - loss: 0.4161 - accuracy: 0.8218 - val_loss: 0.8204 - val_accuracy: 0.5664\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 1s 821us/step - loss: 0.4132 - accuracy: 0.8237 - val_loss: 0.5797 - val_accuracy: 0.6751\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 1s 821us/step - loss: 0.4110 - accuracy: 0.8246 - val_loss: 0.9058 - val_accuracy: 0.5442\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 1s 817us/step - loss: 0.4052 - accuracy: 0.8273 - val_loss: 0.6084 - val_accuracy: 0.6788\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 1s 807us/step - loss: 0.4043 - accuracy: 0.8276 - val_loss: 0.7241 - val_accuracy: 0.6109\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 1s 810us/step - loss: 0.4029 - accuracy: 0.8264 - val_loss: 0.5631 - val_accuracy: 0.6873\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 1s 806us/step - loss: 0.4021 - accuracy: 0.8266 - val_loss: 0.7305 - val_accuracy: 0.5990\n",
      "0.826574981212616\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 1s 866us/step - loss: 0.5018 - accuracy: 0.7669 - val_loss: 0.7919 - val_accuracy: 0.5500\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 1s 806us/step - loss: 0.4280 - accuracy: 0.8139 - val_loss: 0.8568 - val_accuracy: 0.5289\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 1s 806us/step - loss: 0.4166 - accuracy: 0.8212 - val_loss: 0.8963 - val_accuracy: 0.5408\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 1s 810us/step - loss: 0.4102 - accuracy: 0.8248 - val_loss: 0.7055 - val_accuracy: 0.6126\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 1s 807us/step - loss: 0.4086 - accuracy: 0.8255 - val_loss: 0.8446 - val_accuracy: 0.5584\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 1s 812us/step - loss: 0.4087 - accuracy: 0.8247 - val_loss: 0.8534 - val_accuracy: 0.5544\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 1s 811us/step - loss: 0.4049 - accuracy: 0.8271 - val_loss: 0.7010 - val_accuracy: 0.6227\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 1s 814us/step - loss: 0.4041 - accuracy: 0.8280 - val_loss: 0.9418 - val_accuracy: 0.5199\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 1s 814us/step - loss: 0.4023 - accuracy: 0.8267 - val_loss: 0.6176 - val_accuracy: 0.6588\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 1s 814us/step - loss: 0.4029 - accuracy: 0.8278 - val_loss: 0.8928 - val_accuracy: 0.5532\n",
      "0.827750027179718\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 1s 880us/step - loss: 0.5117 - accuracy: 0.7606 - val_loss: 0.8863 - val_accuracy: 0.5017\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 1s 813us/step - loss: 0.4396 - accuracy: 0.8092 - val_loss: 0.7664 - val_accuracy: 0.5594\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 1s 820us/step - loss: 0.4239 - accuracy: 0.8177 - val_loss: 0.8247 - val_accuracy: 0.5446\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 1s 812us/step - loss: 0.4185 - accuracy: 0.8205 - val_loss: 0.7194 - val_accuracy: 0.6017\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 1s 811us/step - loss: 0.4148 - accuracy: 0.8227 - val_loss: 0.8171 - val_accuracy: 0.5669\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 1s 816us/step - loss: 0.4124 - accuracy: 0.8239 - val_loss: 0.7584 - val_accuracy: 0.5938\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 1s 808us/step - loss: 0.4095 - accuracy: 0.8252 - val_loss: 0.7075 - val_accuracy: 0.6174\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 1s 808us/step - loss: 0.4084 - accuracy: 0.8248 - val_loss: 0.7066 - val_accuracy: 0.6245\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 1s 818us/step - loss: 0.4062 - accuracy: 0.8264 - val_loss: 0.6630 - val_accuracy: 0.6384\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 1s 813us/step - loss: 0.4060 - accuracy: 0.8266 - val_loss: 0.7004 - val_accuracy: 0.6241\n",
      "0.8266000151634216\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 1s 885us/step - loss: 0.5076 - accuracy: 0.7627 - val_loss: 0.6781 - val_accuracy: 0.6172\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 1s 814us/step - loss: 0.4378 - accuracy: 0.8077 - val_loss: 0.4620 - val_accuracy: 0.7402\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 1s 818us/step - loss: 0.4247 - accuracy: 0.8157 - val_loss: 0.8080 - val_accuracy: 0.5662\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 1s 815us/step - loss: 0.4191 - accuracy: 0.8205 - val_loss: 0.6515 - val_accuracy: 0.6343\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 1s 798us/step - loss: 0.4160 - accuracy: 0.8227 - val_loss: 0.7541 - val_accuracy: 0.5856\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 1s 794us/step - loss: 0.4160 - accuracy: 0.8208 - val_loss: 0.7735 - val_accuracy: 0.5792\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 1s 812us/step - loss: 0.4129 - accuracy: 0.8227 - val_loss: 0.7623 - val_accuracy: 0.5906\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 1s 822us/step - loss: 0.4119 - accuracy: 0.8216 - val_loss: 0.9154 - val_accuracy: 0.5217\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 1s 808us/step - loss: 0.4097 - accuracy: 0.8237 - val_loss: 0.6398 - val_accuracy: 0.6487\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 1s 806us/step - loss: 0.4084 - accuracy: 0.8239 - val_loss: 0.8869 - val_accuracy: 0.5524\n",
      "0.8239250183105469\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 1s 878us/step - loss: 0.5090 - accuracy: 0.7641 - val_loss: 0.9264 - val_accuracy: 0.4977\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 1s 820us/step - loss: 0.4370 - accuracy: 0.8113 - val_loss: 0.9395 - val_accuracy: 0.4958\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 1s 820us/step - loss: 0.4180 - accuracy: 0.8215 - val_loss: 0.7207 - val_accuracy: 0.6031\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 1s 812us/step - loss: 0.4133 - accuracy: 0.8222 - val_loss: 0.8014 - val_accuracy: 0.5755\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 1s 825us/step - loss: 0.4098 - accuracy: 0.8249 - val_loss: 0.8642 - val_accuracy: 0.5562\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 1s 824us/step - loss: 0.4076 - accuracy: 0.8257 - val_loss: 0.8029 - val_accuracy: 0.5776\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 1s 818us/step - loss: 0.4062 - accuracy: 0.8252 - val_loss: 0.7749 - val_accuracy: 0.5960\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 1s 820us/step - loss: 0.4056 - accuracy: 0.8260 - val_loss: 0.7297 - val_accuracy: 0.6186\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 1s 818us/step - loss: 0.4057 - accuracy: 0.8254 - val_loss: 0.7629 - val_accuracy: 0.6023\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 1s 820us/step - loss: 0.4056 - accuracy: 0.8263 - val_loss: 0.6014 - val_accuracy: 0.6765\n",
      "0.8263000249862671\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 1s 888us/step - loss: 0.5055 - accuracy: 0.7661 - val_loss: 0.7130 - val_accuracy: 0.6007\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 1s 811us/step - loss: 0.4327 - accuracy: 0.8110 - val_loss: 0.8124 - val_accuracy: 0.5501\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 1s 819us/step - loss: 0.4170 - accuracy: 0.8207 - val_loss: 0.5409 - val_accuracy: 0.6996\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 1s 822us/step - loss: 0.4122 - accuracy: 0.8227 - val_loss: 0.6445 - val_accuracy: 0.6425\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 1s 806us/step - loss: 0.4085 - accuracy: 0.8247 - val_loss: 0.6303 - val_accuracy: 0.6551\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 1s 829us/step - loss: 0.4062 - accuracy: 0.8275 - val_loss: 0.7677 - val_accuracy: 0.5830\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 1s 819us/step - loss: 0.4058 - accuracy: 0.8262 - val_loss: 0.8124 - val_accuracy: 0.5707\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 1s 816us/step - loss: 0.4040 - accuracy: 0.8270 - val_loss: 0.7926 - val_accuracy: 0.5717\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 1s 842us/step - loss: 0.4030 - accuracy: 0.8277 - val_loss: 0.6788 - val_accuracy: 0.6264\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 1s 814us/step - loss: 0.4029 - accuracy: 0.8274 - val_loss: 0.7065 - val_accuracy: 0.6149\n",
      "0.8274250030517578\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250/1250 [==============================] - 1s 872us/step - loss: 0.5155 - accuracy: 0.7586 - val_loss: 0.7907 - val_accuracy: 0.5411\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 1s 811us/step - loss: 0.4449 - accuracy: 0.8072 - val_loss: 0.7699 - val_accuracy: 0.5692\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 1s 805us/step - loss: 0.4289 - accuracy: 0.8161 - val_loss: 0.8767 - val_accuracy: 0.5181\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 1s 804us/step - loss: 0.4241 - accuracy: 0.8198 - val_loss: 0.7713 - val_accuracy: 0.5734\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 1s 805us/step - loss: 0.4201 - accuracy: 0.8199 - val_loss: 0.6821 - val_accuracy: 0.6183\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 1s 802us/step - loss: 0.4169 - accuracy: 0.8226 - val_loss: 0.6063 - val_accuracy: 0.6533\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 1s 804us/step - loss: 0.4130 - accuracy: 0.8243 - val_loss: 0.6559 - val_accuracy: 0.6283\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 1s 802us/step - loss: 0.4112 - accuracy: 0.8239 - val_loss: 0.7290 - val_accuracy: 0.5943\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 1s 809us/step - loss: 0.4072 - accuracy: 0.8269 - val_loss: 0.9534 - val_accuracy: 0.5168\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 1s 806us/step - loss: 0.4064 - accuracy: 0.8263 - val_loss: 0.7348 - val_accuracy: 0.5965\n",
      "0.8262749910354614\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 1s 878us/step - loss: 0.5109 - accuracy: 0.7608 - val_loss: 0.7916 - val_accuracy: 0.5461\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 1s 806us/step - loss: 0.4384 - accuracy: 0.8083 - val_loss: 0.7041 - val_accuracy: 0.5989\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 1s 809us/step - loss: 0.4234 - accuracy: 0.8168 - val_loss: 0.7999 - val_accuracy: 0.5698\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 1s 820us/step - loss: 0.4189 - accuracy: 0.8206 - val_loss: 0.6360 - val_accuracy: 0.6414\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 1s 809us/step - loss: 0.4152 - accuracy: 0.8221 - val_loss: 0.5479 - val_accuracy: 0.6926\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 1s 811us/step - loss: 0.4137 - accuracy: 0.8233 - val_loss: 0.8295 - val_accuracy: 0.5592\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 1s 795us/step - loss: 0.4106 - accuracy: 0.8257 - val_loss: 1.0132 - val_accuracy: 0.5031\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 1s 806us/step - loss: 0.4100 - accuracy: 0.8258 - val_loss: 0.6908 - val_accuracy: 0.6218\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 1s 802us/step - loss: 0.4081 - accuracy: 0.8245 - val_loss: 0.8950 - val_accuracy: 0.5408\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 1s 810us/step - loss: 0.4076 - accuracy: 0.8267 - val_loss: 0.8069 - val_accuracy: 0.5785\n",
      "0.8266500234603882\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 1s 891us/step - loss: 0.5097 - accuracy: 0.7613 - val_loss: 0.7483 - val_accuracy: 0.5793\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 1s 808us/step - loss: 0.4395 - accuracy: 0.8082 - val_loss: 0.8800 - val_accuracy: 0.5282\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 1s 809us/step - loss: 0.4235 - accuracy: 0.8177 - val_loss: 0.8227 - val_accuracy: 0.5500\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 1s 802us/step - loss: 0.4188 - accuracy: 0.8212 - val_loss: 0.9482 - val_accuracy: 0.5141\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 1s 810us/step - loss: 0.4163 - accuracy: 0.8230 - val_loss: 0.6774 - val_accuracy: 0.6250\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 1s 821us/step - loss: 0.4113 - accuracy: 0.8237 - val_loss: 0.9566 - val_accuracy: 0.5141\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 1s 814us/step - loss: 0.4091 - accuracy: 0.8245 - val_loss: 0.7069 - val_accuracy: 0.6145\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 1s 805us/step - loss: 0.4091 - accuracy: 0.8235 - val_loss: 0.6573 - val_accuracy: 0.6453\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 1s 818us/step - loss: 0.4066 - accuracy: 0.8251 - val_loss: 0.8759 - val_accuracy: 0.5494\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 1s 808us/step - loss: 0.4057 - accuracy: 0.8254 - val_loss: 0.6782 - val_accuracy: 0.6305\n",
      "0.8253999948501587\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 1s 880us/step - loss: 0.5056 - accuracy: 0.7634 - val_loss: 0.9170 - val_accuracy: 0.4935\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 1s 811us/step - loss: 0.4382 - accuracy: 0.8088 - val_loss: 0.8094 - val_accuracy: 0.5419\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 1s 812us/step - loss: 0.4229 - accuracy: 0.8167 - val_loss: 0.8629 - val_accuracy: 0.5411\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 1s 808us/step - loss: 0.4162 - accuracy: 0.8210 - val_loss: 0.5566 - val_accuracy: 0.6890\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 1s 810us/step - loss: 0.4129 - accuracy: 0.8230 - val_loss: 0.7388 - val_accuracy: 0.5896\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 1s 812us/step - loss: 0.4097 - accuracy: 0.8233 - val_loss: 0.7826 - val_accuracy: 0.5827\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 1s 810us/step - loss: 0.4078 - accuracy: 0.8242 - val_loss: 0.8050 - val_accuracy: 0.5686\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 1s 811us/step - loss: 0.4056 - accuracy: 0.8244 - val_loss: 0.7614 - val_accuracy: 0.5840\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 1s 818us/step - loss: 0.4046 - accuracy: 0.8262 - val_loss: 0.6487 - val_accuracy: 0.6466\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 1s 808us/step - loss: 0.4032 - accuracy: 0.8250 - val_loss: 0.7725 - val_accuracy: 0.5903\n",
      "0.824999988079071\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 1s 920us/step - loss: 0.5065 - accuracy: 0.7623 - val_loss: 0.6604 - val_accuracy: 0.6245\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 1s 808us/step - loss: 0.4384 - accuracy: 0.8074 - val_loss: 0.7402 - val_accuracy: 0.5870\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 1s 828us/step - loss: 0.4254 - accuracy: 0.8156 - val_loss: 0.7501 - val_accuracy: 0.5902\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 1s 804us/step - loss: 0.4193 - accuracy: 0.8195 - val_loss: 0.9114 - val_accuracy: 0.5328\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 1s 811us/step - loss: 0.4142 - accuracy: 0.8225 - val_loss: 0.7232 - val_accuracy: 0.6112\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 1s 809us/step - loss: 0.4127 - accuracy: 0.8223 - val_loss: 0.5738 - val_accuracy: 0.6804\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 1s 812us/step - loss: 0.4093 - accuracy: 0.8238 - val_loss: 0.9112 - val_accuracy: 0.5431\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 1s 815us/step - loss: 0.4079 - accuracy: 0.8244 - val_loss: 0.7961 - val_accuracy: 0.5787\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 1s 816us/step - loss: 0.4068 - accuracy: 0.8248 - val_loss: 0.6351 - val_accuracy: 0.6542\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 1s 809us/step - loss: 0.4045 - accuracy: 0.8265 - val_loss: 0.7601 - val_accuracy: 0.5984\n",
      "0.8265249729156494\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 1s 886us/step - loss: 0.5108 - accuracy: 0.7602 - val_loss: 0.6080 - val_accuracy: 0.6596\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 1s 821us/step - loss: 0.4437 - accuracy: 0.8077 - val_loss: 0.7952 - val_accuracy: 0.5565\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 1s 814us/step - loss: 0.4265 - accuracy: 0.8172 - val_loss: 0.7985 - val_accuracy: 0.5668\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 1s 826us/step - loss: 0.4179 - accuracy: 0.8207 - val_loss: 0.6894 - val_accuracy: 0.6180\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 1s 851us/step - loss: 0.4132 - accuracy: 0.8233 - val_loss: 0.6251 - val_accuracy: 0.6578\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 1s 813us/step - loss: 0.4108 - accuracy: 0.8242 - val_loss: 0.6108 - val_accuracy: 0.6676\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 1s 798us/step - loss: 0.4081 - accuracy: 0.8254 - val_loss: 0.6035 - val_accuracy: 0.6730\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 1s 792us/step - loss: 0.4057 - accuracy: 0.8267 - val_loss: 0.8518 - val_accuracy: 0.5496\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 1s 801us/step - loss: 0.4067 - accuracy: 0.8245 - val_loss: 0.6346 - val_accuracy: 0.6594\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 1s 809us/step - loss: 0.4043 - accuracy: 0.8267 - val_loss: 0.5456 - val_accuracy: 0.7042\n",
      "0.82669997215271\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 1s 874us/step - loss: 0.5133 - accuracy: 0.7577 - val_loss: 0.8858 - val_accuracy: 0.4954\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 1s 799us/step - loss: 0.4459 - accuracy: 0.8038 - val_loss: 0.7924 - val_accuracy: 0.5553\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 1s 808us/step - loss: 0.4273 - accuracy: 0.8140 - val_loss: 0.6667 - val_accuracy: 0.6128\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 1s 806us/step - loss: 0.4235 - accuracy: 0.8168 - val_loss: 0.6938 - val_accuracy: 0.6114\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 1s 867us/step - loss: 0.4188 - accuracy: 0.8209 - val_loss: 0.8088 - val_accuracy: 0.5619\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 1s 932us/step - loss: 0.4180 - accuracy: 0.8203 - val_loss: 0.8614 - val_accuracy: 0.5350\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 1s 862us/step - loss: 0.4175 - accuracy: 0.8209 - val_loss: 0.6859 - val_accuracy: 0.6216\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 1s 874us/step - loss: 0.4150 - accuracy: 0.8226 - val_loss: 0.7058 - val_accuracy: 0.6150\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 1s 970us/step - loss: 0.4133 - accuracy: 0.8219 - val_loss: 0.8302 - val_accuracy: 0.5611\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 1s 872us/step - loss: 0.4112 - accuracy: 0.8242 - val_loss: 0.7159 - val_accuracy: 0.6001\n",
      "0.8242499828338623\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 2s 1ms/step - loss: 0.5168 - accuracy: 0.7563 - val_loss: 0.8842 - val_accuracy: 0.5048\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 1s 981us/step - loss: 0.4433 - accuracy: 0.8062 - val_loss: 0.6324 - val_accuracy: 0.6303\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 1s 862us/step - loss: 0.4257 - accuracy: 0.8153 - val_loss: 0.7050 - val_accuracy: 0.6085\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 1s 815us/step - loss: 0.4196 - accuracy: 0.8169 - val_loss: 0.7144 - val_accuracy: 0.5973\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 1s 857us/step - loss: 0.4185 - accuracy: 0.8198 - val_loss: 0.6640 - val_accuracy: 0.6311\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 1s 868us/step - loss: 0.4159 - accuracy: 0.8217 - val_loss: 0.8787 - val_accuracy: 0.5390\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 1s 845us/step - loss: 0.4150 - accuracy: 0.8205 - val_loss: 0.7210 - val_accuracy: 0.6014\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 1s 850us/step - loss: 0.4131 - accuracy: 0.8217 - val_loss: 0.7504 - val_accuracy: 0.5888\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 1s 875us/step - loss: 0.4120 - accuracy: 0.8216 - val_loss: 0.6492 - val_accuracy: 0.6425\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 1s 841us/step - loss: 0.4100 - accuracy: 0.8228 - val_loss: 0.7347 - val_accuracy: 0.5974\n",
      "0.822825014591217\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 1s 914us/step - loss: 0.5082 - accuracy: 0.7613 - val_loss: 0.7822 - val_accuracy: 0.5591\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 1s 858us/step - loss: 0.4446 - accuracy: 0.8057 - val_loss: 0.5802 - val_accuracy: 0.6618\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 0.4265 - accuracy: 0.8167 - val_loss: 0.6949 - val_accuracy: 0.6037\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 0.4207 - accuracy: 0.8193 - val_loss: 0.7030 - val_accuracy: 0.6017\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 0.4167 - accuracy: 0.8213 - val_loss: 0.8987 - val_accuracy: 0.5321\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 2s 1ms/step - loss: 0.4149 - accuracy: 0.8211 - val_loss: 0.6947 - val_accuracy: 0.6156\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 1s 928us/step - loss: 0.4127 - accuracy: 0.8230 - val_loss: 0.6172 - val_accuracy: 0.6602\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 1s 926us/step - loss: 0.4111 - accuracy: 0.8240 - val_loss: 0.6732 - val_accuracy: 0.6306\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 1s 864us/step - loss: 0.4093 - accuracy: 0.8248 - val_loss: 0.5699 - val_accuracy: 0.6902\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 1s 815us/step - loss: 0.4089 - accuracy: 0.8262 - val_loss: 0.6352 - val_accuracy: 0.6556\n",
      "0.8262249827384949\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 1s 886us/step - loss: 0.5122 - accuracy: 0.7591 - val_loss: 0.8114 - val_accuracy: 0.5413\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 1s 838us/step - loss: 0.4417 - accuracy: 0.8065 - val_loss: 0.6981 - val_accuracy: 0.5971\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 1s 836us/step - loss: 0.4260 - accuracy: 0.8159 - val_loss: 0.7043 - val_accuracy: 0.6059\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 1s 820us/step - loss: 0.4201 - accuracy: 0.8196 - val_loss: 0.6286 - val_accuracy: 0.6514\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 1s 847us/step - loss: 0.4173 - accuracy: 0.8213 - val_loss: 0.7865 - val_accuracy: 0.5684\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 1s 800us/step - loss: 0.4155 - accuracy: 0.8209 - val_loss: 0.7071 - val_accuracy: 0.6097\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 1s 790us/step - loss: 0.4136 - accuracy: 0.8223 - val_loss: 0.7829 - val_accuracy: 0.5811\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 1s 795us/step - loss: 0.4106 - accuracy: 0.8244 - val_loss: 0.7427 - val_accuracy: 0.5949\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 1s 800us/step - loss: 0.4098 - accuracy: 0.8236 - val_loss: 0.6031 - val_accuracy: 0.6671\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 1s 808us/step - loss: 0.4084 - accuracy: 0.8245 - val_loss: 0.6091 - val_accuracy: 0.6724\n",
      "0.824524998664856\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 1s 868us/step - loss: 0.5058 - accuracy: 0.7650 - val_loss: 0.7012 - val_accuracy: 0.5993\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 1s 800us/step - loss: 0.4363 - accuracy: 0.8111 - val_loss: 0.7991 - val_accuracy: 0.5481\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 1s 819us/step - loss: 0.4239 - accuracy: 0.8189 - val_loss: 0.8318 - val_accuracy: 0.5439\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 1s 820us/step - loss: 0.4175 - accuracy: 0.8200 - val_loss: 0.6647 - val_accuracy: 0.6182\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 1s 798us/step - loss: 0.4147 - accuracy: 0.8226 - val_loss: 0.6901 - val_accuracy: 0.6213\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 1s 797us/step - loss: 0.4114 - accuracy: 0.8259 - val_loss: 0.5818 - val_accuracy: 0.6710\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 1s 814us/step - loss: 0.4093 - accuracy: 0.8257 - val_loss: 0.6674 - val_accuracy: 0.6275\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 1s 805us/step - loss: 0.4087 - accuracy: 0.8250 - val_loss: 0.9146 - val_accuracy: 0.5342\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 1s 827us/step - loss: 0.4066 - accuracy: 0.8262 - val_loss: 0.7517 - val_accuracy: 0.5992\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 1s 812us/step - loss: 0.4059 - accuracy: 0.8269 - val_loss: 0.5166 - val_accuracy: 0.7094\n",
      "0.8269000053405762\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250/1250 [==============================] - 1s 892us/step - loss: 0.5197 - accuracy: 0.7539 - val_loss: 0.8250 - val_accuracy: 0.5442\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 1s 796us/step - loss: 0.4472 - accuracy: 0.8037 - val_loss: 0.6559 - val_accuracy: 0.6261\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 1s 800us/step - loss: 0.4279 - accuracy: 0.8141 - val_loss: 0.6571 - val_accuracy: 0.6221\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 1s 805us/step - loss: 0.4203 - accuracy: 0.8177 - val_loss: 0.7058 - val_accuracy: 0.6079\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 1s 805us/step - loss: 0.4155 - accuracy: 0.8209 - val_loss: 0.6523 - val_accuracy: 0.6361\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 1s 796us/step - loss: 0.4135 - accuracy: 0.8206 - val_loss: 0.7436 - val_accuracy: 0.6019\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 1s 798us/step - loss: 0.4116 - accuracy: 0.8228 - val_loss: 0.7142 - val_accuracy: 0.6147\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 1s 800us/step - loss: 0.4101 - accuracy: 0.8232 - val_loss: 0.8294 - val_accuracy: 0.5674\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 1s 816us/step - loss: 0.4088 - accuracy: 0.8236 - val_loss: 0.6838 - val_accuracy: 0.6340\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 1s 804us/step - loss: 0.4078 - accuracy: 0.8241 - val_loss: 0.6087 - val_accuracy: 0.6684\n",
      "0.8240500092506409\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 1s 873us/step - loss: 0.5013 - accuracy: 0.7662 - val_loss: 0.7205 - val_accuracy: 0.5892\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 1s 800us/step - loss: 0.4328 - accuracy: 0.8128 - val_loss: 0.7399 - val_accuracy: 0.5874\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 1s 817us/step - loss: 0.4203 - accuracy: 0.8192 - val_loss: 0.5238 - val_accuracy: 0.7053\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 1s 809us/step - loss: 0.4136 - accuracy: 0.8217 - val_loss: 0.7875 - val_accuracy: 0.5690\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 1s 835us/step - loss: 0.4115 - accuracy: 0.8227 - val_loss: 0.7880 - val_accuracy: 0.5819\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 1s 800us/step - loss: 0.4086 - accuracy: 0.8241 - val_loss: 0.6341 - val_accuracy: 0.6485\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 1s 797us/step - loss: 0.4064 - accuracy: 0.8238 - val_loss: 0.6526 - val_accuracy: 0.6385\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 1s 800us/step - loss: 0.4043 - accuracy: 0.8262 - val_loss: 0.8832 - val_accuracy: 0.5332\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 1s 804us/step - loss: 0.4027 - accuracy: 0.8272 - val_loss: 0.7138 - val_accuracy: 0.6083\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 1s 800us/step - loss: 0.4022 - accuracy: 0.8269 - val_loss: 0.8409 - val_accuracy: 0.5613\n",
      "0.8269249796867371\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 2s 1ms/step - loss: 0.5028 - accuracy: 0.7666 - val_loss: 0.7589 - val_accuracy: 0.5613\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 1s 801us/step - loss: 0.4355 - accuracy: 0.8132 - val_loss: 0.7109 - val_accuracy: 0.5997\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 1s 815us/step - loss: 0.4217 - accuracy: 0.8197 - val_loss: 0.8879 - val_accuracy: 0.5282\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 1s 797us/step - loss: 0.4162 - accuracy: 0.8226 - val_loss: 0.8547 - val_accuracy: 0.5414\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 1s 799us/step - loss: 0.4111 - accuracy: 0.8246 - val_loss: 0.5060 - val_accuracy: 0.7166\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 1s 798us/step - loss: 0.4088 - accuracy: 0.8260 - val_loss: 0.5697 - val_accuracy: 0.6873\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 1s 838us/step - loss: 0.4088 - accuracy: 0.8260 - val_loss: 0.8329 - val_accuracy: 0.5514\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 1s 785us/step - loss: 0.4070 - accuracy: 0.8266 - val_loss: 0.8482 - val_accuracy: 0.5519\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 1s 784us/step - loss: 0.4072 - accuracy: 0.8267 - val_loss: 0.6274 - val_accuracy: 0.6556\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 1s 790us/step - loss: 0.4060 - accuracy: 0.8259 - val_loss: 0.6492 - val_accuracy: 0.6441\n",
      "0.8258749842643738\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 1s 881us/step - loss: 0.5174 - accuracy: 0.7566 - val_loss: 1.0051 - val_accuracy: 0.4724\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 1s 786us/step - loss: 0.4447 - accuracy: 0.8036 - val_loss: 0.8839 - val_accuracy: 0.5235\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 1s 782us/step - loss: 0.4278 - accuracy: 0.8156 - val_loss: 0.8859 - val_accuracy: 0.5257\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 1s 784us/step - loss: 0.4189 - accuracy: 0.8200 - val_loss: 0.6138 - val_accuracy: 0.6598\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 1s 786us/step - loss: 0.4133 - accuracy: 0.8217 - val_loss: 0.7316 - val_accuracy: 0.6054\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 1s 784us/step - loss: 0.4084 - accuracy: 0.8244 - val_loss: 0.6712 - val_accuracy: 0.6340\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 1s 783us/step - loss: 0.4076 - accuracy: 0.8243 - val_loss: 0.7187 - val_accuracy: 0.6176\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 1s 784us/step - loss: 0.4046 - accuracy: 0.8252 - val_loss: 0.8311 - val_accuracy: 0.5788\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 1s 785us/step - loss: 0.4050 - accuracy: 0.8250 - val_loss: 0.7682 - val_accuracy: 0.5927\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 1s 814us/step - loss: 0.4033 - accuracy: 0.8254 - val_loss: 0.9721 - val_accuracy: 0.5254\n",
      "0.8253999948501587\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 1s 882us/step - loss: 0.5042 - accuracy: 0.7649 - val_loss: 0.8199 - val_accuracy: 0.5452\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 1s 789us/step - loss: 0.4362 - accuracy: 0.8102 - val_loss: 0.9654 - val_accuracy: 0.5018\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 1s 792us/step - loss: 0.4217 - accuracy: 0.8190 - val_loss: 0.7196 - val_accuracy: 0.6007\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 1s 787us/step - loss: 0.4189 - accuracy: 0.8200 - val_loss: 0.8498 - val_accuracy: 0.5391\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 1s 791us/step - loss: 0.4131 - accuracy: 0.8234 - val_loss: 0.7028 - val_accuracy: 0.6132\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 1s 790us/step - loss: 0.4103 - accuracy: 0.8236 - val_loss: 0.5992 - val_accuracy: 0.6722\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 1s 871us/step - loss: 0.4084 - accuracy: 0.8240 - val_loss: 0.9706 - val_accuracy: 0.4989\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 1s 844us/step - loss: 0.4060 - accuracy: 0.8253 - val_loss: 0.9171 - val_accuracy: 0.5243\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 1s 790us/step - loss: 0.4045 - accuracy: 0.8256 - val_loss: 0.6956 - val_accuracy: 0.6226\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 1s 789us/step - loss: 0.4036 - accuracy: 0.8264 - val_loss: 0.6186 - val_accuracy: 0.6554\n",
      "0.8263999819755554\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 1s 872us/step - loss: 0.5094 - accuracy: 0.7630 - val_loss: 0.8588 - val_accuracy: 0.5268\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 1s 789us/step - loss: 0.4357 - accuracy: 0.8108 - val_loss: 1.0382 - val_accuracy: 0.4721\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 1s 793us/step - loss: 0.4183 - accuracy: 0.8210 - val_loss: 1.0759 - val_accuracy: 0.4722\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 1s 795us/step - loss: 0.4153 - accuracy: 0.8220 - val_loss: 0.7366 - val_accuracy: 0.5899\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 1s 797us/step - loss: 0.4103 - accuracy: 0.8255 - val_loss: 0.8362 - val_accuracy: 0.5470\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 1s 789us/step - loss: 0.4094 - accuracy: 0.8241 - val_loss: 0.6511 - val_accuracy: 0.6415\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 1s 815us/step - loss: 0.4075 - accuracy: 0.8260 - val_loss: 0.7271 - val_accuracy: 0.5949\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 1s 791us/step - loss: 0.4080 - accuracy: 0.8260 - val_loss: 0.6562 - val_accuracy: 0.6320\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 1s 791us/step - loss: 0.4053 - accuracy: 0.8284 - val_loss: 0.8710 - val_accuracy: 0.5449\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 1s 795us/step - loss: 0.4062 - accuracy: 0.8276 - val_loss: 0.6983 - val_accuracy: 0.6131\n",
      "0.8275750279426575\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 1s 861us/step - loss: 0.5112 - accuracy: 0.7606 - val_loss: 0.7000 - val_accuracy: 0.5940\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 1s 795us/step - loss: 0.4440 - accuracy: 0.8050 - val_loss: 0.6726 - val_accuracy: 0.6036\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 1s 785us/step - loss: 0.4296 - accuracy: 0.8149 - val_loss: 0.8674 - val_accuracy: 0.5315\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 1s 787us/step - loss: 0.4216 - accuracy: 0.8208 - val_loss: 0.9124 - val_accuracy: 0.5092\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 1s 810us/step - loss: 0.4182 - accuracy: 0.8211 - val_loss: 0.8836 - val_accuracy: 0.5329\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 1s 789us/step - loss: 0.4151 - accuracy: 0.8231 - val_loss: 0.6811 - val_accuracy: 0.6132\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 1s 790us/step - loss: 0.4141 - accuracy: 0.8225 - val_loss: 0.8492 - val_accuracy: 0.5400\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 1s 794us/step - loss: 0.4109 - accuracy: 0.8255 - val_loss: 0.8587 - val_accuracy: 0.5452\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 1s 790us/step - loss: 0.4109 - accuracy: 0.8244 - val_loss: 0.6397 - val_accuracy: 0.6456\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 1s 793us/step - loss: 0.4085 - accuracy: 0.8260 - val_loss: 0.9307 - val_accuracy: 0.5062\n",
      "0.825950026512146\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 1s 874us/step - loss: 0.5099 - accuracy: 0.7613 - val_loss: 0.8320 - val_accuracy: 0.5346\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 1s 788us/step - loss: 0.4433 - accuracy: 0.8055 - val_loss: 0.8418 - val_accuracy: 0.5306\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 1s 797us/step - loss: 0.4268 - accuracy: 0.8164 - val_loss: 0.7803 - val_accuracy: 0.5710\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 1s 807us/step - loss: 0.4189 - accuracy: 0.8190 - val_loss: 0.8001 - val_accuracy: 0.5642\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 1s 793us/step - loss: 0.4152 - accuracy: 0.8218 - val_loss: 0.7062 - val_accuracy: 0.6182\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 1s 788us/step - loss: 0.4133 - accuracy: 0.8227 - val_loss: 0.6545 - val_accuracy: 0.6439\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 1s 792us/step - loss: 0.4118 - accuracy: 0.8231 - val_loss: 0.7293 - val_accuracy: 0.6103\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 1s 788us/step - loss: 0.4118 - accuracy: 0.8239 - val_loss: 0.7149 - val_accuracy: 0.6252\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 1s 788us/step - loss: 0.4074 - accuracy: 0.8258 - val_loss: 0.6959 - val_accuracy: 0.6290\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 1s 796us/step - loss: 0.4078 - accuracy: 0.8246 - val_loss: 0.8028 - val_accuracy: 0.5848\n",
      "0.8245750069618225\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 1s 865us/step - loss: 0.5113 - accuracy: 0.7603 - val_loss: 0.8885 - val_accuracy: 0.5050\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 1s 795us/step - loss: 0.4454 - accuracy: 0.8062 - val_loss: 0.5734 - val_accuracy: 0.6706\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 1s 809us/step - loss: 0.4247 - accuracy: 0.8179 - val_loss: 1.0002 - val_accuracy: 0.4818\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 1s 791us/step - loss: 0.4199 - accuracy: 0.8220 - val_loss: 0.5399 - val_accuracy: 0.6959\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 1s 802us/step - loss: 0.4161 - accuracy: 0.8225 - val_loss: 0.5357 - val_accuracy: 0.7011\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 1s 798us/step - loss: 0.4132 - accuracy: 0.8235 - val_loss: 0.7680 - val_accuracy: 0.5768\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 1s 788us/step - loss: 0.4132 - accuracy: 0.8246 - val_loss: 1.0746 - val_accuracy: 0.4705\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 1s 790us/step - loss: 0.4123 - accuracy: 0.8262 - val_loss: 0.7292 - val_accuracy: 0.6040\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 1s 785us/step - loss: 0.4102 - accuracy: 0.8256 - val_loss: 0.7353 - val_accuracy: 0.6014\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 1s 787us/step - loss: 0.4105 - accuracy: 0.8250 - val_loss: 0.7099 - val_accuracy: 0.6175\n",
      "0.8250250220298767\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 1s 876us/step - loss: 0.5105 - accuracy: 0.7611 - val_loss: 0.7722 - val_accuracy: 0.5640\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 1s 786us/step - loss: 0.4434 - accuracy: 0.8056 - val_loss: 0.7373 - val_accuracy: 0.5730\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 1s 789us/step - loss: 0.4252 - accuracy: 0.8166 - val_loss: 0.8087 - val_accuracy: 0.5579\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 1s 800us/step - loss: 0.4176 - accuracy: 0.8203 - val_loss: 0.6722 - val_accuracy: 0.6329\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 1s 784us/step - loss: 0.4153 - accuracy: 0.8217 - val_loss: 0.9340 - val_accuracy: 0.5268\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 1s 792us/step - loss: 0.4131 - accuracy: 0.8228 - val_loss: 0.7458 - val_accuracy: 0.5946\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 1s 916us/step - loss: 0.4110 - accuracy: 0.8245 - val_loss: 0.5697 - val_accuracy: 0.6768\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 1s 934us/step - loss: 0.4104 - accuracy: 0.8232 - val_loss: 0.7103 - val_accuracy: 0.6167\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 1s 870us/step - loss: 0.4088 - accuracy: 0.8241 - val_loss: 0.8171 - val_accuracy: 0.5674\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 0.4066 - accuracy: 0.8247 - val_loss: 0.6564 - val_accuracy: 0.6406\n",
      "0.8247249722480774\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 2s 1ms/step - loss: 0.5137 - accuracy: 0.7578 - val_loss: 0.6876 - val_accuracy: 0.6083\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 0.4434 - accuracy: 0.8047 - val_loss: 0.7309 - val_accuracy: 0.5840\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 1s 901us/step - loss: 0.4271 - accuracy: 0.8149 - val_loss: 0.6389 - val_accuracy: 0.6367\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 1s 898us/step - loss: 0.4218 - accuracy: 0.8196 - val_loss: 0.9123 - val_accuracy: 0.5069\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 1s 891us/step - loss: 0.4173 - accuracy: 0.8209 - val_loss: 0.9492 - val_accuracy: 0.5164\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 1s 910us/step - loss: 0.4135 - accuracy: 0.8236 - val_loss: 0.7149 - val_accuracy: 0.6174\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 1s 999us/step - loss: 0.4122 - accuracy: 0.8250 - val_loss: 0.6112 - val_accuracy: 0.6632\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 1s 914us/step - loss: 0.4103 - accuracy: 0.8249 - val_loss: 0.8040 - val_accuracy: 0.5779\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 1s 938us/step - loss: 0.4079 - accuracy: 0.8260 - val_loss: 0.7914 - val_accuracy: 0.5850\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 1s 947us/step - loss: 0.4071 - accuracy: 0.8262 - val_loss: 0.7600 - val_accuracy: 0.5896\n",
      "0.8261749744415283\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250/1250 [==============================] - 2s 1ms/step - loss: 0.5103 - accuracy: 0.7602 - val_loss: 0.6749 - val_accuracy: 0.6204\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 1s 954us/step - loss: 0.4401 - accuracy: 0.8078 - val_loss: 1.0046 - val_accuracy: 0.4729\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 1s 813us/step - loss: 0.4273 - accuracy: 0.8156 - val_loss: 0.7582 - val_accuracy: 0.5814\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 1s 912us/step - loss: 0.4186 - accuracy: 0.8210 - val_loss: 0.6174 - val_accuracy: 0.6559\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 1s 898us/step - loss: 0.4148 - accuracy: 0.8226 - val_loss: 0.9937 - val_accuracy: 0.5009\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 1s 883us/step - loss: 0.4127 - accuracy: 0.8251 - val_loss: 0.7667 - val_accuracy: 0.6008\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 1s 878us/step - loss: 0.4102 - accuracy: 0.8244 - val_loss: 0.8084 - val_accuracy: 0.5821\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 1s 858us/step - loss: 0.4103 - accuracy: 0.8251 - val_loss: 0.5847 - val_accuracy: 0.6844\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 1s 893us/step - loss: 0.4076 - accuracy: 0.8251 - val_loss: 0.7621 - val_accuracy: 0.5961\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 1s 903us/step - loss: 0.4072 - accuracy: 0.8260 - val_loss: 0.5838 - val_accuracy: 0.6829\n",
      "0.825950026512146\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 1s 985us/step - loss: 0.5127 - accuracy: 0.7596 - val_loss: 0.7801 - val_accuracy: 0.5603\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 1s 883us/step - loss: 0.4390 - accuracy: 0.8091 - val_loss: 0.7592 - val_accuracy: 0.5787\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 1s 860us/step - loss: 0.4243 - accuracy: 0.8176 - val_loss: 0.6572 - val_accuracy: 0.6307\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 1s 852us/step - loss: 0.4186 - accuracy: 0.8198 - val_loss: 0.6732 - val_accuracy: 0.6239\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 1s 854us/step - loss: 0.4161 - accuracy: 0.8216 - val_loss: 0.9118 - val_accuracy: 0.5344\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 1s 841us/step - loss: 0.4163 - accuracy: 0.8209 - val_loss: 0.7925 - val_accuracy: 0.5789\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 1s 856us/step - loss: 0.4131 - accuracy: 0.8230 - val_loss: 0.6895 - val_accuracy: 0.6288\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 1s 883us/step - loss: 0.4114 - accuracy: 0.8234 - val_loss: 0.8291 - val_accuracy: 0.5736\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 1s 830us/step - loss: 0.4104 - accuracy: 0.8224 - val_loss: 0.6412 - val_accuracy: 0.6584\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 1s 840us/step - loss: 0.4094 - accuracy: 0.8229 - val_loss: 0.6463 - val_accuracy: 0.6525\n",
      "0.8228750228881836\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 1s 911us/step - loss: 0.5079 - accuracy: 0.7623 - val_loss: 0.9110 - val_accuracy: 0.4989\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 1s 850us/step - loss: 0.4349 - accuracy: 0.8097 - val_loss: 0.6915 - val_accuracy: 0.6003\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 1s 839us/step - loss: 0.4188 - accuracy: 0.8204 - val_loss: 0.6101 - val_accuracy: 0.6548\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 1s 863us/step - loss: 0.4115 - accuracy: 0.8217 - val_loss: 0.7662 - val_accuracy: 0.5912\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 1s 892us/step - loss: 0.4090 - accuracy: 0.8259 - val_loss: 0.6994 - val_accuracy: 0.6190\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 1s 879us/step - loss: 0.4066 - accuracy: 0.8263 - val_loss: 0.8570 - val_accuracy: 0.5531\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 1s 852us/step - loss: 0.4055 - accuracy: 0.8255 - val_loss: 0.6154 - val_accuracy: 0.6631\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 1s 869us/step - loss: 0.4049 - accuracy: 0.8264 - val_loss: 0.7147 - val_accuracy: 0.6120\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 1s 864us/step - loss: 0.4040 - accuracy: 0.8284 - val_loss: 0.6649 - val_accuracy: 0.6404\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 1s 894us/step - loss: 0.4022 - accuracy: 0.8274 - val_loss: 0.6910 - val_accuracy: 0.6222\n",
      "0.8274000287055969\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 2s 1ms/step - loss: 0.5101 - accuracy: 0.7610 - val_loss: 0.9153 - val_accuracy: 0.4943\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 1s 865us/step - loss: 0.4381 - accuracy: 0.8102 - val_loss: 0.6542 - val_accuracy: 0.6198\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 1s 881us/step - loss: 0.4219 - accuracy: 0.8174 - val_loss: 0.9239 - val_accuracy: 0.5214\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 1s 876us/step - loss: 0.4136 - accuracy: 0.8223 - val_loss: 0.9034 - val_accuracy: 0.5470\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 1s 950us/step - loss: 0.4104 - accuracy: 0.8246 - val_loss: 0.7398 - val_accuracy: 0.6049\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 1s 935us/step - loss: 0.4077 - accuracy: 0.8244 - val_loss: 0.7678 - val_accuracy: 0.5897\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 1s 948us/step - loss: 0.4040 - accuracy: 0.8276 - val_loss: 0.7284 - val_accuracy: 0.6079\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 1s 917us/step - loss: 0.4050 - accuracy: 0.8280 - val_loss: 0.7599 - val_accuracy: 0.5970\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 1s 864us/step - loss: 0.4029 - accuracy: 0.8273 - val_loss: 0.8547 - val_accuracy: 0.5575\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 1s 867us/step - loss: 0.4008 - accuracy: 0.8283 - val_loss: 0.9233 - val_accuracy: 0.5304\n",
      "0.8282750248908997\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 1s 942us/step - loss: 0.5091 - accuracy: 0.7612 - val_loss: 0.8525 - val_accuracy: 0.5245\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 1s 907us/step - loss: 0.4393 - accuracy: 0.8070 - val_loss: 0.8515 - val_accuracy: 0.5274\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 1s 955us/step - loss: 0.4241 - accuracy: 0.8163 - val_loss: 0.7344 - val_accuracy: 0.5856\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 1s 931us/step - loss: 0.4196 - accuracy: 0.8198 - val_loss: 0.6417 - val_accuracy: 0.6409\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 1s 958us/step - loss: 0.4150 - accuracy: 0.8218 - val_loss: 0.7758 - val_accuracy: 0.5815\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 1s 917us/step - loss: 0.4152 - accuracy: 0.8234 - val_loss: 0.6754 - val_accuracy: 0.6281\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 1s 896us/step - loss: 0.4129 - accuracy: 0.8223 - val_loss: 0.6617 - val_accuracy: 0.6414\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 1s 922us/step - loss: 0.4114 - accuracy: 0.8224 - val_loss: 0.6411 - val_accuracy: 0.6528\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 1s 951us/step - loss: 0.4101 - accuracy: 0.8233 - val_loss: 0.6282 - val_accuracy: 0.6673\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 1s 933us/step - loss: 0.4086 - accuracy: 0.8239 - val_loss: 0.7312 - val_accuracy: 0.6109\n",
      "0.8238750100135803\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 0.5090 - accuracy: 0.7603 - val_loss: 0.9764 - val_accuracy: 0.4645\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 1s 939us/step - loss: 0.4415 - accuracy: 0.8066 - val_loss: 0.6792 - val_accuracy: 0.6132\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 1s 977us/step - loss: 0.4279 - accuracy: 0.8145 - val_loss: 0.7585 - val_accuracy: 0.5849\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 1s 918us/step - loss: 0.4210 - accuracy: 0.8169 - val_loss: 0.8755 - val_accuracy: 0.5387\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 1s 927us/step - loss: 0.4183 - accuracy: 0.8211 - val_loss: 0.7165 - val_accuracy: 0.6043\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 1s 869us/step - loss: 0.4155 - accuracy: 0.8213 - val_loss: 0.6354 - val_accuracy: 0.6420\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 1s 917us/step - loss: 0.4120 - accuracy: 0.8220 - val_loss: 0.6818 - val_accuracy: 0.6280\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 1s 911us/step - loss: 0.4096 - accuracy: 0.8235 - val_loss: 0.7681 - val_accuracy: 0.5879\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 1s 906us/step - loss: 0.4088 - accuracy: 0.8216 - val_loss: 0.7725 - val_accuracy: 0.5680\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 1s 884us/step - loss: 0.4065 - accuracy: 0.8227 - val_loss: 0.8860 - val_accuracy: 0.5368\n",
      "0.8227249979972839\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 1s 974us/step - loss: 0.5085 - accuracy: 0.7641 - val_loss: 0.6979 - val_accuracy: 0.6020\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 1s 929us/step - loss: 0.4339 - accuracy: 0.8116 - val_loss: 0.8237 - val_accuracy: 0.5503\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 1s 892us/step - loss: 0.4190 - accuracy: 0.8180 - val_loss: 0.8390 - val_accuracy: 0.5500\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 1s 926us/step - loss: 0.4127 - accuracy: 0.8216 - val_loss: 0.8188 - val_accuracy: 0.5529\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 1s 915us/step - loss: 0.4092 - accuracy: 0.8232 - val_loss: 0.5595 - val_accuracy: 0.6898\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 1s 934us/step - loss: 0.4079 - accuracy: 0.8234 - val_loss: 0.6862 - val_accuracy: 0.6324\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 1s 952us/step - loss: 0.4060 - accuracy: 0.8253 - val_loss: 0.9056 - val_accuracy: 0.5316\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 1s 910us/step - loss: 0.4071 - accuracy: 0.8261 - val_loss: 0.7592 - val_accuracy: 0.5897\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 1s 935us/step - loss: 0.4047 - accuracy: 0.8245 - val_loss: 0.7458 - val_accuracy: 0.6045\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 1s 922us/step - loss: 0.4044 - accuracy: 0.8245 - val_loss: 0.6396 - val_accuracy: 0.6525\n",
      "0.824524998664856\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 1s 948us/step - loss: 0.5128 - accuracy: 0.7600 - val_loss: 0.6991 - val_accuracy: 0.6127\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 1s 872us/step - loss: 0.4374 - accuracy: 0.8117 - val_loss: 1.0592 - val_accuracy: 0.4695\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 1s 862us/step - loss: 0.4203 - accuracy: 0.8202 - val_loss: 0.8795 - val_accuracy: 0.5244\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 1s 910us/step - loss: 0.4149 - accuracy: 0.8234 - val_loss: 0.9326 - val_accuracy: 0.5144\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 1s 923us/step - loss: 0.4094 - accuracy: 0.8245 - val_loss: 0.8419 - val_accuracy: 0.5485\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 1s 946us/step - loss: 0.4077 - accuracy: 0.8245 - val_loss: 0.6919 - val_accuracy: 0.6285\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 1s 909us/step - loss: 0.4057 - accuracy: 0.8245 - val_loss: 0.8198 - val_accuracy: 0.5776\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 1s 953us/step - loss: 0.4043 - accuracy: 0.8270 - val_loss: 0.9359 - val_accuracy: 0.5296\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 1s 908us/step - loss: 0.4030 - accuracy: 0.8267 - val_loss: 0.6040 - val_accuracy: 0.6769\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 0.4022 - accuracy: 0.8269 - val_loss: 0.7280 - val_accuracy: 0.6126\n",
      "0.8268749713897705\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 2s 1ms/step - loss: 0.5175 - accuracy: 0.7550 - val_loss: 0.7647 - val_accuracy: 0.5596\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 0.4444 - accuracy: 0.8059 - val_loss: 0.6940 - val_accuracy: 0.6056\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 0.4299 - accuracy: 0.8137 - val_loss: 0.8060 - val_accuracy: 0.5548\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 1s 939us/step - loss: 0.4210 - accuracy: 0.8185 - val_loss: 0.6559 - val_accuracy: 0.6258\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 1s 986us/step - loss: 0.4168 - accuracy: 0.8199 - val_loss: 0.7618 - val_accuracy: 0.5821\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 0.4121 - accuracy: 0.8230 - val_loss: 0.6698 - val_accuracy: 0.6352\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 1s 963us/step - loss: 0.4103 - accuracy: 0.8231 - val_loss: 0.8487 - val_accuracy: 0.5519\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 1s 997us/step - loss: 0.4099 - accuracy: 0.8239 - val_loss: 0.7510 - val_accuracy: 0.5914\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 0.4076 - accuracy: 0.8259 - val_loss: 0.7478 - val_accuracy: 0.5996\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 0.4062 - accuracy: 0.8253 - val_loss: 0.7463 - val_accuracy: 0.6050\n",
      "0.8253499865531921\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 1s 963us/step - loss: 0.5076 - accuracy: 0.7618 - val_loss: 0.7734 - val_accuracy: 0.5555\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 1s 957us/step - loss: 0.4414 - accuracy: 0.8063 - val_loss: 0.7467 - val_accuracy: 0.5764\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 1s 925us/step - loss: 0.4259 - accuracy: 0.8160 - val_loss: 0.6886 - val_accuracy: 0.6141\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 1s 916us/step - loss: 0.4221 - accuracy: 0.8194 - val_loss: 0.9570 - val_accuracy: 0.4999\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 1s 908us/step - loss: 0.4159 - accuracy: 0.8223 - val_loss: 0.7243 - val_accuracy: 0.6037\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 1s 889us/step - loss: 0.4124 - accuracy: 0.8235 - val_loss: 0.7764 - val_accuracy: 0.5934\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 1s 900us/step - loss: 0.4072 - accuracy: 0.8237 - val_loss: 0.7143 - val_accuracy: 0.6212\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 1s 952us/step - loss: 0.4077 - accuracy: 0.8239 - val_loss: 0.8881 - val_accuracy: 0.5208\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 0.4033 - accuracy: 0.8247 - val_loss: 0.6862 - val_accuracy: 0.6370\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 2s 1ms/step - loss: 0.4040 - accuracy: 0.8252 - val_loss: 0.7195 - val_accuracy: 0.6019\n",
      "0.8252000212669373\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 2s 1ms/step - loss: 0.5096 - accuracy: 0.7621 - val_loss: 0.9423 - val_accuracy: 0.4773\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 1s 952us/step - loss: 0.4377 - accuracy: 0.8109 - val_loss: 1.0084 - val_accuracy: 0.4719\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 1s 925us/step - loss: 0.4219 - accuracy: 0.8182 - val_loss: 0.6108 - val_accuracy: 0.6594\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 1s 903us/step - loss: 0.4159 - accuracy: 0.8201 - val_loss: 0.8388 - val_accuracy: 0.5478\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 1s 893us/step - loss: 0.4122 - accuracy: 0.8231 - val_loss: 0.7165 - val_accuracy: 0.6040\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 1s 919us/step - loss: 0.4087 - accuracy: 0.8249 - val_loss: 0.7375 - val_accuracy: 0.5932\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 0.4061 - accuracy: 0.8260 - val_loss: 0.6802 - val_accuracy: 0.6308\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 0.4058 - accuracy: 0.8240 - val_loss: 0.7788 - val_accuracy: 0.5840\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 0.4025 - accuracy: 0.8258 - val_loss: 0.8354 - val_accuracy: 0.5588\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 0.4021 - accuracy: 0.8262 - val_loss: 0.8825 - val_accuracy: 0.5461\n",
      "0.826200008392334\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250/1250 [==============================] - 2s 1ms/step - loss: 0.5122 - accuracy: 0.7595 - val_loss: 0.7381 - val_accuracy: 0.5730\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 1s 948us/step - loss: 0.4394 - accuracy: 0.8076 - val_loss: 0.6824 - val_accuracy: 0.6114\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 1s 846us/step - loss: 0.4235 - accuracy: 0.8175 - val_loss: 0.7817 - val_accuracy: 0.5738\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 1s 827us/step - loss: 0.4165 - accuracy: 0.8220 - val_loss: 0.9101 - val_accuracy: 0.5224\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 1s 864us/step - loss: 0.4140 - accuracy: 0.8224 - val_loss: 0.7965 - val_accuracy: 0.5661\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 1s 972us/step - loss: 0.4123 - accuracy: 0.8225 - val_loss: 0.7079 - val_accuracy: 0.6099\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 1s 936us/step - loss: 0.4108 - accuracy: 0.8246 - val_loss: 0.7058 - val_accuracy: 0.6118\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 1s 813us/step - loss: 0.4092 - accuracy: 0.8254 - val_loss: 1.0462 - val_accuracy: 0.4944\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 1s 804us/step - loss: 0.4090 - accuracy: 0.8252 - val_loss: 0.6655 - val_accuracy: 0.6402\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 1s 816us/step - loss: 0.4080 - accuracy: 0.8253 - val_loss: 0.6425 - val_accuracy: 0.6475\n",
      "0.8252500295639038\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 1s 896us/step - loss: 0.5160 - accuracy: 0.7604 - val_loss: 0.9293 - val_accuracy: 0.4917\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 1s 980us/step - loss: 0.4438 - accuracy: 0.8061 - val_loss: 0.7932 - val_accuracy: 0.5510\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 1s 906us/step - loss: 0.4297 - accuracy: 0.8143 - val_loss: 0.8182 - val_accuracy: 0.5504\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 1s 908us/step - loss: 0.4218 - accuracy: 0.8191 - val_loss: 1.0349 - val_accuracy: 0.4709\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 1s 836us/step - loss: 0.4160 - accuracy: 0.8216 - val_loss: 0.8886 - val_accuracy: 0.5205\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 1s 780us/step - loss: 0.4129 - accuracy: 0.8234 - val_loss: 0.5114 - val_accuracy: 0.7195\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 1s 786us/step - loss: 0.4097 - accuracy: 0.8248 - val_loss: 0.9227 - val_accuracy: 0.5124\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 1s 773us/step - loss: 0.4084 - accuracy: 0.8238 - val_loss: 0.6491 - val_accuracy: 0.6426\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 1s 807us/step - loss: 0.4073 - accuracy: 0.8249 - val_loss: 0.7864 - val_accuracy: 0.5794\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 1s 782us/step - loss: 0.4069 - accuracy: 0.8252 - val_loss: 0.8603 - val_accuracy: 0.5275\n",
      "0.8251500129699707\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 1s 863us/step - loss: 0.5055 - accuracy: 0.7645 - val_loss: 0.8874 - val_accuracy: 0.5054\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 1s 861us/step - loss: 0.4350 - accuracy: 0.8115 - val_loss: 0.6822 - val_accuracy: 0.5987\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 1s 879us/step - loss: 0.4226 - accuracy: 0.8188 - val_loss: 0.8649 - val_accuracy: 0.5223\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 1s 805us/step - loss: 0.4156 - accuracy: 0.8230 - val_loss: 0.8921 - val_accuracy: 0.5303\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 1s 802us/step - loss: 0.4148 - accuracy: 0.8231 - val_loss: 0.9771 - val_accuracy: 0.4992\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 1s 822us/step - loss: 0.4119 - accuracy: 0.8253 - val_loss: 0.6351 - val_accuracy: 0.6518\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 1s 810us/step - loss: 0.4097 - accuracy: 0.8257 - val_loss: 0.7413 - val_accuracy: 0.5980\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 1s 770us/step - loss: 0.4077 - accuracy: 0.8270 - val_loss: 0.7862 - val_accuracy: 0.5757\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 1s 776us/step - loss: 0.4059 - accuracy: 0.8277 - val_loss: 0.8127 - val_accuracy: 0.5679\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 1s 782us/step - loss: 0.4061 - accuracy: 0.8272 - val_loss: 0.7129 - val_accuracy: 0.6186\n",
      "0.8271999955177307\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 1s 879us/step - loss: 0.5067 - accuracy: 0.7627 - val_loss: 0.9021 - val_accuracy: 0.4947\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 1s 803us/step - loss: 0.4368 - accuracy: 0.8110 - val_loss: 0.8504 - val_accuracy: 0.5285\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 1s 794us/step - loss: 0.4200 - accuracy: 0.8192 - val_loss: 0.6020 - val_accuracy: 0.6667\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 1s 787us/step - loss: 0.4117 - accuracy: 0.8227 - val_loss: 0.7426 - val_accuracy: 0.5965\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 1s 771us/step - loss: 0.4091 - accuracy: 0.8250 - val_loss: 0.7203 - val_accuracy: 0.6191\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 1s 796us/step - loss: 0.4082 - accuracy: 0.8237 - val_loss: 0.5557 - val_accuracy: 0.6957\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 1s 799us/step - loss: 0.4046 - accuracy: 0.8244 - val_loss: 0.6213 - val_accuracy: 0.6580\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 0.4039 - accuracy: 0.8257 - val_loss: 0.7784 - val_accuracy: 0.5820\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 0.4027 - accuracy: 0.8256 - val_loss: 0.6959 - val_accuracy: 0.6282\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 0.4017 - accuracy: 0.8276 - val_loss: 0.7432 - val_accuracy: 0.6129\n",
      "0.8276249766349792\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 2s 1ms/step - loss: 0.5162 - accuracy: 0.7564 - val_loss: 0.6674 - val_accuracy: 0.6169\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 1s 868us/step - loss: 0.4443 - accuracy: 0.8056 - val_loss: 0.7474 - val_accuracy: 0.5770\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 1s 900us/step - loss: 0.4266 - accuracy: 0.8150 - val_loss: 0.6821 - val_accuracy: 0.6236\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 1s 921us/step - loss: 0.4203 - accuracy: 0.8186 - val_loss: 0.8326 - val_accuracy: 0.5720\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 1s 936us/step - loss: 0.4164 - accuracy: 0.8214 - val_loss: 0.6792 - val_accuracy: 0.6286\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 1s 939us/step - loss: 0.4136 - accuracy: 0.8220 - val_loss: 0.7298 - val_accuracy: 0.6163\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 1s 878us/step - loss: 0.4126 - accuracy: 0.8223 - val_loss: 0.8412 - val_accuracy: 0.5682\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 1s 919us/step - loss: 0.4094 - accuracy: 0.8232 - val_loss: 0.7943 - val_accuracy: 0.5838\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 1s 892us/step - loss: 0.4093 - accuracy: 0.8232 - val_loss: 0.8093 - val_accuracy: 0.5763\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 1s 886us/step - loss: 0.4094 - accuracy: 0.8235 - val_loss: 0.8216 - val_accuracy: 0.5778\n",
      "0.8234999775886536\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 2s 964us/step - loss: 0.5112 - accuracy: 0.7588 - val_loss: 0.8902 - val_accuracy: 0.5072\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 1s 909us/step - loss: 0.4397 - accuracy: 0.8083 - val_loss: 0.6600 - val_accuracy: 0.6285\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 1s 910us/step - loss: 0.4248 - accuracy: 0.8177 - val_loss: 0.5742 - val_accuracy: 0.6725\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 1s 957us/step - loss: 0.4198 - accuracy: 0.8197 - val_loss: 0.5347 - val_accuracy: 0.7033\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 1s 933us/step - loss: 0.4151 - accuracy: 0.8229 - val_loss: 0.7120 - val_accuracy: 0.6129\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 1s 853us/step - loss: 0.4131 - accuracy: 0.8234 - val_loss: 0.7401 - val_accuracy: 0.6038\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 1s 876us/step - loss: 0.4109 - accuracy: 0.8242 - val_loss: 0.6761 - val_accuracy: 0.6254\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 1s 843us/step - loss: 0.4092 - accuracy: 0.8260 - val_loss: 0.6932 - val_accuracy: 0.6204\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 1s 874us/step - loss: 0.4098 - accuracy: 0.8238 - val_loss: 0.6828 - val_accuracy: 0.6294\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 1s 875us/step - loss: 0.4052 - accuracy: 0.8272 - val_loss: 0.6436 - val_accuracy: 0.6572\n",
      "0.8271999955177307\n",
      "yhn tk\n",
      "1\n",
      "Done\n",
      "yhn tk\n",
      "1\n",
      "Done\n",
      "yhn tk\n",
      "1\n",
      "Done\n",
      "yhn tk\n",
      "1\n",
      "Done\n",
      "yhn tk\n",
      "1\n",
      "Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ayushkv/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:45: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10938/10938 [==============================] - 6s 573us/step - loss: 0.4511 - accuracy: 0.7932\n",
      "4688/4688 [==============================] - 3s 573us/step - loss: 0.4520 - accuracy: 0.7911\n",
      "10938/10938 [==============================] - 6s 558us/step - loss: 0.4842 - accuracy: 0.7769\n",
      "4688/4688 [==============================] - 3s 550us/step - loss: 0.4842 - accuracy: 0.7754\n",
      "10938/10938 [==============================] - 6s 547us/step - loss: 0.4483 - accuracy: 0.7942\n",
      "4688/4688 [==============================] - 3s 560us/step - loss: 0.4493 - accuracy: 0.7917\n",
      "10938/10938 [==============================] - 6s 538us/step - loss: 0.4433 - accuracy: 0.7964\n",
      "4688/4688 [==============================] - 3s 546us/step - loss: 0.4444 - accuracy: 0.7954\n",
      "10938/10938 [==============================] - 6s 540us/step - loss: 0.4457 - accuracy: 0.7964\n",
      "4688/4688 [==============================] - 3s 559us/step - loss: 0.4467 - accuracy: 0.7948\n",
      "15625/15625 [==============================] - 7s 450us/step\n",
      "15625/15625 [==============================] - 7s 458us/step\n",
      "15625/15625 [==============================] - 8s 533us/step\n",
      "15625/15625 [==============================] - 7s 469us/step\n",
      "15625/15625 [==============================] - 7s 456us/step\n"
     ]
    }
   ],
   "source": [
    "# adding dense layer\n",
    "# adding dense layer\n",
    "initial_model= get_initial_model_2(dataset.shape[1]-1, 2)\n",
    "Models=[]\n",
    "val_acc=[]\n",
    "train_acc=[]\n",
    "test_acc=[]\n",
    "val_loss=[]\n",
    "train_loss=[]\n",
    "ind=0\n",
    "add_weights=[]\n",
    "true_values=[]\n",
    "while ind<len(samples):\n",
    "  \n",
    "  train_data=samples[ind]\n",
    "  ind=ind+1\n",
    "  \n",
    "  \n",
    "  ann_model=get_initial_model_2(dataset.shape[1]-1, 2) #same intial weights\n",
    "  ann_model.set_weights(initial_model.get_weights())\n",
    "  X_train=train_data.drop(columns=[target_variable])\n",
    "  \n",
    "\n",
    "  y_train=to_categorical(train_data[target_variable])\n",
    "    \n",
    "  ann_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) # metrics=['accuracy']\n",
    "  history = ann_model.fit(X_train, y_train, epochs=10, validation_split=0.2, verbose=1)\n",
    "  print(history.history['accuracy'][-1])\n",
    "\n",
    "  present=False\n",
    "  for i in range(len(Models)):\n",
    "    if (check_models(Models[i][0], ann_model.get_weights())):\n",
    "      print(\"if any\")\n",
    "      Models[i][1]=Models[i][1]+1\n",
    "      add_weights[i].append(ann_model.get_weights())\n",
    "      present=True\n",
    "      break;\n",
    "  if present==False:\n",
    "    add_weights.append([ann_model.get_weights()])\n",
    "    Models.append([ann_model.get_weights(), 1])\n",
    "\n",
    "len(Models)   \n",
    "#here use only top 5-10 integrally private models.\n",
    "#add_weights=add_weights[top_5]\n",
    "A=np.argsort(np.array(Models).T[1])[::-1][:5]\n",
    "recommended_models=[]\n",
    "for i in range(len(A)):\n",
    "    recommended_models.append(add_weights[A[i]])\n",
    "#add_weights=add_weights[A]\n",
    "\n",
    "# Now trying to generate Streaming settings for the dataset\n",
    "# lets find the outputs from all the \n",
    "mean_model_weights, mean_model_acc, mean_model_loss, mean_model_test_acc, mean_model_test_loss = epsilon_mean_recommendation_2(recommended_models, data_init)\n",
    "\n",
    "y_pred_total=[]\n",
    "y_pred_uncertainty_total=[]\n",
    "\n",
    "y_pred, y_pred_uncertainty = drift_detection_2(mean_model_weights, data_init.copy())\n",
    "\n",
    "y_pred_total.append(y_pred)\n",
    "y_pred_uncertainty_total.append(y_pred_uncertainty)\n",
    "\n",
    "true_values.append(data_init.copy()[target_variable])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "64e6c831",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15625/15625 [==============================] - 7s 475us/step\n",
      "15625/15625 [==============================] - 9s 543us/step\n",
      "15625/15625 [==============================] - 7s 477us/step\n",
      "15625/15625 [==============================] - 8s 499us/step\n",
      "15625/15625 [==============================] - 8s 497us/step\n",
      "3125/3125 [==============================] - 2s 479us/step\n",
      "3125/3125 [==============================] - 2s 481us/step\n",
      "3125/3125 [==============================] - 1s 475us/step\n",
      "3125/3125 [==============================] - 1s 473us/step\n",
      "3125/3125 [==============================] - 2s 477us/step\n",
      "100000\n",
      "One lap done.\n",
      "15625/15625 [==============================] - 8s 481us/step\n",
      "15625/15625 [==============================] - 8s 487us/step\n",
      "15625/15625 [==============================] - 8s 489us/step\n",
      "15625/15625 [==============================] - 8s 482us/step\n",
      "15625/15625 [==============================] - 8s 497us/step\n",
      "3125/3125 [==============================] - 2s 491us/step\n",
      "3125/3125 [==============================] - 2s 491us/step\n",
      "3125/3125 [==============================] - 2s 482us/step\n",
      "3125/3125 [==============================] - 2s 491us/step\n",
      "3125/3125 [==============================] - 2s 481us/step\n",
      "100000\n",
      "One lap done.\n",
      "15625/15625 [==============================] - 8s 499us/step\n",
      "15625/15625 [==============================] - 8s 524us/step\n",
      "15625/15625 [==============================] - 8s 513us/step\n",
      "15625/15625 [==============================] - 8s 487us/step\n",
      "15625/15625 [==============================] - 7s 450us/step\n",
      "3125/3125 [==============================] - 2s 484us/step\n",
      "3125/3125 [==============================] - 2s 484us/step\n",
      "3125/3125 [==============================] - 2s 479us/step\n",
      "3125/3125 [==============================] - 2s 477us/step\n",
      "3125/3125 [==============================] - 2s 477us/step\n",
      "100000\n",
      "One lap done.\n",
      "15625/15625 [==============================] - 7s 430us/step\n",
      "15625/15625 [==============================] - 7s 436us/step\n",
      "15625/15625 [==============================] - 7s 439us/step\n",
      "15625/15625 [==============================] - 7s 473us/step\n",
      "15625/15625 [==============================] - 7s 466us/step\n",
      "3125/3125 [==============================] - 1s 450us/step\n",
      "3125/3125 [==============================] - 1s 454us/step\n",
      "3125/3125 [==============================] - 1s 444us/step\n",
      "3125/3125 [==============================] - 1s 455us/step\n",
      "3125/3125 [==============================] - 2s 484us/step\n",
      "100000\n",
      "One lap done.\n",
      "15625/15625 [==============================] - 8s 534us/step\n",
      "15625/15625 [==============================] - 8s 483us/step\n",
      "15625/15625 [==============================] - 8s 496us/step\n",
      "15625/15625 [==============================] - 8s 496us/step\n",
      "15625/15625 [==============================] - 8s 484us/step\n",
      "3125/3125 [==============================] - 2s 472us/step\n",
      "3125/3125 [==============================] - 1s 475us/step\n",
      "3125/3125 [==============================] - 2s 479us/step\n",
      "3125/3125 [==============================] - 1s 475us/step\n",
      "3125/3125 [==============================] - 2s 479us/step\n",
      "100000\n",
      "One lap done.\n",
      "15625/15625 [==============================] - 8s 478us/step\n",
      "15625/15625 [==============================] - 9s 545us/step\n",
      "15625/15625 [==============================] - 8s 493us/step\n",
      "15625/15625 [==============================] - 8s 489us/step\n",
      "15625/15625 [==============================] - 8s 485us/step\n",
      "3125/3125 [==============================] - 2s 509us/step\n",
      "3125/3125 [==============================] - 2s 512us/step\n",
      "3125/3125 [==============================] - 2s 516us/step\n",
      "3125/3125 [==============================] - 2s 480us/step\n",
      "3125/3125 [==============================] - 2s 485us/step\n",
      "100000\n",
      "One lap done.\n",
      "15625/15625 [==============================] - 8s 478us/step\n",
      "15625/15625 [==============================] - 8s 494us/step\n",
      "15625/15625 [==============================] - 9s 548us/step\n",
      "15625/15625 [==============================] - 8s 507us/step\n",
      "15625/15625 [==============================] - 8s 498us/step\n",
      "3125/3125 [==============================] - 2s 507us/step\n",
      "3125/3125 [==============================] - 2s 519us/step\n",
      "3125/3125 [==============================] - 2s 492us/step\n",
      "3125/3125 [==============================] - 2s 489us/step\n",
      "3125/3125 [==============================] - 2s 509us/step\n",
      "100000\n",
      "One lap done.\n",
      "15625/15625 [==============================] - 41s 3ms/step\n",
      "15625/15625 [==============================] - 40s 3ms/step\n",
      "15625/15625 [==============================] - 29s 2ms/step\n",
      "15625/15625 [==============================] - 8s 497us/step\n",
      "15625/15625 [==============================] - 9s 558us/step\n",
      "3125/3125 [==============================] - 2s 528us/step\n",
      "3125/3125 [==============================] - 2s 514us/step\n",
      "3125/3125 [==============================] - 2s 539us/step\n",
      "3125/3125 [==============================] - 2s 513us/step\n",
      "3125/3125 [==============================] - 2s 516us/step\n",
      "100000\n",
      "One lap done.\n",
      "15625/15625 [==============================] - 8s 532us/step\n",
      "15625/15625 [==============================] - 9s 548us/step\n",
      "15625/15625 [==============================] - 9s 555us/step\n",
      "15625/15625 [==============================] - 8s 539us/step\n",
      "15625/15625 [==============================] - 8s 536us/step\n",
      "3125/3125 [==============================] - 2s 540us/step\n",
      "3125/3125 [==============================] - 2s 521us/step\n",
      "3125/3125 [==============================] - 2s 508us/step\n",
      "3125/3125 [==============================] - 2s 518us/step\n",
      "3125/3125 [==============================] - 2s 506us/step\n",
      "100000\n",
      "One lap done.\n",
      "15625/15625 [==============================] - 8s 519us/step\n",
      "15625/15625 [==============================] - 8s 533us/step\n",
      "15625/15625 [==============================] - 8s 515us/step\n",
      "15625/15625 [==============================] - 8s 504us/step\n",
      "15625/15625 [==============================] - 8s 526us/step\n",
      "3125/3125 [==============================] - 2s 603us/step\n",
      "3125/3125 [==============================] - 2s 569us/step\n",
      "3125/3125 [==============================] - 2s 582us/step\n",
      "3125/3125 [==============================] - 2s 557us/step\n",
      "3125/3125 [==============================] - 2s 533us/step\n",
      "100000\n",
      "One lap done.\n",
      "15625/15625 [==============================] - 8s 522us/step\n",
      "15625/15625 [==============================] - 9s 554us/step\n",
      "15625/15625 [==============================] - 9s 565us/step\n",
      "15625/15625 [==============================] - 8s 524us/step\n",
      "15625/15625 [==============================] - 8s 528us/step\n",
      "3125/3125 [==============================] - 2s 520us/step\n",
      "3125/3125 [==============================] - 2s 518us/step\n",
      "3125/3125 [==============================] - 2s 508us/step\n",
      "3125/3125 [==============================] - 2s 523us/step\n",
      "3125/3125 [==============================] - 2s 512us/step\n",
      "100000\n",
      "One lap done.\n",
      "15625/15625 [==============================] - 8s 532us/step\n",
      "15625/15625 [==============================] - 8s 512us/step\n",
      "15625/15625 [==============================] - 8s 515us/step\n",
      "15625/15625 [==============================] - 8s 539us/step\n",
      "15625/15625 [==============================] - 8s 527us/step\n",
      "3125/3125 [==============================] - 2s 527us/step\n",
      "3125/3125 [==============================] - 2s 519us/step\n",
      "3125/3125 [==============================] - 2s 522us/step\n",
      "3125/3125 [==============================] - 2s 518us/step\n",
      "3125/3125 [==============================] - 2s 523us/step\n",
      "100000\n",
      "One lap done.\n",
      "15625/15625 [==============================] - 8s 502us/step\n",
      "15625/15625 [==============================] - 8s 504us/step\n",
      "15625/15625 [==============================] - 8s 509us/step\n",
      "15625/15625 [==============================] - 8s 501us/step\n",
      "15625/15625 [==============================] - 8s 496us/step\n",
      "3125/3125 [==============================] - 2s 512us/step\n",
      "3125/3125 [==============================] - 2s 516us/step\n",
      "3125/3125 [==============================] - 2s 547us/step\n",
      "3125/3125 [==============================] - 2s 509us/step\n",
      "3125/3125 [==============================] - 2s 510us/step\n",
      "100000\n",
      "One lap done.\n",
      "15625/15625 [==============================] - 7s 472us/step\n",
      "15625/15625 [==============================] - 7s 473us/step\n",
      "15625/15625 [==============================] - 7s 451us/step\n",
      "15625/15625 [==============================] - 7s 453us/step\n",
      "15625/15625 [==============================] - 7s 454us/step\n",
      "3125/3125 [==============================] - 2s 494us/step\n",
      "3125/3125 [==============================] - 2s 503us/step\n",
      "3125/3125 [==============================] - 2s 497us/step\n",
      "3125/3125 [==============================] - 2s 491us/step\n",
      "3125/3125 [==============================] - 2s 495us/step\n",
      "100000\n",
      "One lap done.\n",
      "15625/15625 [==============================] - 7s 456us/step\n",
      "15625/15625 [==============================] - 7s 468us/step\n",
      "15625/15625 [==============================] - 7s 469us/step\n",
      "15625/15625 [==============================] - 7s 474us/step\n",
      "15625/15625 [==============================] - 7s 467us/step\n",
      "3125/3125 [==============================] - 2s 499us/step\n",
      "3125/3125 [==============================] - 2s 490us/step\n",
      "3125/3125 [==============================] - 2s 503us/step\n",
      "3125/3125 [==============================] - 2s 489us/step\n",
      "3125/3125 [==============================] - 2s 492us/step\n",
      "100000\n",
      "One lap done.\n",
      "15625/15625 [==============================] - 7s 444us/step\n",
      "15625/15625 [==============================] - 7s 453us/step\n",
      "15625/15625 [==============================] - 7s 451us/step\n",
      "15625/15625 [==============================] - 7s 450us/step\n",
      "15625/15625 [==============================] - 7s 452us/step\n",
      "3125/3125 [==============================] - 2s 495us/step\n",
      "3125/3125 [==============================] - 2s 500us/step\n",
      "3125/3125 [==============================] - 2s 488us/step\n",
      "3125/3125 [==============================] - 2s 491us/step\n",
      "3125/3125 [==============================] - 2s 495us/step\n",
      "100000\n",
      "One lap done.\n",
      "15625/15625 [==============================] - 8s 485us/step\n",
      "15625/15625 [==============================] - 7s 451us/step\n",
      "15625/15625 [==============================] - 7s 467us/step\n",
      "15625/15625 [==============================] - 7s 465us/step\n",
      "15625/15625 [==============================] - 7s 464us/step\n",
      "3125/3125 [==============================] - 2s 511us/step\n",
      "3125/3125 [==============================] - 2s 489us/step\n",
      "3125/3125 [==============================] - 2s 486us/step\n",
      "3125/3125 [==============================] - 2s 515us/step\n",
      "3125/3125 [==============================] - 2s 486us/step\n",
      "100000\n",
      "One lap done.\n",
      "15625/15625 [==============================] - 7s 455us/step\n",
      "15625/15625 [==============================] - 7s 451us/step\n",
      "15625/15625 [==============================] - 7s 450us/step\n",
      "15625/15625 [==============================] - 7s 450us/step\n",
      "15625/15625 [==============================] - 7s 451us/step\n",
      "3125/3125 [==============================] - 2s 487us/step\n",
      "3125/3125 [==============================] - 2s 497us/step\n",
      "3125/3125 [==============================] - 2s 508us/step\n",
      "3125/3125 [==============================] - 2s 488us/step\n",
      "3125/3125 [==============================] - 2s 489us/step\n",
      "100000\n",
      "One lap done.\n",
      "15625/15625 [==============================] - 8s 502us/step\n",
      "15625/15625 [==============================] - 7s 450us/step\n",
      "15625/15625 [==============================] - 7s 450us/step\n",
      "15625/15625 [==============================] - 8s 482us/step\n",
      "15625/15625 [==============================] - 7s 462us/step\n",
      "3125/3125 [==============================] - 2s 514us/step\n",
      "3125/3125 [==============================] - 2s 519us/step\n",
      "3125/3125 [==============================] - 2s 507us/step\n",
      "3125/3125 [==============================] - 2s 490us/step\n",
      "3125/3125 [==============================] - 2s 490us/step\n",
      "100000\n",
      "One lap done.\n",
      "15625/15625 [==============================] - 7s 451us/step\n",
      "15625/15625 [==============================] - 7s 458us/step\n",
      "15625/15625 [==============================] - 7s 449us/step\n",
      "15625/15625 [==============================] - 7s 453us/step\n",
      "15625/15625 [==============================] - 7s 454us/step\n",
      "3125/3125 [==============================] - 2s 490us/step\n",
      "3125/3125 [==============================] - 2s 493us/step\n",
      "3125/3125 [==============================] - 2s 490us/step\n",
      "3125/3125 [==============================] - 2s 495us/step\n",
      "3125/3125 [==============================] - 2s 488us/step\n",
      "100000\n",
      "One lap done.\n",
      "15625/15625 [==============================] - 8s 502us/step\n",
      "15625/15625 [==============================] - 8s 492us/step\n",
      "15625/15625 [==============================] - 8s 505us/step\n",
      "15625/15625 [==============================] - 9s 572us/step\n",
      "15625/15625 [==============================] - 9s 569us/step\n",
      "3125/3125 [==============================] - 2s 490us/step\n",
      "3125/3125 [==============================] - 2s 513us/step\n",
      "3125/3125 [==============================] - 2s 516us/step\n",
      "3125/3125 [==============================] - 2s 524us/step\n",
      "3125/3125 [==============================] - 2s 550us/step\n",
      "100000\n",
      "One lap done.\n",
      "15625/15625 [==============================] - 8s 531us/step\n",
      "15625/15625 [==============================] - 8s 517us/step\n",
      "15625/15625 [==============================] - 8s 534us/step\n",
      "15625/15625 [==============================] - 8s 516us/step\n",
      "15625/15625 [==============================] - 8s 514us/step\n",
      "3125/3125 [==============================] - 2s 487us/step\n",
      "3125/3125 [==============================] - 2s 485us/step\n",
      "3125/3125 [==============================] - 2s 552us/step\n",
      "3125/3125 [==============================] - 2s 515us/step\n",
      "3125/3125 [==============================] - 2s 543us/step\n",
      "100000\n",
      "One lap done.\n",
      "15625/15625 [==============================] - 8s 524us/step\n",
      "15625/15625 [==============================] - 9s 566us/step\n",
      "15625/15625 [==============================] - 9s 550us/step\n",
      "15625/15625 [==============================] - 9s 548us/step\n",
      "15625/15625 [==============================] - 9s 553us/step\n",
      "3125/3125 [==============================] - 2s 538us/step\n",
      "3125/3125 [==============================] - 2s 509us/step\n",
      "3125/3125 [==============================] - 2s 507us/step\n",
      "3125/3125 [==============================] - 2s 548us/step\n",
      "3125/3125 [==============================] - 2s 535us/step\n",
      "100000\n",
      "One lap done.\n",
      "15625/15625 [==============================] - 8s 494us/step\n",
      "15625/15625 [==============================] - 8s 488us/step\n",
      "15625/15625 [==============================] - 8s 481us/step\n",
      "15625/15625 [==============================] - 8s 486us/step\n",
      "15625/15625 [==============================] - 8s 490us/step\n",
      "3125/3125 [==============================] - 2s 515us/step\n",
      "3125/3125 [==============================] - 2s 523us/step\n",
      "3125/3125 [==============================] - 2s 515us/step\n",
      "3125/3125 [==============================] - 2s 519us/step\n",
      "3125/3125 [==============================] - 2s 522us/step\n",
      "100000\n",
      "One lap done.\n",
      "15625/15625 [==============================] - 8s 515us/step\n",
      "15625/15625 [==============================] - 8s 519us/step\n",
      "15625/15625 [==============================] - 8s 514us/step\n",
      "15625/15625 [==============================] - 8s 519us/step\n",
      "15625/15625 [==============================] - 8s 514us/step\n",
      "3125/3125 [==============================] - 2s 532us/step\n",
      "3125/3125 [==============================] - 2s 509us/step\n",
      "3125/3125 [==============================] - 2s 511us/step\n",
      "3125/3125 [==============================] - 2s 521us/step\n",
      "3125/3125 [==============================] - 2s 508us/step\n",
      "100000\n",
      "One lap done.\n",
      "15625/15625 [==============================] - 8s 506us/step\n",
      "15625/15625 [==============================] - 8s 515us/step\n",
      "15625/15625 [==============================] - 8s 512us/step\n",
      "15625/15625 [==============================] - 8s 521us/step\n",
      "15625/15625 [==============================] - 8s 514us/step\n",
      "3125/3125 [==============================] - 2s 513us/step\n",
      "3125/3125 [==============================] - 2s 520us/step\n",
      "3125/3125 [==============================] - 2s 511us/step\n",
      "3125/3125 [==============================] - 2s 504us/step\n",
      "3125/3125 [==============================] - 2s 518us/step\n",
      "100000\n",
      "One lap done.\n",
      "15625/15625 [==============================] - 8s 525us/step\n",
      "15625/15625 [==============================] - 8s 509us/step\n",
      "15625/15625 [==============================] - 8s 512us/step\n",
      "15625/15625 [==============================] - 8s 502us/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15625/15625 [==============================] - 8s 481us/step\n",
      "3125/3125 [==============================] - 2s 527us/step\n",
      "3125/3125 [==============================] - 2s 524us/step\n",
      "3125/3125 [==============================] - 2s 512us/step\n",
      "3125/3125 [==============================] - 2s 502us/step\n",
      "3125/3125 [==============================] - 2s 517us/step\n",
      "100000\n",
      "One lap done.\n",
      "15625/15625 [==============================] - 7s 465us/step\n",
      "15625/15625 [==============================] - 8s 481us/step\n",
      "15625/15625 [==============================] - 7s 474us/step\n",
      "15625/15625 [==============================] - 7s 479us/step\n",
      "15625/15625 [==============================] - 7s 468us/step\n",
      "3125/3125 [==============================] - 2s 506us/step\n",
      "3125/3125 [==============================] - 2s 510us/step\n",
      "3125/3125 [==============================] - 2s 519us/step\n",
      "3125/3125 [==============================] - 2s 520us/step\n",
      "3125/3125 [==============================] - 2s 511us/step\n",
      "100000\n",
      "One lap done.\n",
      "15625/15625 [==============================] - 8s 517us/step\n",
      "15625/15625 [==============================] - 8s 522us/step\n",
      "15625/15625 [==============================] - 8s 515us/step\n",
      "15625/15625 [==============================] - 8s 518us/step\n",
      "15625/15625 [==============================] - 7s 473us/step\n",
      "3125/3125 [==============================] - 2s 502us/step\n",
      "3125/3125 [==============================] - 2s 514us/step\n",
      "3125/3125 [==============================] - 2s 528us/step\n",
      "3125/3125 [==============================] - 2s 523us/step\n",
      "3125/3125 [==============================] - 2s 504us/step\n",
      "100000\n",
      "One lap done.\n",
      "15625/15625 [==============================] - 7s 467us/step\n",
      "15625/15625 [==============================] - 7s 469us/step\n",
      "15625/15625 [==============================] - 8s 488us/step\n",
      "15625/15625 [==============================] - 7s 470us/step\n",
      "15625/15625 [==============================] - 7s 465us/step\n",
      "3125/3125 [==============================] - 2s 519us/step\n",
      "3125/3125 [==============================] - 2s 517us/step\n",
      "3125/3125 [==============================] - 2s 543us/step\n",
      "3125/3125 [==============================] - 2s 513us/step\n",
      "3125/3125 [==============================] - 2s 509us/step\n",
      "100000\n",
      "One lap done.\n",
      "15625/15625 [==============================] - 7s 472us/step\n",
      "15625/15625 [==============================] - 7s 475us/step\n",
      "15625/15625 [==============================] - 7s 475us/step\n",
      "15625/15625 [==============================] - 7s 479us/step\n",
      "15625/15625 [==============================] - 8s 484us/step\n",
      "3125/3125 [==============================] - 2s 521us/step\n",
      "3125/3125 [==============================] - 2s 527us/step\n",
      "3125/3125 [==============================] - 2s 542us/step\n",
      "3125/3125 [==============================] - 2s 495us/step\n",
      "3125/3125 [==============================] - 2s 511us/step\n",
      "100000\n",
      "One lap done.\n",
      "15625/15625 [==============================] - 8s 522us/step\n",
      "15625/15625 [==============================] - 8s 522us/step\n",
      "15625/15625 [==============================] - 8s 520us/step\n",
      "15625/15625 [==============================] - 8s 519us/step\n",
      "15625/15625 [==============================] - 8s 518us/step\n",
      "3125/3125 [==============================] - 2s 518us/step\n",
      "3125/3125 [==============================] - 2s 509us/step\n",
      "3125/3125 [==============================] - 2s 513us/step\n",
      "3125/3125 [==============================] - 2s 583us/step\n",
      "3125/3125 [==============================] - 2s 531us/step\n",
      "100000\n",
      "One lap done.\n",
      "15625/15625 [==============================] - 8s 515us/step\n",
      "15625/15625 [==============================] - 8s 508us/step\n",
      "15625/15625 [==============================] - 8s 511us/step\n",
      "15625/15625 [==============================] - 8s 507us/step\n",
      "15625/15625 [==============================] - 8s 513us/step\n",
      "3125/3125 [==============================] - 2s 509us/step\n",
      "3125/3125 [==============================] - 2s 505us/step\n",
      "3125/3125 [==============================] - 2s 507us/step\n",
      "3125/3125 [==============================] - 2s 504us/step\n",
      "3125/3125 [==============================] - 2s 502us/step\n",
      "100000\n",
      "One lap done.\n",
      "15625/15625 [==============================] - 8s 503us/step\n",
      "15625/15625 [==============================] - 8s 517us/step\n",
      "15625/15625 [==============================] - 8s 515us/step\n",
      "15625/15625 [==============================] - 8s 510us/step\n",
      "15625/15625 [==============================] - 8s 514us/step\n",
      "3125/3125 [==============================] - 2s 513us/step\n",
      "3125/3125 [==============================] - 2s 516us/step\n",
      "3125/3125 [==============================] - 2s 507us/step\n",
      "3125/3125 [==============================] - 2s 514us/step\n",
      "3125/3125 [==============================] - 2s 528us/step\n",
      "100000\n",
      "One lap done.\n",
      "15625/15625 [==============================] - 8s 528us/step\n",
      "15625/15625 [==============================] - 8s 523us/step\n",
      "15625/15625 [==============================] - 8s 508us/step\n",
      "15625/15625 [==============================] - 8s 523us/step\n",
      "15625/15625 [==============================] - 8s 505us/step\n",
      "3125/3125 [==============================] - 2s 508us/step\n",
      "3125/3125 [==============================] - 2s 506us/step\n",
      "3125/3125 [==============================] - 2s 498us/step\n",
      "3125/3125 [==============================] - 2s 520us/step\n",
      "3125/3125 [==============================] - 2s 503us/step\n",
      "100000\n",
      "One lap done.\n",
      "15625/15625 [==============================] - 7s 462us/step\n",
      "15625/15625 [==============================] - 7s 466us/step\n",
      "15625/15625 [==============================] - 7s 476us/step\n",
      "15625/15625 [==============================] - 7s 476us/step\n",
      "15625/15625 [==============================] - 8s 484us/step\n",
      "3125/3125 [==============================] - 2s 526us/step\n",
      "3125/3125 [==============================] - 2s 507us/step\n",
      "3125/3125 [==============================] - 2s 512us/step\n",
      "3125/3125 [==============================] - 2s 509us/step\n",
      "3125/3125 [==============================] - 2s 508us/step\n",
      "100000\n",
      "One lap done.\n",
      "15625/15625 [==============================] - 8s 525us/step\n",
      "15625/15625 [==============================] - 8s 529us/step\n",
      "15625/15625 [==============================] - 8s 527us/step\n",
      "15625/15625 [==============================] - 8s 507us/step\n",
      "15625/15625 [==============================] - 8s 511us/step\n",
      "3125/3125 [==============================] - 2s 505us/step\n",
      "3125/3125 [==============================] - 2s 513us/step\n",
      "3125/3125 [==============================] - 2s 505us/step\n",
      "3125/3125 [==============================] - 2s 501us/step\n",
      "3125/3125 [==============================] - 2s 516us/step\n",
      "100000\n",
      "One lap done.\n",
      "15625/15625 [==============================] - 8s 485us/step\n",
      "15625/15625 [==============================] - 7s 470us/step\n",
      "15625/15625 [==============================] - 7s 464us/step\n",
      "15625/15625 [==============================] - 8s 481us/step\n",
      "15625/15625 [==============================] - 7s 476us/step\n",
      "3125/3125 [==============================] - 2s 516us/step\n",
      "3125/3125 [==============================] - 2s 503us/step\n",
      "3125/3125 [==============================] - 2s 507us/step\n",
      "3125/3125 [==============================] - 2s 507us/step\n",
      "3125/3125 [==============================] - 2s 516us/step\n",
      "100000\n",
      "One lap done.\n",
      "15625/15625 [==============================] - 8s 488us/step\n",
      "15625/15625 [==============================] - 8s 493us/step\n",
      "15625/15625 [==============================] - 7s 479us/step\n",
      "15625/15625 [==============================] - 8s 482us/step\n",
      "15625/15625 [==============================] - 7s 474us/step\n",
      "3125/3125 [==============================] - 2s 520us/step\n",
      "3125/3125 [==============================] - 2s 512us/step\n",
      "3125/3125 [==============================] - 2s 506us/step\n",
      "3125/3125 [==============================] - 2s 508us/step\n",
      "3125/3125 [==============================] - 2s 515us/step\n",
      "100000\n",
      "One lap done.\n",
      "15625/15625 [==============================] - 8s 519us/step\n",
      "15625/15625 [==============================] - 8s 515us/step\n",
      "15625/15625 [==============================] - 8s 511us/step\n",
      "15625/15625 [==============================] - 8s 505us/step\n",
      "15625/15625 [==============================] - 8s 526us/step\n",
      "3125/3125 [==============================] - 2s 532us/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3125/3125 [==============================] - 2s 505us/step\n",
      "3125/3125 [==============================] - 2s 504us/step\n",
      "3125/3125 [==============================] - 2s 503us/step\n",
      "3125/3125 [==============================] - 2s 504us/step\n",
      "100000\n",
      "One lap done.\n",
      "15625/15625 [==============================] - 8s 500us/step\n",
      "15625/15625 [==============================] - 8s 516us/step\n",
      "15625/15625 [==============================] - 8s 528us/step\n",
      "15625/15625 [==============================] - 8s 511us/step\n",
      "15625/15625 [==============================] - 8s 504us/step\n",
      "3125/3125 [==============================] - 2s 510us/step\n",
      "3125/3125 [==============================] - 2s 506us/step\n",
      "3125/3125 [==============================] - 2s 513us/step\n",
      "3125/3125 [==============================] - 2s 513us/step\n",
      "3125/3125 [==============================] - 2s 508us/step\n",
      "100000\n",
      "One lap done.\n",
      "15625/15625 [==============================] - 8s 506us/step\n",
      "15625/15625 [==============================] - 8s 503us/step\n",
      "15625/15625 [==============================] - 7s 463us/step\n",
      "15625/15625 [==============================] - 7s 457us/step\n",
      "15625/15625 [==============================] - 7s 460us/step\n",
      "3125/3125 [==============================] - 2s 516us/step\n",
      "3125/3125 [==============================] - 2s 506us/step\n",
      "3125/3125 [==============================] - 2s 499us/step\n",
      "3125/3125 [==============================] - 2s 498us/step\n",
      "3125/3125 [==============================] - 2s 507us/step\n",
      "100000\n",
      "One lap done.\n",
      "15625/15625 [==============================] - 8s 495us/step\n",
      "15625/15625 [==============================] - 8s 517us/step\n",
      "15625/15625 [==============================] - 8s 523us/step\n",
      "15625/15625 [==============================] - 8s 510us/step\n",
      "15625/15625 [==============================] - 8s 529us/step\n",
      "3125/3125 [==============================] - 2s 506us/step\n",
      "3125/3125 [==============================] - 2s 502us/step\n",
      "3125/3125 [==============================] - 2s 498us/step\n",
      "3125/3125 [==============================] - 2s 505us/step\n",
      "3125/3125 [==============================] - 2s 503us/step\n",
      "100000\n",
      "One lap done.\n",
      "15625/15625 [==============================] - 8s 501us/step\n",
      "15625/15625 [==============================] - 8s 504us/step\n",
      "15625/15625 [==============================] - 8s 504us/step\n",
      "15625/15625 [==============================] - 7s 462us/step\n",
      "15625/15625 [==============================] - 7s 462us/step\n",
      "3125/3125 [==============================] - 2s 510us/step\n",
      "3125/3125 [==============================] - 2s 510us/step\n",
      "3125/3125 [==============================] - 2s 505us/step\n",
      "3125/3125 [==============================] - 2s 511us/step\n",
      "3125/3125 [==============================] - 2s 502us/step\n",
      "100000\n",
      "One lap done.\n",
      "15625/15625 [==============================] - 8s 501us/step\n",
      "15625/15625 [==============================] - 8s 502us/step\n",
      "15625/15625 [==============================] - 8s 518us/step\n",
      "15625/15625 [==============================] - 8s 508us/step\n",
      "15625/15625 [==============================] - 8s 507us/step\n",
      "3125/3125 [==============================] - 2s 508us/step\n",
      "3125/3125 [==============================] - 2s 504us/step\n",
      "3125/3125 [==============================] - 2s 499us/step\n",
      "3125/3125 [==============================] - 2s 511us/step\n",
      "3125/3125 [==============================] - 2s 504us/step\n",
      "100000\n",
      "One lap done.\n",
      "0\n",
      "0.7934046\n",
      "0.5902353734002767\n",
      "0.7447040985419635\n",
      "0.7828535895846916\n"
     ]
    }
   ],
   "source": [
    "stream_mean_results=[]\n",
    "current_window = data_init.copy()\n",
    "delta=0.01\n",
    "adwin = ADWIN(delta)\n",
    "detected = False\n",
    "retraining_count=0\n",
    "data_window=data_init.copy()\n",
    "\n",
    "\n",
    "\n",
    "num_models=[]\n",
    "while stream.n_remaining_samples()>1:\n",
    "    \n",
    "    y_pred, y_pred_uncertainty = drift_detection_2(mean_model_weights, data_window.copy())\n",
    "    for i in range(len(y_pred_uncertainty)):\n",
    "        adwin.add_element(y_pred_uncertainty[i])\n",
    "        \n",
    "    incoming_data = stream.next_sample(two_percent)\n",
    "    \n",
    "    data_incoming = pd.DataFrame(incoming_data[0], columns=dataset.columns[:-1])\n",
    "    data_incoming[19]=incoming_data[1]\n",
    "    true_values.append(data_incoming[target_variable])\n",
    "    \n",
    "    y_pred, y_pred_uncertainty = drift_detection_2(mean_model_weights, data_incoming.copy()) \n",
    "    \n",
    "    y_pred_total.append(y_pred)\n",
    "    y_pred_uncertainty_total.append(y_pred_uncertainty)\n",
    "    print(len(y_pred_uncertainty))\n",
    "    \n",
    "    detected=False\n",
    "    for i in range(len(data_incoming[target_variable])):\n",
    "        adwin.add_element(y_pred_uncertainty[i])\n",
    "        if adwin.detected_change():\n",
    "            detected = True\n",
    "            break;\n",
    "    \n",
    "    if detected:\n",
    "        print(\"drift has been detecte models must be retrained\")\n",
    "        \n",
    "        data_window = update_train_data(data_window, data_incoming)\n",
    "        \n",
    "        retraining_count+=1\n",
    "        \n",
    "        Models=[]\n",
    "        val_acc=[]\n",
    "        train_acc=[]\n",
    "        test_acc=[]\n",
    "        val_loss=[]\n",
    "        train_loss=[]\n",
    "        ind=0\n",
    "        add_weights=[]\n",
    "        \n",
    "        samples = generate_samples(data_window, 50, N)\n",
    "        \n",
    "        while ind<len(samples):\n",
    "            \n",
    "            train_data=samples[ind]\n",
    "            ind+=1\n",
    "            \n",
    "            ann_model = get_initial_model_2(dataset.shape[1]-1, 2) #same intial weights\n",
    "            ann_model.set_weights(initial_model.get_weights())\n",
    "            X_train=train_data.drop(columns=[target_variable])\n",
    "            print(X_train.shape[1])\n",
    "\n",
    "            y_train=to_categorical(train_data[target_variable])\n",
    "\n",
    "            ann_model.compile(loss='categorical_crossentropy', optimizer='accuracy', metrics=[f1_m]) # metrics=['accuracy']\n",
    "            history = ann_model.fit(X_train, y_train, epochs=10, validation_split=0.2, verbose=1)\n",
    "            print(history.history['accuracy'][-1])\n",
    "\n",
    "\n",
    "\n",
    "            present=False\n",
    "            for i in range(len(Models)):\n",
    "                if (check_models(Models[i][0], ann_model.get_weights())):\n",
    "                  print(\"if any\")\n",
    "                  Models[i][1]=Models[i][1]+1\n",
    "                  add_weights[i].append(ann_model.get_weights())\n",
    "                  present=True\n",
    "                  break;\n",
    "            if present==False:\n",
    "                add_weights.append([ann_model.get_weights()])\n",
    "                Models.append([ann_model.get_weights(), 1])\n",
    "        \n",
    "        A=np.argsort(np.array(Models).T[1])[::-1][:5]\n",
    "        \n",
    "        num_models.append(len(A))\n",
    "        recommended_models=[]\n",
    "        for i in range(len(A)):\n",
    "            recommended_models.append(add_weights[A[i]])\n",
    "            \n",
    "        mean_model_weights, mean_model_acc, mean_model_loss, mean_model_test_acc, mean_model_test_loss = epsilon_mean_recommendation_2(recommended_models, data_window)\n",
    "    else:\n",
    "        print(\"One lap done.\")\n",
    "        continue\n",
    "    print(\"one thing\")\n",
    "print(retraining_count)\n",
    "\n",
    "predictions=[]\n",
    "for i in y_pred_total:\n",
    "    predictions.append(list(np.argmax(i,axis=1)))\n",
    "predictions = list(np.concatenate(predictions))\n",
    "y_pred_total[0], len(predictions)\n",
    "\n",
    "true_values = list(np.concatenate(true_values))\n",
    "\n",
    "\n",
    "from sklearn.metrics import log_loss, mean_squared_error, accuracy_score, mean_absolute_error, f1_score, matthews_corrcoef, mean_squared_error, mean_squared_log_error, roc_auc_score\n",
    "\n",
    "# this is for classification\n",
    "acc_score = accuracy_score(true_values, predictions)\n",
    "print(acc_score)\n",
    "mcc = matthews_corrcoef(true_values, predictions)\n",
    "f1_score = f1_score(true_values, predictions)\n",
    "auc_score = roc_auc_score(true_values, predictions)\n",
    "\n",
    "print(mcc)\n",
    "print(f1_score)\n",
    "print(auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "95665048",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "287e4a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#try with no retraining\n",
    "\n",
    "#Now lets see for No training\n",
    "stream = DataStream(dataset)\n",
    "\n",
    "two_percent = int(stream.n_remaining_samples()*0.02)\n",
    "five_percent = int(stream.n_remaining_samples()*0.05)\n",
    "initial_data = stream.next_sample(int(stream.n_remaining_samples()*0.10))\n",
    "\n",
    "data_init=pd.DataFrame(initial_data[0], columns=dataset.columns[:-1])\n",
    "data_init[19]=initial_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "80369293",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n",
      "Epoch 1/10\n",
      "12500/12500 [==============================] - 11s 850us/step - loss: 0.4844 - f1_m: 0.7711 - val_loss: 0.4492 - val_f1_m: 0.7927\n",
      "Epoch 2/10\n",
      "12500/12500 [==============================] - 10s 823us/step - loss: 0.4458 - f1_m: 0.7953 - val_loss: 0.4433 - val_f1_m: 0.7945\n",
      "Epoch 3/10\n",
      "12500/12500 [==============================] - 10s 793us/step - loss: 0.4413 - f1_m: 0.7971 - val_loss: 0.4378 - val_f1_m: 0.7987\n",
      "Epoch 4/10\n",
      "12500/12500 [==============================] - 11s 885us/step - loss: 0.4391 - f1_m: 0.7979 - val_loss: 0.4367 - val_f1_m: 0.7985\n",
      "Epoch 5/10\n",
      "12500/12500 [==============================] - 11s 894us/step - loss: 0.4377 - f1_m: 0.7982 - val_loss: 0.4436 - val_f1_m: 0.7931\n",
      "Epoch 6/10\n",
      "12500/12500 [==============================] - 10s 813us/step - loss: 0.4367 - f1_m: 0.7986 - val_loss: 0.4355 - val_f1_m: 0.7995\n",
      "Epoch 7/10\n",
      "12500/12500 [==============================] - 10s 822us/step - loss: 0.4362 - f1_m: 0.7986 - val_loss: 0.4346 - val_f1_m: 0.7995\n",
      "Epoch 8/10\n",
      "12500/12500 [==============================] - 10s 794us/step - loss: 0.4357 - f1_m: 0.7992 - val_loss: 0.4367 - val_f1_m: 0.7972\n",
      "Epoch 9/10\n",
      "12500/12500 [==============================] - 10s 774us/step - loss: 0.4356 - f1_m: 0.7990 - val_loss: 0.4366 - val_f1_m: 0.7982\n",
      "Epoch 10/10\n",
      "12500/12500 [==============================] - 10s 826us/step - loss: 0.4353 - f1_m: 0.7992 - val_loss: 0.4350 - val_f1_m: 0.7986\n",
      "0.7991799116134644\n"
     ]
    }
   ],
   "source": [
    "\n",
    "Models=[]\n",
    "val_acc=[]\n",
    "train_acc=[]\n",
    "test_acc=[]\n",
    "val_loss=[]\n",
    "train_loss=[]\n",
    "ind=0\n",
    "add_weights=[]\n",
    "\n",
    "ann_model=get_initial_model(dataset.shape[1]-1, 2) #same intial weights\n",
    "\n",
    "#ann_model.set_weights(initial_model.get_weights())\n",
    "X_train=data_init.drop(columns=[target_variable])\n",
    "print(X_train.shape[1])\n",
    "\n",
    "\n",
    "y_train=to_categorical(data_init[target_variable])\n",
    "\n",
    "ann_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[f1_m]) # metrics=['accuracy']\n",
    "history = ann_model.fit(X_train, y_train, epochs=10, validation_split=0.2, verbose=1)\n",
    "print(history.history['f1_m'][-1])\n",
    "\n",
    "add_weights.append([ann_model.get_weights()])\n",
    "Models.append([ann_model.get_weights(), 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "177ccd95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "andr aara h\n",
      "andr aara h\n",
      "Done\n",
      "10938/10938 [==============================] - 7s 634us/step - loss: 0.4340 - accuracy: 0.7998\n",
      "4688/4688 [==============================] - 3s 592us/step - loss: 0.4356 - accuracy: 0.7987\n"
     ]
    }
   ],
   "source": [
    "mean_model_weights, mean_model_acc, mean_model_loss, mean_model_test_acc, mean_model_test_loss = epsilon_mean_recommendation(add_weights, data_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f59b6700",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3125/3125 [==============================] - 2s 475us/step\n",
      "one lap done\n",
      "3125/3125 [==============================] - 2s 708us/step\n",
      "one lap done\n",
      "3125/3125 [==============================] - 2s 478us/step\n",
      "one lap done\n",
      "3125/3125 [==============================] - 1s 462us/step\n",
      "one lap done\n",
      "3125/3125 [==============================] - 2s 500us/step\n",
      "one lap done\n",
      "3125/3125 [==============================] - 2s 521us/step\n",
      "one lap done\n",
      "3125/3125 [==============================] - 2s 477us/step\n",
      "one lap done\n",
      "3125/3125 [==============================] - 2s 475us/step\n",
      "one lap done\n",
      "3125/3125 [==============================] - 2s 509us/step\n",
      "one lap done\n",
      "3125/3125 [==============================] - 2s 477us/step\n",
      "one lap done\n",
      "3125/3125 [==============================] - 2s 485us/step\n",
      "one lap done\n",
      "3125/3125 [==============================] - 1s 469us/step\n",
      "one lap done\n",
      "3125/3125 [==============================] - 1s 470us/step\n",
      "one lap done\n",
      "3125/3125 [==============================] - 2s 473us/step\n",
      "one lap done\n",
      "3125/3125 [==============================] - 2s 485us/step\n",
      "one lap done\n",
      "3125/3125 [==============================] - 2s 552us/step\n",
      "one lap done\n",
      "3125/3125 [==============================] - 2s 487us/step\n",
      "one lap done\n",
      "3125/3125 [==============================] - 2s 485us/step\n",
      "one lap done\n",
      "3125/3125 [==============================] - 2s 472us/step\n",
      "one lap done\n",
      "3125/3125 [==============================] - 2s 478us/step\n",
      "one lap done\n",
      "3125/3125 [==============================] - 2s 497us/step\n",
      "one lap done\n",
      "3125/3125 [==============================] - 2s 476us/step\n",
      "one lap done\n",
      "3125/3125 [==============================] - 2s 484us/step\n",
      "one lap done\n",
      "3125/3125 [==============================] - 2s 479us/step\n",
      "one lap done\n",
      "3125/3125 [==============================] - 2s 484us/step\n",
      "one lap done\n",
      "3125/3125 [==============================] - 2s 510us/step\n",
      "one lap done\n",
      "3125/3125 [==============================] - 2s 481us/step\n",
      "one lap done\n",
      "3125/3125 [==============================] - 2s 474us/step\n",
      "one lap done\n",
      "3125/3125 [==============================] - 2s 474us/step\n",
      "one lap done\n",
      "3125/3125 [==============================] - 2s 482us/step\n",
      "one lap done\n",
      "3125/3125 [==============================] - 1s 471us/step\n",
      "one lap done\n",
      "3125/3125 [==============================] - 1s 468us/step\n",
      "one lap done\n",
      "3125/3125 [==============================] - 1s 471us/step\n",
      "one lap done\n",
      "3125/3125 [==============================] - 1s 471us/step\n",
      "one lap done\n",
      "3125/3125 [==============================] - 2s 477us/step\n",
      "one lap done\n",
      "3125/3125 [==============================] - 2s 477us/step\n",
      "one lap done\n",
      "3125/3125 [==============================] - 2s 473us/step\n",
      "one lap done\n",
      "3125/3125 [==============================] - 2s 475us/step\n",
      "one lap done\n",
      "3125/3125 [==============================] - 2s 474us/step\n",
      "one lap done\n",
      "3125/3125 [==============================] - 2s 479us/step\n",
      "one lap done\n",
      "3125/3125 [==============================] - 2s 476us/step\n",
      "one lap done\n",
      "3125/3125 [==============================] - 2s 480us/step\n",
      "one lap done\n",
      "3125/3125 [==============================] - 2s 477us/step\n",
      "one lap done\n",
      "3125/3125 [==============================] - 2s 473us/step\n",
      "one lap done\n",
      "3125/3125 [==============================] - 2s 479us/step\n",
      "one lap done\n",
      "3125/3125 [==============================] - 1s 469us/step\n",
      "one lap done\n",
      "0\n",
      "0.7985371739130435\n",
      "0.5968974016793225\n",
      "0.7579546383523982\n",
      "0.7899939502402902\n"
     ]
    }
   ],
   "source": [
    "#right now trying for mean models\n",
    "stream_mean_results=[]\n",
    "current_window = data_init.copy()\n",
    "#delta=0.95\n",
    "#adwin = ADWIN(delta)\n",
    "#detected = False\n",
    "#retraining_count=0\n",
    "data_window=data_init.copy()\n",
    "y_pred_total = []\n",
    "y_pred_uncertainty_total = []\n",
    "true_values = []\n",
    "num_models=[]\n",
    "while stream.n_remaining_samples()>1:\n",
    "    incoming_data = stream.next_sample(two_percent)\n",
    "    #rint(incoming_data)\n",
    "    \n",
    "    data_incoming = pd.DataFrame(incoming_data[0], columns=dataset.columns[:-1])\n",
    "    data_incoming[19]=incoming_data[1]\n",
    "    true_values.append(data_incoming[target_variable])\n",
    "    #print(data_incoming)\n",
    "    y_pred, y_pred_uncertainty = drift_detection(mean_model_weights, data_incoming.copy()) \n",
    "    #detected, evaluate, adwin = drift_detection_each_model(mean_model_weights, data_incoming.copy(), adwin)\n",
    "    y_pred_total.append(y_pred)\n",
    "    print(\"one lap done\")\n",
    "\n",
    "print(retraining_count)\n",
    "\n",
    "predictions=[]\n",
    "for i in y_pred_total:\n",
    "    predictions.append(list(np.argmax(i,axis=1)))\n",
    "predictions = list(np.concatenate(predictions))\n",
    "y_pred_total[0], len(predictions)\n",
    "\n",
    "true_values = list(np.concatenate(true_values))\n",
    "\n",
    "\n",
    "from sklearn.metrics import log_loss, mean_squared_error, accuracy_score, mean_absolute_error, f1_score, matthews_corrcoef, mean_squared_error, mean_squared_log_error, roc_auc_score\n",
    "\n",
    "# this is for classification\n",
    "acc_score = accuracy_score(true_values, predictions)\n",
    "print(acc_score)\n",
    "mcc = matthews_corrcoef(true_values, predictions)\n",
    "f1_score = f1_score(true_values, predictions)\n",
    "auc_score = roc_auc_score(true_values, predictions)\n",
    "\n",
    "print(mcc)\n",
    "print(f1_score)\n",
    "print(auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "18ac5a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#experiments for ADWIN unlimited\n",
    "\n",
    "#try with no retraining\n",
    "\n",
    "#Now lets see for No training\n",
    "stream = DataStream(dataset)\n",
    "\n",
    "two_percent = int(stream.n_remaining_samples()*0.02)\n",
    "five_percent = int(stream.n_remaining_samples()*0.05)\n",
    "initial_data = stream.next_sample(int(stream.n_remaining_samples()*0.10))\n",
    "\n",
    "data_init=pd.DataFrame(initial_data[0], columns=dataset.columns[:-1])\n",
    "data_init[19]=initial_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bbdce4b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "12500/12500 [==============================] - 12s 939us/step - loss: 0.4869 - accuracy: 0.7697 - val_loss: 0.4553 - val_accuracy: 0.7897\n",
      "Epoch 2/10\n",
      "12500/12500 [==============================] - 10s 820us/step - loss: 0.4471 - accuracy: 0.7946 - val_loss: 0.4404 - val_accuracy: 0.7977\n",
      "Epoch 3/10\n",
      "12500/12500 [==============================] - 10s 808us/step - loss: 0.4408 - accuracy: 0.7975 - val_loss: 0.4374 - val_accuracy: 0.7981\n",
      "Epoch 4/10\n",
      "12500/12500 [==============================] - 10s 798us/step - loss: 0.4387 - accuracy: 0.7973 - val_loss: 0.4400 - val_accuracy: 0.7965\n",
      "Epoch 5/10\n",
      "12500/12500 [==============================] - 10s 781us/step - loss: 0.4378 - accuracy: 0.7981 - val_loss: 0.4381 - val_accuracy: 0.7975\n",
      "Epoch 6/10\n",
      "12500/12500 [==============================] - 10s 781us/step - loss: 0.4370 - accuracy: 0.7985 - val_loss: 0.4364 - val_accuracy: 0.7984\n",
      "Epoch 7/10\n",
      "12500/12500 [==============================] - 10s 788us/step - loss: 0.4366 - accuracy: 0.7981 - val_loss: 0.4405 - val_accuracy: 0.7954\n",
      "Epoch 8/10\n",
      "12500/12500 [==============================] - 10s 784us/step - loss: 0.4361 - accuracy: 0.7987 - val_loss: 0.4351 - val_accuracy: 0.7999\n",
      "Epoch 9/10\n",
      "12500/12500 [==============================] - 10s 786us/step - loss: 0.4359 - accuracy: 0.7989 - val_loss: 0.4332 - val_accuracy: 0.8001\n",
      "Epoch 10/10\n",
      "12500/12500 [==============================] - 10s 790us/step - loss: 0.4357 - accuracy: 0.7987 - val_loss: 0.4330 - val_accuracy: 0.7998\n",
      "3125/3125 [==============================] - 2s 502us/step\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 2s 519us/step\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 2s 527us/step\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 2s 494us/step\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 2s 489us/step\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 2s 521us/step\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 2s 524us/step\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 2s 636us/step\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 2s 656us/step\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 2s 601us/step\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 2s 602us/step\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 2s 678us/step\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 2s 561us/step\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 2s 535us/step\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 2s 527us/step\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 2s 650us/step\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 2s 531us/step\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 2s 557us/step\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 2s 533us/step\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 2s 526us/step\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 2s 542us/step\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 2s 497us/step\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 2s 535us/step\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 2s 532us/step\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 2s 533us/step\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 2s 537us/step\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 2s 548us/step\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 2s 600us/step\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 2s 591us/step\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 2s 597us/step\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 2s 566us/step\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 2s 538us/step\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 2s 548us/step\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 2s 548us/step\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 2s 533us/step\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 2s 546us/step\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 2s 540us/step\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 2s 547us/step\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 2s 533us/step\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 2s 521us/step\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 2s 540us/step\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 2s 528us/step\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 2s 538us/step\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 2s 551us/step\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 2s 546us/step\n",
      "One lap done.\n"
     ]
    }
   ],
   "source": [
    "#compare with ADWIN unlimited availability\n",
    "stream_adwin_results=[]\n",
    "current_window = pd.DataFrame(data_init)\n",
    "adwin_model = get_initial_model(dataset.shape[1]-1, 2)\n",
    "X_train = current_window.drop(columns=[target_variable])\n",
    "y_train = to_categorical(current_window[target_variable])\n",
    "true_adwin_values = []\n",
    "adwin_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history = adwin_model.fit(X_train, y_train, epochs=10, validation_split=0.2, verbose=1)\n",
    "delta = 0.01\n",
    "adwin = ADWIN(delta)\n",
    "detected = False\n",
    "retraining_count=0\n",
    "\n",
    "y_adwin_pred = []\n",
    "retraining_count_adwin = 0\n",
    "#y_pred_uncertainty_total = []\n",
    "#num_models=[]\n",
    "while stream.n_remaining_samples()>1:\n",
    "    incoming_data = stream.next_sample(two_percent)\n",
    "    #rint(incoming_data)\n",
    "    \n",
    "    data_incoming = pd.DataFrame(incoming_data[0], columns=dataset.columns[:-1])\n",
    "    data_incoming[19] = incoming_data[1]\n",
    "    true_adwin_values.append(data_incoming[target_variable])\n",
    "    \n",
    "    y_pred = list(np.argmax(adwin_model.predict(data_incoming[data_incoming.columns[:-1]]), axis=1))\n",
    "    detected = False\n",
    "    for i in range(data_incoming.shape[0]):\n",
    "        adwin.add_element(data_incoming[target_variable][i])\n",
    "        if adwin.detected_change():\n",
    "            detected = True\n",
    "            break;\n",
    "    if detected:\n",
    "        print(\"drift has been detecte models must be retrained\")\n",
    "        current_window = update_train_data(current_window, data_incoming)\n",
    "        retraining_count_adwin += 1\n",
    "        initial_model = get_initial_model(dataset.shape[1]-1, 2)\n",
    "        ind = 0\n",
    "        adwin_model = get_initial_model(dataset.shape[1]-1, 2) #same intial weights\n",
    "        adwin_model.set_weights(initial_model.get_weights())\n",
    "        X_train = current_window.drop(columns=[target_variable])\n",
    "        y_train = to_categorical(current_window[target_variable])\n",
    "        adwin_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) # metrics=['accuracy']\n",
    "        history = ann_model.fit(X_train, y_train, epochs=10, validation_split=0.2, verbose=1)\n",
    "        present = False\n",
    "        y_pred = list(np.argmax(adwin_model.predict(X_train), axis=1))\n",
    "        y_adwin_pred.append(y_pred)\n",
    "        #print(len(y_pred_uncertainty))\n",
    "        detected = False  \n",
    "    else:\n",
    "        y_adwin_pred.append(y_pred)\n",
    "        print(\"One lap done.\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "83427a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4701315555555556\n",
      "0.10401617204791203\n",
      "0.6332566954576524\n",
      "0.5116755918077118\n"
     ]
    }
   ],
   "source": [
    "print(retraining_count_adwin)\n",
    "y_adwin_pred_total = list(np.concatenate(y_adwin_pred))\n",
    "y_adwin_pred_total\n",
    "true_adwin_values_total = list(np.concatenate(true_adwin_values))\n",
    "print(true_adwin_values_total)\n",
    "#here for computing metrics \n",
    "#retraining count = no. of drift detected\n",
    "#accuracy score, mean_absolute error for regressiom, f1_score, matthews_score \n",
    "from sklearn.metrics import log_loss, mean_squared_error, accuracy_score, mean_absolute_error, f1_score, matthews_corrcoef, mean_squared_error, mean_squared_log_error, roc_auc_score\n",
    "\n",
    "# this is for classification\n",
    "acc_score = accuracy_score(true_adwin_values_total, y_adwin_pred_total)\n",
    "print(acc_score)\n",
    "mcc = matthews_corrcoef(true_adwin_values_total, y_adwin_pred_total)\n",
    "f1_score = f1_score(true_adwin_values_total, y_adwin_pred_total)\n",
    "auc_score = roc_auc_score(true_adwin_values_total, y_adwin_pred_total)\n",
    "\n",
    "\n",
    "print(mcc)\n",
    "print(f1_score)\n",
    "print(auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3eca9d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experiments ADWIN limited label availability\n",
    "\n",
    "\n",
    "stream = DataStream(dataset)\n",
    "\n",
    "two_percent = int(stream.n_remaining_samples()*0.02)\n",
    "five_percent = int(stream.n_remaining_samples()*0.05)\n",
    "initial_data = stream.next_sample(int(stream.n_remaining_samples()*0.10))\n",
    "\n",
    "data_init=pd.DataFrame(initial_data[0], columns=dataset.columns[:-1])\n",
    "data_init[19]=initial_data[1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ed8c5f7e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-06 19:58:46.726202: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n",
      "Epoch 1/10\n",
      "12500/12500 [==============================] - 11s 869us/step - loss: 0.4893 - accuracy: 0.7685 - val_loss: 0.4611 - val_accuracy: 0.7855\n",
      "Epoch 2/10\n",
      "12500/12500 [==============================] - 11s 886us/step - loss: 0.4492 - accuracy: 0.7932 - val_loss: 0.4531 - val_accuracy: 0.7891\n",
      "Epoch 3/10\n",
      "12500/12500 [==============================] - 10s 825us/step - loss: 0.4429 - accuracy: 0.7958 - val_loss: 0.4458 - val_accuracy: 0.7928\n",
      "Epoch 4/10\n",
      "12500/12500 [==============================] - 11s 858us/step - loss: 0.4414 - accuracy: 0.7970 - val_loss: 0.4384 - val_accuracy: 0.7980\n",
      "Epoch 5/10\n",
      "12500/12500 [==============================] - 10s 834us/step - loss: 0.4402 - accuracy: 0.7974 - val_loss: 0.4376 - val_accuracy: 0.7982\n",
      "Epoch 6/10\n",
      "12500/12500 [==============================] - 10s 835us/step - loss: 0.4398 - accuracy: 0.7976 - val_loss: 0.4378 - val_accuracy: 0.7983\n",
      "Epoch 7/10\n",
      "12500/12500 [==============================] - 11s 841us/step - loss: 0.4390 - accuracy: 0.7978 - val_loss: 0.4378 - val_accuracy: 0.7975\n",
      "Epoch 8/10\n",
      "12500/12500 [==============================] - 10s 839us/step - loss: 0.4388 - accuracy: 0.7980 - val_loss: 0.4378 - val_accuracy: 0.7983\n",
      "Epoch 9/10\n",
      "12500/12500 [==============================] - 10s 836us/step - loss: 0.4386 - accuracy: 0.7976 - val_loss: 0.4366 - val_accuracy: 0.7993\n",
      "Epoch 10/10\n",
      "12500/12500 [==============================] - 11s 867us/step - loss: 0.4384 - accuracy: 0.7979 - val_loss: 0.4359 - val_accuracy: 0.7994\n",
      "0.797902524471283\n",
      "1\n",
      "[1]\n",
      "[0]\n",
      "andr aara h\n",
      "andr aara h\n",
      "Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ayushkv/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:36: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "/Users/ayushkv/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:37: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10938/10938 [==============================] - 7s 642us/step - loss: 0.4363 - accuracy: 0.7992\n",
      "4688/4688 [==============================] - 3s 629us/step - loss: 0.4347 - accuracy: 0.8000\n",
      "15625/15625 [==============================] - 8s 500us/step\n"
     ]
    }
   ],
   "source": [
    "# adding dense layer\n",
    "#initial_model= get_initial_model(dataset.shape[1]-1, 2)\n",
    "Models=[]\n",
    "val_acc=[]\n",
    "train_acc=[]\n",
    "test_acc=[]\n",
    "val_loss=[]\n",
    "train_loss=[]\n",
    "ind=0\n",
    "add_weights=[]\n",
    "true_values=[]\n",
    "\n",
    "\n",
    "ann_model=get_initial_model(dataset.shape[1]-1, 2) \n",
    "\n",
    "\n",
    "X_train=data_init.drop(columns=[target_variable])\n",
    "print(X_train.shape[1])\n",
    "\n",
    "y_train=to_categorical(data_init[target_variable])\n",
    "\n",
    "ann_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) # metrics=['accuracy']\n",
    "history = ann_model.fit(X_train, y_train, epochs=10, validation_split=0.2, verbose=1)\n",
    "print(history.history['accuracy'][-1])\n",
    "\n",
    "\n",
    "\n",
    "add_weights.append([ann_model.get_weights()])\n",
    "Models.append([ann_model.get_weights(), 1])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(len(Models))\n",
    "\n",
    "A=np.argsort(np.array(Models).T[1])[::-1][:5]\n",
    "print(np.array(Models).T[1])\n",
    "print(A)\n",
    "recommended_models=[]\n",
    "for i in range(len(A)):\n",
    "    recommended_models.append(add_weights[A[i]])\n",
    "#add_weights=add_weights[A]\n",
    "# Now trying to generate Streaming settings for the dataset\n",
    "# lets find the outputs from all the \n",
    "mean_model_weights, mean_model_acc, mean_model_loss, mean_model_test_acc, mean_model_test_loss = epsilon_mean_recommendation(recommended_models, data_init)\n",
    "\n",
    "\n",
    "y_pred_total=[]\n",
    "y_pred_uncertainty_total=[]\n",
    "\n",
    "y_pred, y_pred_uncertainty = drift_detection(mean_model_weights, data_init.copy())\n",
    "\n",
    "y_pred_total.append(y_pred)\n",
    "y_pred_uncertainty_total.append(y_pred_uncertainty)\n",
    "\n",
    "true_values.append(data_init.copy()[target_variable])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d3d151ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3125/3125 [==============================] - 2s 534us/step\n",
      "100000\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 2s 546us/step\n",
      "100000\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 2s 521us/step\n",
      "100000\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 2s 512us/step\n",
      "100000\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 2s 521us/step\n",
      "100000\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 2s 521us/step\n",
      "100000\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 2s 516us/step\n",
      "100000\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 2s 527us/step\n",
      "100000\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 2s 530us/step\n",
      "100000\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 2s 525us/step\n",
      "100000\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 2s 538us/step\n",
      "100000\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 2s 508us/step\n",
      "100000\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 2s 519us/step\n",
      "100000\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 2s 534us/step\n",
      "100000\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 2s 554us/step\n",
      "100000\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 2s 519us/step\n",
      "100000\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 2s 552us/step\n",
      "100000\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 2s 507us/step\n",
      "100000\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 2s 545us/step\n",
      "100000\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 2s 554us/step\n",
      "100000\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 2s 556us/step\n",
      "100000\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 2s 533us/step\n",
      "100000\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 2s 538us/step\n",
      "100000\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 2s 543us/step\n",
      "100000\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 2s 492us/step\n",
      "100000\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 2s 538us/step\n",
      "100000\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 2s 550us/step\n",
      "100000\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 2s 508us/step\n",
      "100000\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 2s 521us/step\n",
      "100000\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 2s 552us/step\n",
      "100000\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 2s 533us/step\n",
      "100000\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 2s 540us/step\n",
      "100000\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 2s 518us/step\n",
      "100000\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 2s 528us/step\n",
      "100000\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 2s 507us/step\n",
      "100000\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 2s 502us/step\n",
      "100000\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 2s 529us/step\n",
      "100000\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 2s 540us/step\n",
      "100000\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 2s 574us/step\n",
      "100000\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 2s 562us/step\n",
      "100000\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 2s 524us/step\n",
      "100000\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 2s 523us/step\n",
      "100000\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 2s 523us/step\n",
      "100000\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 2s 548us/step\n",
      "100000\n",
      "One lap done.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "stream_mean_results=[]\n",
    "current_window = data_init.copy()\n",
    "delta=0.01\n",
    "adwin = ADWIN(delta)\n",
    "detected = False\n",
    "retraining_count=0\n",
    "data_window=data_init.copy()\n",
    "while stream.n_remaining_samples()>1:\n",
    "    incoming_data = stream.next_sample(two_percent)\n",
    "    \n",
    "    \n",
    "    \n",
    "    data_incoming = pd.DataFrame(incoming_data[0], columns=dataset.columns[:-1])\n",
    "    data_incoming[19]=incoming_data[1]\n",
    "    true_values.append(data_incoming[target_variable])\n",
    "    \n",
    "    \n",
    "    y_pred, y_pred_uncertainty = drift_detection(mean_model_weights, data_incoming.copy()) \n",
    "    \n",
    "    \n",
    "    y_pred_total.append(y_pred)\n",
    "    y_pred_uncertainty_total.append(y_pred_uncertainty)\n",
    "    print(len(y_pred_uncertainty))\n",
    "    \n",
    "    detected=False\n",
    "    for i in range(len(data_incoming[target_variable])):\n",
    "        adwin.add_element(y_pred_uncertainty[i])\n",
    "        if adwin.detected_change():\n",
    "            detected = True\n",
    "            break;\n",
    "    \n",
    "    if detected:\n",
    "        print(\"drift has been detecte models must be retrained\")\n",
    "        \n",
    "        data_window = update_train_data(data_window, data_incoming)\n",
    "        \n",
    "        retraining_count+=1\n",
    "        \n",
    "        Models=[]\n",
    "        val_acc=[]\n",
    "        train_acc=[]\n",
    "        test_acc=[]\n",
    "        val_loss=[]\n",
    "        train_loss=[]\n",
    "        ind=0\n",
    "        add_weights=[]\n",
    "        true_values=[]\n",
    "\n",
    "\n",
    "        ann_model=get_initial_model(dataset.shape[1]-1, 2) \n",
    "\n",
    "\n",
    "        X_train=data_init.drop(columns=[target_variable])\n",
    "        print(X_train.shape[1])\n",
    "\n",
    "        y_train=to_categorical(data_init[target_variable])\n",
    "\n",
    "        ann_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) # metrics=['accuracy']\n",
    "        history = ann_model.fit(X_train, y_train, epochs=10, validation_split=0.2, verbose=1)\n",
    "        print(history.history['accuracy'][-1])\n",
    "\n",
    "\n",
    "\n",
    "        add_weights.append([ann_model.get_weights()])\n",
    "        Models.append([ann_model.get_weights(), 1])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        print(len(Models))\n",
    "\n",
    "        A=np.argsort(np.array(Models).T[1])[::-1][:5]\n",
    "        print(np.array(Models).T[1])\n",
    "        print(A)\n",
    "        recommended_models=[]\n",
    "        for i in range(len(A)):\n",
    "            recommended_models.append(add_weights[A[i]])\n",
    "        #add_weights=add_weights[A]\n",
    "        # Now trying to generate Streaming settings for the dataset\n",
    "        # lets find the outputs from all the \n",
    "        mean_model_weights, mean_model_acc, mean_model_loss, mean_model_test_acc, mean_model_test_loss = epsilon_mean_recommendation(recommended_models, data_init)\n",
    "\n",
    "        \n",
    "    else:\n",
    "        print(\"One lap done.\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e70bee93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0.7984167346938775\n",
      "0.5936715004952063\n",
      "0.7660591873159752\n",
      "0.7923852650778145\n"
     ]
    }
   ],
   "source": [
    "print(retraining_count)\n",
    "\n",
    "predictions=[]\n",
    "for i in y_pred_total:\n",
    "    predictions.append(list(np.argmax(i,axis=1)))\n",
    "predictions = list(np.concatenate(predictions))\n",
    "y_pred_total[0], len(predictions)\n",
    "\n",
    "true_values = list(np.concatenate(true_values))\n",
    "\n",
    "\n",
    "from sklearn.metrics import log_loss, mean_squared_error, accuracy_score, mean_absolute_error, f1_score, matthews_corrcoef, mean_squared_error, mean_squared_log_error, roc_auc_score\n",
    "\n",
    "# this is for classification\n",
    "acc_score = accuracy_score(true_values, predictions)\n",
    "print(acc_score)\n",
    "mcc = matthews_corrcoef(true_values, predictions)\n",
    "f1_score = f1_score(true_values, predictions)\n",
    "auc_score = roc_auc_score(true_values, predictions)\n",
    "\n",
    "print(mcc)\n",
    "print(f1_score)\n",
    "print(auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "87a0e70a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DP-SGD with sampling rate = 0.0064% and noise_multiplier = 0.262 iterated over 156250 steps satisfies differential privacy with eps = 50.1 and delta = 1e-05.\n",
      "The optimal RDP order is 1.5.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "50.14881264838195"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "compute_dp_sgd_privacy.compute_dp_sgd_privacy(n=5*two_percent,\n",
    "                                              batch_size=32,\n",
    "                                              noise_multiplier=0.262,\n",
    "                                              epochs=10,\n",
    "                                              delta=1e-5)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ea169119",
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_norm_clip = 1.0\n",
    "noise_multiplier = 0.262\n",
    "num_microbatches = 1\n",
    "\n",
    "optimizer = tensorflow_privacy.DPKerasAdamOptimizer(\n",
    "    l2_norm_clip=l2_norm_clip,\n",
    "    noise_multiplier=noise_multiplier,\n",
    "    num_microbatches=num_microbatches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5f53e320",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experiments ADWIN limited label availability\n",
    "\n",
    "\n",
    "stream = DataStream(dataset)\n",
    "\n",
    "two_percent = int(stream.n_remaining_samples()*0.02)\n",
    "five_percent = int(stream.n_remaining_samples()*0.05)\n",
    "initial_data = stream.next_sample(int(stream.n_remaining_samples()*0.10))\n",
    "\n",
    "data_init=pd.DataFrame(initial_data[0], columns=dataset.columns[:-1])\n",
    "data_init[19]=initial_data[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c6337196",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n",
      "Epoch 1/10\n",
      "12500/12500 [==============================] - 11s 857us/step - loss: 0.6074 - accuracy: 0.6679 - val_loss: 0.5264 - val_accuracy: 0.7482\n",
      "Epoch 2/10\n",
      "12500/12500 [==============================] - 10s 811us/step - loss: 0.5065 - accuracy: 0.7588 - val_loss: 0.4907 - val_accuracy: 0.7681\n",
      "Epoch 3/10\n",
      "12500/12500 [==============================] - 10s 838us/step - loss: 0.4805 - accuracy: 0.7755 - val_loss: 0.4715 - val_accuracy: 0.7808\n",
      "Epoch 4/10\n",
      "12500/12500 [==============================] - 10s 803us/step - loss: 0.4664 - accuracy: 0.7839 - val_loss: 0.4585 - val_accuracy: 0.7882\n",
      "Epoch 5/10\n",
      "12500/12500 [==============================] - 10s 778us/step - loss: 0.4601 - accuracy: 0.7881 - val_loss: 0.4592 - val_accuracy: 0.7873\n",
      "Epoch 6/10\n",
      "12500/12500 [==============================] - 11s 911us/step - loss: 0.4590 - accuracy: 0.7874 - val_loss: 0.4651 - val_accuracy: 0.7837\n",
      "Epoch 7/10\n",
      "12500/12500 [==============================] - 11s 848us/step - loss: 0.4588 - accuracy: 0.7884 - val_loss: 0.4572 - val_accuracy: 0.7883\n",
      "Epoch 8/10\n",
      "12500/12500 [==============================] - 10s 791us/step - loss: 0.4576 - accuracy: 0.7881 - val_loss: 0.4550 - val_accuracy: 0.7895\n",
      "Epoch 9/10\n",
      "12500/12500 [==============================] - 10s 823us/step - loss: 0.4563 - accuracy: 0.7893 - val_loss: 0.4541 - val_accuracy: 0.7914\n",
      "Epoch 10/10\n",
      "12500/12500 [==============================] - 10s 809us/step - loss: 0.4570 - accuracy: 0.7899 - val_loss: 0.4562 - val_accuracy: 0.7881\n",
      "0.7898874878883362\n",
      "1\n",
      "[1]\n",
      "[0]\n",
      "yhn tk\n",
      "1\n",
      "Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ayushkv/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:36: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "/Users/ayushkv/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:37: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10938/10938 [==============================] - 6s 495us/step - loss: 0.4560 - accuracy: 0.7887\n",
      "4688/4688 [==============================] - 2s 480us/step - loss: 0.4549 - accuracy: 0.7899\n",
      "15625/15625 [==============================] - 6s 378us/step\n"
     ]
    }
   ],
   "source": [
    "# adding dense layer\n",
    "#initial_model= get_initial_model(dataset.shape[1]-1, 2)\n",
    "Models=[]\n",
    "val_acc=[]\n",
    "train_acc=[]\n",
    "test_acc=[]\n",
    "val_loss=[]\n",
    "train_loss=[]\n",
    "ind=0\n",
    "add_weights=[]\n",
    "true_values=[]\n",
    "\n",
    "\n",
    "ann_model=get_initial_model_2(dataset.shape[1]-1, 2) \n",
    "\n",
    "\n",
    "X_train=data_init.drop(columns=[target_variable])\n",
    "print(X_train.shape[1])\n",
    "\n",
    "y_train=to_categorical(data_init[target_variable])\n",
    "\n",
    "ann_model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy']) # metrics=['accuracy']\n",
    "history = ann_model.fit(X_train, y_train, epochs=10, validation_split=0.2, verbose=1)\n",
    "print(history.history['accuracy'][-1])\n",
    "\n",
    "\n",
    "\n",
    "add_weights.append([ann_model.get_weights()])\n",
    "Models.append([ann_model.get_weights(), 1])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(len(Models))\n",
    "\n",
    "A=np.argsort(np.array(Models).T[1])[::-1][:5]\n",
    "print(np.array(Models).T[1])\n",
    "print(A)\n",
    "recommended_models=[]\n",
    "for i in range(len(A)):\n",
    "    recommended_models.append(add_weights[A[i]])\n",
    "#add_weights=add_weights[A]\n",
    "# Now trying to generate Streaming settings for the dataset\n",
    "# lets find the outputs from all the \n",
    "mean_model_weights, mean_model_acc, mean_model_loss, mean_model_test_acc, mean_model_test_loss = epsilon_mean_recommendation_2(recommended_models, data_init)\n",
    "\n",
    "\n",
    "y_pred_total=[]\n",
    "y_pred_uncertainty_total=[]\n",
    "\n",
    "y_pred, y_pred_uncertainty = drift_detection_2(mean_model_weights, data_init.copy())\n",
    "\n",
    "y_pred_total.append(y_pred)\n",
    "y_pred_uncertainty_total.append(y_pred_uncertainty)\n",
    "\n",
    "true_values.append(data_init.copy()[target_variable])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "542bfdb7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3125/3125 [==============================] - 1s 378us/step\n",
      "100000\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 1s 413us/step\n",
      "100000\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 1s 421us/step\n",
      "100000\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 1s 425us/step\n",
      "100000\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 1s 409us/step\n",
      "100000\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 1s 411us/step\n",
      "100000\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 1s 407us/step\n",
      "100000\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 1s 375us/step\n",
      "100000\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 1s 438us/step\n",
      "100000\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 1s 456us/step\n",
      "100000\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 1s 438us/step\n",
      "100000\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 1s 465us/step\n",
      "100000\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 2s 470us/step\n",
      "100000\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 1s 456us/step\n",
      "100000\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 1s 408us/step\n",
      "100000\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 1s 430us/step\n",
      "100000\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 1s 415us/step\n",
      "100000\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 1s 412us/step\n",
      "100000\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 1s 414us/step\n",
      "100000\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 1s 410us/step\n",
      "100000\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 1s 410us/step\n",
      "100000\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 1s 405us/step\n",
      "100000\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 1s 406us/step\n",
      "100000\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 1s 403us/step\n",
      "100000\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 1s 398us/step\n",
      "100000\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 1s 394us/step\n",
      "100000\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 1s 401us/step\n",
      "100000\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 1s 429us/step\n",
      "100000\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 1s 403us/step\n",
      "100000\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 1s 412us/step\n",
      "100000\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 1s 406us/step\n",
      "100000\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 1s 411us/step\n",
      "100000\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 1s 404us/step\n",
      "100000\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 1s 404us/step\n",
      "100000\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 1s 431us/step\n",
      "100000\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 1s 401us/step\n",
      "100000\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 1s 397us/step\n",
      "100000\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 1s 408us/step\n",
      "100000\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 1s 403us/step\n",
      "100000\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 1s 405us/step\n",
      "100000\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 1s 407us/step\n",
      "100000\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 1s 429us/step\n",
      "100000\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 1s 408us/step\n",
      "100000\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 1s 409us/step\n",
      "100000\n",
      "One lap done.\n",
      "3125/3125 [==============================] - 1s 408us/step\n",
      "100000\n",
      "One lap done.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "stream_mean_results=[]\n",
    "current_window = data_init.copy()\n",
    "delta=0.01\n",
    "adwin = ADWIN(delta)\n",
    "detected = False\n",
    "retraining_count=0\n",
    "data_window=data_init.copy()\n",
    "while stream.n_remaining_samples()>1:\n",
    "    incoming_data = stream.next_sample(two_percent)\n",
    "    \n",
    "    \n",
    "    \n",
    "    data_incoming = pd.DataFrame(incoming_data[0], columns=dataset.columns[:-1])\n",
    "    data_incoming[19]=incoming_data[1]\n",
    "    true_values.append(data_incoming[target_variable])\n",
    "    \n",
    "    \n",
    "    y_pred, y_pred_uncertainty = drift_detection_2(mean_model_weights, data_incoming.copy()) \n",
    "    \n",
    "    \n",
    "    y_pred_total.append(y_pred)\n",
    "    y_pred_uncertainty_total.append(y_pred_uncertainty)\n",
    "    print(len(y_pred_uncertainty))\n",
    "    \n",
    "    detected=False\n",
    "    for i in range(len(data_incoming[target_variable])):\n",
    "        adwin.add_element(y_pred_uncertainty[i])\n",
    "        if adwin.detected_change():\n",
    "            detected = True\n",
    "            break;\n",
    "    \n",
    "    if detected:\n",
    "        print(\"drift has been detecte models must be retrained\")\n",
    "        \n",
    "        data_window = update_train_data(data_window, data_incoming)\n",
    "        \n",
    "        retraining_count+=1\n",
    "        \n",
    "        Models=[]\n",
    "        val_acc=[]\n",
    "        train_acc=[]\n",
    "        test_acc=[]\n",
    "        val_loss=[]\n",
    "        train_loss=[]\n",
    "        ind=0\n",
    "        add_weights=[]\n",
    "\n",
    "\n",
    "        ann_model=get_initial_model_2(dataset.shape[1]-1, 2) \n",
    "\n",
    "\n",
    "        X_train=data_init.drop(columns=[target_variable])\n",
    "        print(X_train.shape[1])\n",
    "\n",
    "        y_train=to_categorical(data_init[target_variable])\n",
    "\n",
    "        ann_model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy']) # metrics=['accuracy']\n",
    "        history = ann_model.fit(X_train, y_train, epochs=10, validation_split=0.2, verbose=1)\n",
    "        print(history.history['accuracy'][-1])\n",
    "\n",
    "\n",
    "\n",
    "        add_weights.append([ann_model.get_weights()])\n",
    "        Models.append([ann_model.get_weights(), 1])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        print(len(Models))\n",
    "\n",
    "        A=np.argsort(np.array(Models).T[1])[::-1][:5]\n",
    "        print(np.array(Models).T[1])\n",
    "        print(A)\n",
    "        recommended_models=[]\n",
    "        for i in range(len(A)):\n",
    "            recommended_models.append(add_weights[A[i]])\n",
    "        #add_weights=add_weights[A]\n",
    "        # Now trying to generate Streaming settings for the dataset\n",
    "        # lets find the outputs from all the \n",
    "        mean_model_weights, mean_model_acc, mean_model_loss, mean_model_test_acc, mean_model_test_loss = epsilon_mean_recommendation_2(recommended_models, data_init)\n",
    "\n",
    "        \n",
    "    else:\n",
    "        print(\"One lap done.\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1fb5040e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0.7879238\n",
      "0.5718035587699595\n",
      "0.7566760955221771\n",
      "0.782662225051205\n"
     ]
    }
   ],
   "source": [
    "print(retraining_count)\n",
    "\n",
    "predictions=[]\n",
    "for i in y_pred_total:\n",
    "    predictions.append(list(np.argmax(i,axis=1)))\n",
    "predictions = list(np.concatenate(predictions))\n",
    "y_pred_total[0], len(predictions)\n",
    "\n",
    "true_values = list(np.concatenate(true_values))\n",
    "\n",
    "\n",
    "from sklearn.metrics import log_loss, mean_squared_error, accuracy_score, mean_absolute_error, f1_score, matthews_corrcoef, mean_squared_error, mean_squared_log_error, roc_auc_score\n",
    "\n",
    "# this is for classification\n",
    "acc_score = accuracy_score(true_values, predictions)\n",
    "print(acc_score)\n",
    "mcc = matthews_corrcoef(true_values, predictions)\n",
    "f1_score = f1_score(true_values, predictions)\n",
    "auc_score = roc_auc_score(true_values, predictions)\n",
    "\n",
    "print(mcc)\n",
    "print(f1_score)\n",
    "print(auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "557b98dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#here onwards plotting the acc_score for epsilon values and ip models acc_score has been calculate for say epsilon 3 ≈(noise_multiplier 0.65 in compute_sgd)\n",
    "#plt.yticks([0.70, 0.75, 0.80, 0.90,0.95],[\"0.70\",\"0.75\",\"0.80\",\"0.85\",\"0.90\"])\n",
    "epsilon_list = ['0.1', '0.5', '1', '3', '5', '15', '50']\n",
    "plt.xticks([0,1,2,3,4,5,6], epsilon_list)\n",
    "\n",
    "plt.ylim([0.7,0.85])\n",
    "acc_DP = [ 0.7391, 0.7727, 0.7822, 0.7792, 0.7837, 0.7896, 0.7879]\n",
    "ipdd_acc=[0.7934]*len(acc_DP)\n",
    "plt.plot(ipdd_acc, 'r--', label = \"IP\" )\n",
    "plt.plot(acc_DP, label=\"DP\")\n",
    "plt.xlabel(\"epsilon in DP\")\n",
    "plt.legend()\n",
    "plt.savefig(\"SUSY DPvsOnlineIP.pdf\",dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5734cf5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
