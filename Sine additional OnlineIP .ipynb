{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2d2352b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import random\n",
    "import math\n",
    "import tensorflow as tf\n",
    "%matplotlib inline\n",
    "\n",
    "from skmultiflow.drift_detection import PageHinkley, ADWIN\n",
    "from skmultiflow.data import DataStream\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense , Dropout , Lambda, Flatten\n",
    "from tensorflow.keras.optimizers import Adam ,RMSprop\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import  backend as K\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.utils import to_categorical\n",
    "from scipy.stats import spearmanr\n",
    "from scipy.stats import entropy\n",
    "from tensorflow import keras\n",
    "from skmultiflow.drift_detection import ADWIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76469397",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_initial_model(inp, out): #get initial model\n",
    "    model = tf.keras.Sequential([\n",
    "        Dense(10, input_dim=inp, kernel_initializer='normal', activation='relu'),\n",
    "        Dense(out, activation='softmax')\n",
    "        ])\n",
    "    #optimizer = keras.optimizers.Adam(learning_rate=0.0001)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1bbe0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_initial_model_2(inp, out):\n",
    "    model = tf.keras.Sequential([\n",
    "        Dense(10, input_dim=inp, kernel_initializer='normal', activation='relu'),\n",
    "        Dense(20, activation='relu'),\n",
    "        Dense(10, activation='relu'),\n",
    "        Dense(out, activation='softmax')\n",
    "        ])\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a32ddad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DP_get_initial_model(inp, out): #get initial model\n",
    "    model = tf.keras.Sequential([\n",
    "        Dense(10, input_dim=inp, kernel_initializer='normal', activation='relu'),\n",
    "        Dense(out, activation='softmax')\n",
    "        ])\n",
    "    #optimizer = keras.optimizers.Adam(learning_rate=0.0001)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "540e5ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DP_get_initial_model_2(inp, out): #get initial model\n",
    "    model = tf.keras.Sequential([\n",
    "        Dense(10, input_dim=inp, kernel_initializer='normal', activation='relu'),\n",
    "        Dense(20, activation='relu'),\n",
    "        Dense(10, activation='relu'),\n",
    "        Dense(out, activation='softmax')\n",
    "        ])\n",
    "    #optimizer = keras.optimizers.Adam(learning_rate=0.0001)\n",
    "    #model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2940dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def node_comparison1(node1, node2, epsilon=0.05): #this function is to see if the nodes are atmost epsilon distance apart\n",
    "  for x, y in zip(node1,node2):\n",
    "    #print(x,y)\n",
    "    if isinstance(x, list):\n",
    "        if((np.linalg.norm(np.array(x)-np.array(y))/len(x))<=epsilon):\n",
    "            continue\n",
    "        else:\n",
    "            return False\n",
    "    else:\n",
    "        if(math.sqrt((x-y)*(x-y))<=epsilon):\n",
    "            continue\n",
    "        else:\n",
    "            return False\n",
    "  return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c3d0195",
   "metadata": {},
   "outputs": [],
   "source": [
    "def node_comparison(node1, node2):\n",
    "  for x, y in zip(node1,node2):\n",
    "    #print(x,y)\n",
    "    if isinstance(x, list):\n",
    "        if(sorted(x)==sorted(y)):\n",
    "            continue\n",
    "        else:\n",
    "            return False\n",
    "    else:\n",
    "        if(x==y):\n",
    "            continue\n",
    "        else:\n",
    "            return False\n",
    "  return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8ffe788",
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_comparison(layer1, layer2): #compare same layers in two different DNNs\n",
    "  for node1 in layer1:\n",
    "    present=False\n",
    "    for node2 in layer2:\n",
    "      if (node_comparison1(node1, node2)):\n",
    "        present=True\n",
    "    if present==False:\n",
    "      return False\n",
    "  return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3265b8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_models(Model_weights1, Model_weights2): # compare two deep neural networks based on their weights\n",
    "  for i in range(0,len(Model_weights1), 2):\n",
    "    layer1=[]\n",
    "    layer2=[]\n",
    "    for j in range(len(Model_weights1[i+1].T)):\n",
    "      Node1=[]\n",
    "      Node2=[]\n",
    "      Node1.append(list(Model_weights1[i].T[j]))\n",
    "      Node1.append(Model_weights1[i+1][j])\n",
    "      if (i+2<len(Model_weights1)):\n",
    "        Node1.append(list(Model_weights1[i+2][j]))\n",
    "      Node2.append(list(Model_weights2[i].T[j]))\n",
    "      Node2.append(Model_weights2[i+1][j])\n",
    "      if (i+2<len(Model_weights2)):\n",
    "        Node2.append(list(Model_weights2[i+2][j]))\n",
    "      layer1.append(Node1)\n",
    "      layer2.append(Node2)\n",
    "    if (layer_comparison(layer1, layer2)):\n",
    "      continue\n",
    "    else:\n",
    "      return False\n",
    "  return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "103fc149",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function is used to average out all the models in the epsilon range\n",
    "#the problem is different here than compared with model comparison. Shape not important.\n",
    "def get_avg_weights(models_weights, inp_shape, out_shape):\n",
    "    avg_sum=get_initial_model(inp_shape, out_shape).get_weights()\n",
    "    #print(avg_sum)\n",
    "    for i in range(0,len(avg_sum),2):\n",
    "        if (i+2<=len(avg_sum)):\n",
    "            for j in range(len(avg_sum[i])):\n",
    "                for k in range(len(avg_sum[i][j])):\n",
    "                    avg_sum[i][j][k]=0\n",
    "            for j in range(len(avg_sum[i+1])):\n",
    "                avg_sum[i+1][j]=0\n",
    "    #print(avg_sum)\n",
    "    print(models_weights[0])\n",
    "    for i in range(len(models_weights)):\n",
    "        for j in range(0, len(avg_sum),2):\n",
    "            #print(isinstance(avg_sum[j], np.ndarray))\n",
    "            #if(isinstance(avg_sum[j][0], np.ndarray)):\n",
    "            for k in range(len(avg_sum[j])):\n",
    "                avg_sum[j][k]=[avg_sum[j][k][l]+models_weights[i][j][k][l] for l in range(len(avg_sum[j][k]))]\n",
    "            #print(isinstance(avg_sum[j], np.ndarray))\n",
    "            #else: gayab kr diya\n",
    "            for k in range(len(avg_sum[j+1])):\n",
    "                avg_sum[j+1][k]=avg_sum[j+1][k]+models_weights[i][j+1][k]\n",
    "    print(\"yhn tk\")\n",
    "    mean_size=len(models_weights)\n",
    "    print(mean_size)\n",
    "    for i in range(0,len(avg_sum),2):\n",
    "        if (i+2<=len(avg_sum)):\n",
    "            for j in range(len(avg_sum[i])):\n",
    "                #print(\"yhn tk\")\n",
    "                avg_sum[i][j]=[avg_sum[i][j][k]/mean_size for k in range(len(avg_sum[i][j]))]\n",
    "            for j in range(len(avg_sum[i+1])):\n",
    "                avg_sum[i+1][j]=avg_sum[i+1][j]/mean_size\n",
    "    print(\"Done\")\n",
    "    return avg_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7f64855",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function is used to average out all the models in the epsilon range\n",
    "#the problem is different here than compared with model comparison. Shape not important.\n",
    "def get_avg_weights_2(models_weights, inp_shape, out_shape):\n",
    "    avg_sum=get_initial_model_2(inp_shape, out_shape).get_weights()\n",
    "    #print(avg_sum)\n",
    "    for i in range(0,len(avg_sum),2):\n",
    "        if (i+2<=len(avg_sum)):\n",
    "            for j in range(len(avg_sum[i])):\n",
    "                for k in range(len(avg_sum[i][j])):\n",
    "                    avg_sum[i][j][k]=0\n",
    "            for j in range(len(avg_sum[i+1])):\n",
    "                avg_sum[i+1][j]=0\n",
    "    #print(avg_sum)\n",
    "    print(models_weights[0])\n",
    "    for i in range(len(models_weights)):\n",
    "        for j in range(0, len(avg_sum),2):\n",
    "            #print(isinstance(avg_sum[j], np.ndarray))\n",
    "            #if(isinstance(avg_sum[j][0], np.ndarray)):\n",
    "            for k in range(len(avg_sum[j])):\n",
    "                avg_sum[j][k]=[avg_sum[j][k][l]+models_weights[i][j][k][l] for l in range(len(avg_sum[j][k]))]\n",
    "            #print(isinstance(avg_sum[j], np.ndarray))\n",
    "            #else: gayab kr diya\n",
    "            for k in range(len(avg_sum[j+1])):\n",
    "                avg_sum[j+1][k]=avg_sum[j+1][k]+models_weights[i][j+1][k]\n",
    "    print(\"yhn tk\")\n",
    "    mean_size=len(models_weights)\n",
    "    print(mean_size)\n",
    "    for i in range(0,len(avg_sum),2):\n",
    "        if (i+2<=len(avg_sum)):\n",
    "            for j in range(len(avg_sum[i])):\n",
    "                #print(\"yhn tk\")\n",
    "                avg_sum[i][j]=[avg_sum[i][j][k]/mean_size for k in range(len(avg_sum[i][j]))]\n",
    "            for j in range(len(avg_sum[i+1])):\n",
    "                avg_sum[i+1][j]=avg_sum[i+1][j]/mean_size\n",
    "    print(\"Done\")\n",
    "    return avg_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "307b2dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom functions for f1, precision and recall\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.utils import to_categorical\n",
    "from sklearn import preprocessing\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "001d177f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to check if the two samples are different:\n",
    "def return_samples(Positive, Negative, data, N):\n",
    "    datasets=[]\n",
    "    positiveN=int((Positive.shape[0]/data.shape[0])*N)\n",
    "    negativeN=int(N-positiveN)\n",
    "    print(negativeN)\n",
    "    while (Positive.empty==False and Negative.empty==False):\n",
    "        df1=Positive.sample(min(positiveN, len(Positive)))\n",
    "        Positive.drop(df1.index, inplace=True)\n",
    "        \"\"\"\n",
    "        drop_df1=np.random.choice(df1.index,(int)(min(positiveN, math.ceil(len(Positive))/2)), replace=False)\n",
    "        if len(Positive)<positiveN:\n",
    "            Positive.drop(df1.index, inplace=True)\n",
    "        else:\n",
    "            Positive.drop(drop_df1, inplace=True)\n",
    "        #print(len(Positive))\n",
    "        \"\"\"\n",
    "        df2=Negative.sample(min(negativeN, len(Negative)))\n",
    "        Negative.drop(df2.index, inplace=True)\n",
    "        \"\"\"\n",
    "        drop_df2 = np.random.choice(df2.index,(int)(min(negativeN, math.ceil(len(Negative))/2)))\n",
    "        if len(Negative)<negativeN:\n",
    "            Negative.drop(df2.index, inplace = True)\n",
    "        else:\n",
    "            Negative.drop(drop_df2, inplace=True)\n",
    "        \"\"\"\n",
    "        dataset=df1.append(df2, ignore_index=True)\n",
    "        dataset.sample(frac = 1)\n",
    "        dataset.sample(frac = 1)\n",
    "        dataset.sample(frac = 1)\n",
    "        datasets.append(dataset)\n",
    "    print(\"returned datasets\")\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de18625c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate sampele which are plausible deniable\n",
    "#there exists a data lets say data\n",
    "def generate_samples(data, num_samples, N):\n",
    "    samples=[]\n",
    "    intersection=[]\n",
    "    while num_samples>0:    \n",
    "        Positive = data[data[target_variable]==0]\n",
    "        Negative = data[data[target_variable]==1]\n",
    "        positiveN=int((Positive.shape[0]/data.shape[0])*N)\n",
    "        negativeN=int(N-positiveN)\n",
    "        df1=Positive.sample(positiveN)\n",
    "        #Positive.drop(df1.index, inplace=True)\n",
    "        df2=Negative.sample(negativeN)\n",
    "        #Negative.drop(df2.index, inplace=True)\n",
    "        sample=df1.append(df2, ignore_index=False)\n",
    "        samples.append(sample)\n",
    "        num_samples-=1\n",
    "\n",
    "    intersection=list(set(samples[0].index).intersection(samples[1].index))\n",
    "    for i in range(2,len(samples)):\n",
    "        intersection=list(set(samples[i].index).intersection(intersection))\n",
    "    if intersection:\n",
    "        rnum = random.randint(0, len(samples)-1)\n",
    "        samples[rnum].drop(intersection)\n",
    "        print(\"some intersection\")\n",
    "    print(\"finished\")\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eafc2594",
   "metadata": {},
   "outputs": [],
   "source": [
    "#do we need to find all the models or only the mean recommended models?\n",
    "def epsilon_mean_recommendation(add_weights,data):\n",
    "    mean_model_weights=[]\n",
    "    for i in range(len(add_weights)):\n",
    "        mean_model_weights.append(get_avg_weights(add_weights[i],data.shape[1]-1, 2))\n",
    "    mean_models=[]\n",
    "    mean_model_train_metrics=[]\n",
    "    mean_model_loss=[]\n",
    "    mean_model_acc=[]\n",
    "    mean_model_test_metrics=[]\n",
    "    mean_model_test_loss=[]\n",
    "    mean_model_test_acc=[]\n",
    "    y = to_categorical(data[target_variable])\n",
    "    X = data.drop(columns=target_variable)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "    for i in range(len(add_weights)):\n",
    "        init_model=get_initial_model(X_test.shape[1], 2)\n",
    "        init_model.set_weights(mean_model_weights[i])\n",
    "        mean_model_train_metrics.append(init_model.evaluate(X_train, y_train))\n",
    "        mean_model_loss.append(mean_model_train_metrics[i][0])\n",
    "        mean_model_acc.append(mean_model_train_metrics[i][1])\n",
    "        mean_model_test_metrics.append(init_model.evaluate(X_test, y_test))\n",
    "        mean_model_test_loss.append(mean_model_test_metrics[i][0])\n",
    "        mean_model_test_acc.append(mean_model_test_metrics[i][1])\n",
    "    return mean_model_weights, mean_model_acc, mean_model_loss, mean_model_test_acc, mean_model_test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f7c6f663",
   "metadata": {},
   "outputs": [],
   "source": [
    "#do we need to find all the models or only the mean recommended models?\n",
    "def epsilon_mean_recommendation_2(add_weights,data):\n",
    "    mean_model_weights=[]\n",
    "    for i in range(len(add_weights)):\n",
    "        mean_model_weights.append(get_avg_weights_2(add_weights[i],data.shape[1]-1, 2))\n",
    "    mean_models=[]\n",
    "    mean_model_train_metrics=[]\n",
    "    mean_model_loss=[]\n",
    "    mean_model_acc=[]\n",
    "    mean_model_test_metrics=[]\n",
    "    mean_model_test_loss=[]\n",
    "    mean_model_test_acc=[]\n",
    "    y = to_categorical(data[target_variable])\n",
    "    X = data.drop(columns=target_variable)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "    for i in range(len(add_weights)):\n",
    "        init_model=get_initial_model_2(X_test.shape[1], 2)\n",
    "        init_model.set_weights(mean_model_weights[i])\n",
    "        mean_model_train_metrics.append(init_model.evaluate(X_train, y_train))\n",
    "        mean_model_loss.append(mean_model_train_metrics[i][0])\n",
    "        mean_model_acc.append(mean_model_train_metrics[i][1])\n",
    "        mean_model_test_metrics.append(init_model.evaluate(X_test, y_test))\n",
    "        mean_model_test_loss.append(mean_model_test_metrics[i][0])\n",
    "        mean_model_test_acc.append(mean_model_test_metrics[i][1])\n",
    "    return mean_model_weights, mean_model_acc, mean_model_loss, mean_model_test_acc, mean_model_test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1e0bc1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drift_detection(models, incoming_data):\n",
    "    predictions = []\n",
    "    prediction_probe=[]\n",
    "    \n",
    "    \n",
    "    y_test=to_categorical(incoming_data[target_variable])\n",
    "    #print(y_test)\n",
    "    X_test=incoming_data.drop(columns=target_variable)\n",
    "    \n",
    "    init_model=get_initial_model(incoming_data.shape[1]-1, 2)\n",
    "    \n",
    "    for i in range(len(models)):\n",
    "        init_model.set_weights(models[i])\n",
    "        predictions.append(init_model.predict(X_test))\n",
    "    prediction_probe = np.mean(predictions, axis = 0)\n",
    "    entro = entropy(prediction_probe, base=2, axis=1)\n",
    "    \n",
    "    entropy_list = entro.tolist()\n",
    "    \n",
    "    return prediction_probe, entropy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fd90f3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drift_detection_2(models, incoming_data):\n",
    "    predictions = []\n",
    "    prediction_probe=[]\n",
    "    \n",
    "    \n",
    "    y_test=to_categorical(incoming_data[target_variable])\n",
    "    #print(y_test)\n",
    "    X_test=incoming_data.drop(columns=target_variable)\n",
    "    \n",
    "    init_model=get_initial_model_2(incoming_data.shape[1]-1, 2)\n",
    "    \n",
    "    for i in range(len(models)):\n",
    "        init_model.set_weights(models[i])\n",
    "        predictions.append(init_model.predict(X_test))\n",
    "    prediction_probe = np.mean(predictions, axis = 0)\n",
    "    entro = entropy(prediction_probe, base=2, axis=1)\n",
    "    \n",
    "    entropy_list = entro.tolist()\n",
    "    \n",
    "    return prediction_probe, entropy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "60d6a2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_train_data(train_data, incoming_data):\n",
    "    train_data.drop(index=train_data.index[:len(incoming_data)], inplace=True)\n",
    "    train_data=train_data.append(incoming_data, ignore_index=True)\n",
    "    return train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "444a9ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n",
      "             x1        x2        x3        x4  y\n",
      "0      0.900395  0.284435  0.308094  0.242561  0\n",
      "1      0.078758  0.030236  0.047542  0.638051  0\n",
      "2      0.954808  0.348666  0.934022  0.770769  0\n",
      "3      0.462751  0.012494  0.782287  0.705243  0\n",
      "4      0.692544  0.322162  0.614659  0.670095  0\n",
      "...         ...       ...       ...       ... ..\n",
      "19995  0.335412  0.241783  0.755269  0.465764  0\n",
      "19996  0.623528  0.464222  0.062435  0.462664  0\n",
      "19997  0.380068  0.813165  0.175575  0.930339  1\n",
      "19998  0.175327  0.258326  0.658530  0.256465  1\n",
      "19999  0.614989  0.069757  0.331914  0.579301  0\n",
      "\n",
      "[20000 rows x 5 columns]\n",
      "finished\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"sin_classification_large_drift.csv\")\n",
    "from skmultiflow.data import DataStream\n",
    "\n",
    "stream = DataStream(data)\n",
    "\n",
    "two_percent = int(stream.n_remaining_samples()*0.02)\n",
    "five_percent = int(stream.n_remaining_samples()*0.05)\n",
    "initial_data = stream.next_sample(int(stream.n_remaining_samples()*0.1))\n",
    "print(len(initial_data[1]))\n",
    "data_init=pd.DataFrame(initial_data[0], columns=data.columns[:-1])\n",
    "target_variable = 'y'\n",
    "data_init['y']=initial_data[1]\n",
    "print(data_init)\n",
    "\n",
    "target_variable='y'\n",
    "\n",
    "N = int(data_init.shape[0]*0.25)\n",
    "\n",
    "samples = generate_samples(data_init, 100, N)\n",
    "initial_model= get_initial_model(data.shape[1]-1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "297d6284",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.6506 - f1_m: 0.6930 - val_loss: 0.7203 - val_f1_m: 0.3379\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 838us/step - loss: 0.5476 - f1_m: 0.8152 - val_loss: 0.5696 - val_f1_m: 0.7695\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 834us/step - loss: 0.4276 - f1_m: 0.9205 - val_loss: 0.4613 - val_f1_m: 0.8252\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 884us/step - loss: 0.3234 - f1_m: 0.9470 - val_loss: 0.3142 - val_f1_m: 0.9424\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 827us/step - loss: 0.2530 - f1_m: 0.9643 - val_loss: 0.2618 - val_f1_m: 0.9463\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 813us/step - loss: 0.2087 - f1_m: 0.9697 - val_loss: 0.2372 - val_f1_m: 0.9375\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 812us/step - loss: 0.1795 - f1_m: 0.9740 - val_loss: 0.1857 - val_f1_m: 0.9688\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 847us/step - loss: 0.1580 - f1_m: 0.9772 - val_loss: 0.1488 - val_f1_m: 0.9814\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 828us/step - loss: 0.1431 - f1_m: 0.9808 - val_loss: 0.1781 - val_f1_m: 0.9492\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 826us/step - loss: 0.1307 - f1_m: 0.9797 - val_loss: 0.1495 - val_f1_m: 0.9668\n",
      "0.9797499775886536\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.6533 - f1_m: 0.6552 - val_loss: 0.7036 - val_f1_m: 0.4141\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 810us/step - loss: 0.5494 - f1_m: 0.8375 - val_loss: 0.5646 - val_f1_m: 0.7471\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 821us/step - loss: 0.4206 - f1_m: 0.9300 - val_loss: 0.4685 - val_f1_m: 0.7812\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 801us/step - loss: 0.3215 - f1_m: 0.9473 - val_loss: 0.3397 - val_f1_m: 0.9121\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 808us/step - loss: 0.2578 - f1_m: 0.9595 - val_loss: 0.2885 - val_f1_m: 0.9229\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 803us/step - loss: 0.2156 - f1_m: 0.9665 - val_loss: 0.2267 - val_f1_m: 0.9629\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 798us/step - loss: 0.1866 - f1_m: 0.9720 - val_loss: 0.1853 - val_f1_m: 0.9795\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 795us/step - loss: 0.1659 - f1_m: 0.9743 - val_loss: 0.1824 - val_f1_m: 0.9697\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 799us/step - loss: 0.1504 - f1_m: 0.9775 - val_loss: 0.1829 - val_f1_m: 0.9531\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 795us/step - loss: 0.1375 - f1_m: 0.9790 - val_loss: 0.1430 - val_f1_m: 0.9805\n",
      "0.9789999723434448\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.6513 - f1_m: 0.7075 - val_loss: 0.6904 - val_f1_m: 0.4902\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 806us/step - loss: 0.5358 - f1_m: 0.8567 - val_loss: 0.5881 - val_f1_m: 0.7012\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 812us/step - loss: 0.4060 - f1_m: 0.9280 - val_loss: 0.4594 - val_f1_m: 0.7959\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 790us/step - loss: 0.3093 - f1_m: 0.9535 - val_loss: 0.3402 - val_f1_m: 0.8857\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 795us/step - loss: 0.2479 - f1_m: 0.9650 - val_loss: 0.2593 - val_f1_m: 0.9521\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 798us/step - loss: 0.2075 - f1_m: 0.9712 - val_loss: 0.2444 - val_f1_m: 0.9219\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 787us/step - loss: 0.1793 - f1_m: 0.9740 - val_loss: 0.2120 - val_f1_m: 0.9434\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 802us/step - loss: 0.1589 - f1_m: 0.9787 - val_loss: 0.1798 - val_f1_m: 0.9609\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 803us/step - loss: 0.1435 - f1_m: 0.9810 - val_loss: 0.1878 - val_f1_m: 0.9375\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 790us/step - loss: 0.1315 - f1_m: 0.9820 - val_loss: 0.1641 - val_f1_m: 0.9512\n",
      "0.9819999933242798\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.6527 - f1_m: 0.6830 - val_loss: 0.6790 - val_f1_m: 0.5508\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 828us/step - loss: 0.5438 - f1_m: 0.8465 - val_loss: 0.5630 - val_f1_m: 0.7627\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 823us/step - loss: 0.4151 - f1_m: 0.9252 - val_loss: 0.4380 - val_f1_m: 0.8369\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 798us/step - loss: 0.3177 - f1_m: 0.9505 - val_loss: 0.3229 - val_f1_m: 0.9141\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 787us/step - loss: 0.2549 - f1_m: 0.9660 - val_loss: 0.2633 - val_f1_m: 0.9375\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 805us/step - loss: 0.2132 - f1_m: 0.9715 - val_loss: 0.2293 - val_f1_m: 0.9395\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 806us/step - loss: 0.1851 - f1_m: 0.9768 - val_loss: 0.2397 - val_f1_m: 0.8975\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 806us/step - loss: 0.1638 - f1_m: 0.9778 - val_loss: 0.1824 - val_f1_m: 0.9482\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 808us/step - loss: 0.1476 - f1_m: 0.9818 - val_loss: 0.1810 - val_f1_m: 0.9395\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 801us/step - loss: 0.1357 - f1_m: 0.9820 - val_loss: 0.1678 - val_f1_m: 0.9424\n",
      "0.9819999933242798\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.6514 - f1_m: 0.6742 - val_loss: 0.6957 - val_f1_m: 0.4551\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 833us/step - loss: 0.5401 - f1_m: 0.8415 - val_loss: 0.5424 - val_f1_m: 0.8008\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 804us/step - loss: 0.4059 - f1_m: 0.9383 - val_loss: 0.4224 - val_f1_m: 0.8574\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 812us/step - loss: 0.3072 - f1_m: 0.9582 - val_loss: 0.3340 - val_f1_m: 0.8965\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 810us/step - loss: 0.2447 - f1_m: 0.9680 - val_loss: 0.2316 - val_f1_m: 0.9668\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 800us/step - loss: 0.2046 - f1_m: 0.9770 - val_loss: 0.2458 - val_f1_m: 0.9238\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 800us/step - loss: 0.1774 - f1_m: 0.9752 - val_loss: 0.1827 - val_f1_m: 0.9648\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 802us/step - loss: 0.1572 - f1_m: 0.9810 - val_loss: 0.1889 - val_f1_m: 0.9443\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 800us/step - loss: 0.1418 - f1_m: 0.9812 - val_loss: 0.1654 - val_f1_m: 0.9531\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 791us/step - loss: 0.1294 - f1_m: 0.9815 - val_loss: 0.1381 - val_f1_m: 0.9697\n",
      "0.9815000295639038\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.6527 - f1_m: 0.6675 - val_loss: 0.7077 - val_f1_m: 0.3965\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 811us/step - loss: 0.5477 - f1_m: 0.8217 - val_loss: 0.5598 - val_f1_m: 0.7646\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 830us/step - loss: 0.4266 - f1_m: 0.9192 - val_loss: 0.4180 - val_f1_m: 0.8994\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 807us/step - loss: 0.3314 - f1_m: 0.9495 - val_loss: 0.3412 - val_f1_m: 0.9121\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 805us/step - loss: 0.2668 - f1_m: 0.9630 - val_loss: 0.2647 - val_f1_m: 0.9531\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 870us/step - loss: 0.2231 - f1_m: 0.9707 - val_loss: 0.2389 - val_f1_m: 0.9453\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 851us/step - loss: 0.1926 - f1_m: 0.9772 - val_loss: 0.2400 - val_f1_m: 0.9102\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 856us/step - loss: 0.1707 - f1_m: 0.9768 - val_loss: 0.1982 - val_f1_m: 0.9463\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 853us/step - loss: 0.1534 - f1_m: 0.9805 - val_loss: 0.1723 - val_f1_m: 0.9541\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 857us/step - loss: 0.1401 - f1_m: 0.9800 - val_loss: 0.1471 - val_f1_m: 0.9707\n",
      "0.9800000190734863\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.6508 - f1_m: 0.6530 - val_loss: 0.7000 - val_f1_m: 0.4248\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 810us/step - loss: 0.5464 - f1_m: 0.8245 - val_loss: 0.5584 - val_f1_m: 0.7852\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 814us/step - loss: 0.4255 - f1_m: 0.9152 - val_loss: 0.4384 - val_f1_m: 0.8613\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 795us/step - loss: 0.3304 - f1_m: 0.9495 - val_loss: 0.3603 - val_f1_m: 0.8857\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 800us/step - loss: 0.2662 - f1_m: 0.9563 - val_loss: 0.2780 - val_f1_m: 0.9512\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 789us/step - loss: 0.2233 - f1_m: 0.9685 - val_loss: 0.2378 - val_f1_m: 0.9600\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 797us/step - loss: 0.1933 - f1_m: 0.9758 - val_loss: 0.2379 - val_f1_m: 0.9268\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 795us/step - loss: 0.1715 - f1_m: 0.9753 - val_loss: 0.2089 - val_f1_m: 0.9424\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 796us/step - loss: 0.1551 - f1_m: 0.9765 - val_loss: 0.1731 - val_f1_m: 0.9639\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 785us/step - loss: 0.1417 - f1_m: 0.9795 - val_loss: 0.1788 - val_f1_m: 0.9473\n",
      "0.9794999957084656\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.6518 - f1_m: 0.6517 - val_loss: 0.6997 - val_f1_m: 0.4199\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 807us/step - loss: 0.5512 - f1_m: 0.8372 - val_loss: 0.5954 - val_f1_m: 0.7012\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 817us/step - loss: 0.4330 - f1_m: 0.9062 - val_loss: 0.4267 - val_f1_m: 0.9121\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 799us/step - loss: 0.3370 - f1_m: 0.9457 - val_loss: 0.3513 - val_f1_m: 0.9170\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 801us/step - loss: 0.2713 - f1_m: 0.9613 - val_loss: 0.2984 - val_f1_m: 0.9316\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 809us/step - loss: 0.2273 - f1_m: 0.9660 - val_loss: 0.2439 - val_f1_m: 0.9570\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 802us/step - loss: 0.1968 - f1_m: 0.9718 - val_loss: 0.2101 - val_f1_m: 0.9629\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 801us/step - loss: 0.1745 - f1_m: 0.9758 - val_loss: 0.2188 - val_f1_m: 0.9434\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 799us/step - loss: 0.1575 - f1_m: 0.9760 - val_loss: 0.1746 - val_f1_m: 0.9658\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 798us/step - loss: 0.1439 - f1_m: 0.9778 - val_loss: 0.1814 - val_f1_m: 0.9551\n",
      "0.9777500033378601\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.6525 - f1_m: 0.6670 - val_loss: 0.6882 - val_f1_m: 0.4736\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 811us/step - loss: 0.5511 - f1_m: 0.8370 - val_loss: 0.6062 - val_f1_m: 0.6602\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 804us/step - loss: 0.4259 - f1_m: 0.9132 - val_loss: 0.4618 - val_f1_m: 0.8213\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 800us/step - loss: 0.3195 - f1_m: 0.9455 - val_loss: 0.3183 - val_f1_m: 0.9424\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 796us/step - loss: 0.2523 - f1_m: 0.9572 - val_loss: 0.2713 - val_f1_m: 0.9404\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 788us/step - loss: 0.2102 - f1_m: 0.9690 - val_loss: 0.2295 - val_f1_m: 0.9502\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 799us/step - loss: 0.1821 - f1_m: 0.9712 - val_loss: 0.2081 - val_f1_m: 0.9492\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 798us/step - loss: 0.1618 - f1_m: 0.9765 - val_loss: 0.1893 - val_f1_m: 0.9531\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 811us/step - loss: 0.1463 - f1_m: 0.9780 - val_loss: 0.1602 - val_f1_m: 0.9678\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 794us/step - loss: 0.1344 - f1_m: 0.9808 - val_loss: 0.1561 - val_f1_m: 0.9609\n",
      "0.9807500243186951\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 1s 1ms/step - loss: 0.6550 - f1_m: 0.7222 - val_loss: 0.6937 - val_f1_m: 0.4600\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 802us/step - loss: 0.5451 - f1_m: 0.8415 - val_loss: 0.5374 - val_f1_m: 0.8057\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 825us/step - loss: 0.4154 - f1_m: 0.9257 - val_loss: 0.4361 - val_f1_m: 0.8203\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 804us/step - loss: 0.3178 - f1_m: 0.9500 - val_loss: 0.3350 - val_f1_m: 0.8994\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 790us/step - loss: 0.2550 - f1_m: 0.9628 - val_loss: 0.2693 - val_f1_m: 0.9248\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 812us/step - loss: 0.2135 - f1_m: 0.9707 - val_loss: 0.2166 - val_f1_m: 0.9570\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 787us/step - loss: 0.1848 - f1_m: 0.9735 - val_loss: 0.1933 - val_f1_m: 0.9570\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 794us/step - loss: 0.1645 - f1_m: 0.9747 - val_loss: 0.1875 - val_f1_m: 0.9443\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 806us/step - loss: 0.1480 - f1_m: 0.9787 - val_loss: 0.1495 - val_f1_m: 0.9697\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 799us/step - loss: 0.1355 - f1_m: 0.9803 - val_loss: 0.1552 - val_f1_m: 0.9551\n",
      "0.9802500009536743\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.6528 - f1_m: 0.6850 - val_loss: 0.6932 - val_f1_m: 0.4678\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 815us/step - loss: 0.5500 - f1_m: 0.8272 - val_loss: 0.5739 - val_f1_m: 0.7461\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 821us/step - loss: 0.4278 - f1_m: 0.9182 - val_loss: 0.4564 - val_f1_m: 0.8291\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 807us/step - loss: 0.3299 - f1_m: 0.9517 - val_loss: 0.3800 - val_f1_m: 0.8545\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 805us/step - loss: 0.2653 - f1_m: 0.9613 - val_loss: 0.3016 - val_f1_m: 0.8994\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 796us/step - loss: 0.2220 - f1_m: 0.9647 - val_loss: 0.2260 - val_f1_m: 0.9570\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 805us/step - loss: 0.1921 - f1_m: 0.9775 - val_loss: 0.2209 - val_f1_m: 0.9307\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 792us/step - loss: 0.1699 - f1_m: 0.9755 - val_loss: 0.1754 - val_f1_m: 0.9678\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 795us/step - loss: 0.1535 - f1_m: 0.9790 - val_loss: 0.1585 - val_f1_m: 0.9697\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 792us/step - loss: 0.1397 - f1_m: 0.9827 - val_loss: 0.1652 - val_f1_m: 0.9482\n",
      "0.9827499985694885\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.6544 - f1_m: 0.7175 - val_loss: 0.7177 - val_f1_m: 0.3330\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 810us/step - loss: 0.5517 - f1_m: 0.8262 - val_loss: 0.5839 - val_f1_m: 0.7266\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 833us/step - loss: 0.4315 - f1_m: 0.9103 - val_loss: 0.4388 - val_f1_m: 0.8711\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 812us/step - loss: 0.3360 - f1_m: 0.9425 - val_loss: 0.3634 - val_f1_m: 0.8838\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 801us/step - loss: 0.2712 - f1_m: 0.9557 - val_loss: 0.2935 - val_f1_m: 0.9199\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 806us/step - loss: 0.2284 - f1_m: 0.9645 - val_loss: 0.2469 - val_f1_m: 0.9385\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 791us/step - loss: 0.1981 - f1_m: 0.9695 - val_loss: 0.2144 - val_f1_m: 0.9492\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 805us/step - loss: 0.1758 - f1_m: 0.9747 - val_loss: 0.1839 - val_f1_m: 0.9609\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - 0s 802us/step - loss: 0.1592 - f1_m: 0.9765 - val_loss: 0.1695 - val_f1_m: 0.9629\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 791us/step - loss: 0.1456 - f1_m: 0.9800 - val_loss: 0.1582 - val_f1_m: 0.9639\n",
      "0.9800000190734863\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.6507 - f1_m: 0.7087 - val_loss: 0.7400 - val_f1_m: 0.2656\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 811us/step - loss: 0.5411 - f1_m: 0.8280 - val_loss: 0.5371 - val_f1_m: 0.8037\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 802us/step - loss: 0.4163 - f1_m: 0.9262 - val_loss: 0.4242 - val_f1_m: 0.8691\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 791us/step - loss: 0.3211 - f1_m: 0.9510 - val_loss: 0.3395 - val_f1_m: 0.8955\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 781us/step - loss: 0.2582 - f1_m: 0.9600 - val_loss: 0.2535 - val_f1_m: 0.9561\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 803us/step - loss: 0.2174 - f1_m: 0.9672 - val_loss: 0.2172 - val_f1_m: 0.9619\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 804us/step - loss: 0.1888 - f1_m: 0.9730 - val_loss: 0.1856 - val_f1_m: 0.9688\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 786us/step - loss: 0.1680 - f1_m: 0.9768 - val_loss: 0.1694 - val_f1_m: 0.9697\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 788us/step - loss: 0.1521 - f1_m: 0.9793 - val_loss: 0.1689 - val_f1_m: 0.9541\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 796us/step - loss: 0.1396 - f1_m: 0.9797 - val_loss: 0.1506 - val_f1_m: 0.9678\n",
      "0.9797499775886536\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.6512 - f1_m: 0.6775 - val_loss: 0.7030 - val_f1_m: 0.3965\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 811us/step - loss: 0.5437 - f1_m: 0.8400 - val_loss: 0.5620 - val_f1_m: 0.7705\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 817us/step - loss: 0.4202 - f1_m: 0.9185 - val_loss: 0.4611 - val_f1_m: 0.8066\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 806us/step - loss: 0.3237 - f1_m: 0.9447 - val_loss: 0.3447 - val_f1_m: 0.9043\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 792us/step - loss: 0.2593 - f1_m: 0.9647 - val_loss: 0.3006 - val_f1_m: 0.9004\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 800us/step - loss: 0.2164 - f1_m: 0.9668 - val_loss: 0.2373 - val_f1_m: 0.9512\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 798us/step - loss: 0.1864 - f1_m: 0.9785 - val_loss: 0.2349 - val_f1_m: 0.9277\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 809us/step - loss: 0.1648 - f1_m: 0.9765 - val_loss: 0.2073 - val_f1_m: 0.9375\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 796us/step - loss: 0.1483 - f1_m: 0.9793 - val_loss: 0.2015 - val_f1_m: 0.9336\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 793us/step - loss: 0.1356 - f1_m: 0.9822 - val_loss: 0.1704 - val_f1_m: 0.9502\n",
      "0.9822499752044678\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.6512 - f1_m: 0.6975 - val_loss: 0.6825 - val_f1_m: 0.5186\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 809us/step - loss: 0.5386 - f1_m: 0.8612 - val_loss: 0.5518 - val_f1_m: 0.7822\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 822us/step - loss: 0.4105 - f1_m: 0.9302 - val_loss: 0.4364 - val_f1_m: 0.8457\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 796us/step - loss: 0.3144 - f1_m: 0.9450 - val_loss: 0.3267 - val_f1_m: 0.9189\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 798us/step - loss: 0.2520 - f1_m: 0.9595 - val_loss: 0.2817 - val_f1_m: 0.9209\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 801us/step - loss: 0.2115 - f1_m: 0.9665 - val_loss: 0.2413 - val_f1_m: 0.9316\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 796us/step - loss: 0.1839 - f1_m: 0.9712 - val_loss: 0.2139 - val_f1_m: 0.9375\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 791us/step - loss: 0.1636 - f1_m: 0.9707 - val_loss: 0.1778 - val_f1_m: 0.9668\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 806us/step - loss: 0.1482 - f1_m: 0.9735 - val_loss: 0.1783 - val_f1_m: 0.9482\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 817us/step - loss: 0.1364 - f1_m: 0.9760 - val_loss: 0.1587 - val_f1_m: 0.9580\n",
      "0.9760000109672546\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.6506 - f1_m: 0.6842 - val_loss: 0.6730 - val_f1_m: 0.5713\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 811us/step - loss: 0.5369 - f1_m: 0.8645 - val_loss: 0.5446 - val_f1_m: 0.7891\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 812us/step - loss: 0.4095 - f1_m: 0.9295 - val_loss: 0.4214 - val_f1_m: 0.8516\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 798us/step - loss: 0.3128 - f1_m: 0.9557 - val_loss: 0.3330 - val_f1_m: 0.8809\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 800us/step - loss: 0.2493 - f1_m: 0.9660 - val_loss: 0.2844 - val_f1_m: 0.8916\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 803us/step - loss: 0.2083 - f1_m: 0.9715 - val_loss: 0.2156 - val_f1_m: 0.9453\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 789us/step - loss: 0.1797 - f1_m: 0.9750 - val_loss: 0.2049 - val_f1_m: 0.9336\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 811us/step - loss: 0.1590 - f1_m: 0.9772 - val_loss: 0.2047 - val_f1_m: 0.9141\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 798us/step - loss: 0.1430 - f1_m: 0.9778 - val_loss: 0.1720 - val_f1_m: 0.9414\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 800us/step - loss: 0.1305 - f1_m: 0.9820 - val_loss: 0.1776 - val_f1_m: 0.9248\n",
      "0.9819999933242798\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.6522 - f1_m: 0.6660 - val_loss: 0.6969 - val_f1_m: 0.4424\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 807us/step - loss: 0.5494 - f1_m: 0.8337 - val_loss: 0.5888 - val_f1_m: 0.7051\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 825us/step - loss: 0.4300 - f1_m: 0.9135 - val_loss: 0.4556 - val_f1_m: 0.8457\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 791us/step - loss: 0.3332 - f1_m: 0.9405 - val_loss: 0.3470 - val_f1_m: 0.9209\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 797us/step - loss: 0.2675 - f1_m: 0.9570 - val_loss: 0.3107 - val_f1_m: 0.9004\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 803us/step - loss: 0.2236 - f1_m: 0.9650 - val_loss: 0.2719 - val_f1_m: 0.9141\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 812us/step - loss: 0.1927 - f1_m: 0.9690 - val_loss: 0.2382 - val_f1_m: 0.9219\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 807us/step - loss: 0.1704 - f1_m: 0.9737 - val_loss: 0.2248 - val_f1_m: 0.9199\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 799us/step - loss: 0.1533 - f1_m: 0.9762 - val_loss: 0.2164 - val_f1_m: 0.9150\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 796us/step - loss: 0.1401 - f1_m: 0.9768 - val_loss: 0.1774 - val_f1_m: 0.9541\n",
      "0.9767500162124634\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.6550 - f1_m: 0.7267 - val_loss: 0.7191 - val_f1_m: 0.3369\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 813us/step - loss: 0.5537 - f1_m: 0.8242 - val_loss: 0.5887 - val_f1_m: 0.7139\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 817us/step - loss: 0.4333 - f1_m: 0.9092 - val_loss: 0.4687 - val_f1_m: 0.8008\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 805us/step - loss: 0.3361 - f1_m: 0.9457 - val_loss: 0.3799 - val_f1_m: 0.8535\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 799us/step - loss: 0.2710 - f1_m: 0.9565 - val_loss: 0.3057 - val_f1_m: 0.9053\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 847us/step - loss: 0.2273 - f1_m: 0.9663 - val_loss: 0.2393 - val_f1_m: 0.9580\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 865us/step - loss: 0.1967 - f1_m: 0.9715 - val_loss: 0.2159 - val_f1_m: 0.9570\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 843us/step - loss: 0.1748 - f1_m: 0.9730 - val_loss: 0.1842 - val_f1_m: 0.9707\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 825us/step - loss: 0.1577 - f1_m: 0.9775 - val_loss: 0.1783 - val_f1_m: 0.9639\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 880us/step - loss: 0.1441 - f1_m: 0.9790 - val_loss: 0.1556 - val_f1_m: 0.9736\n",
      "0.9789999723434448\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.6537 - f1_m: 0.6720 - val_loss: 0.7184 - val_f1_m: 0.3418\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 810us/step - loss: 0.5490 - f1_m: 0.8150 - val_loss: 0.5488 - val_f1_m: 0.8018\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 804us/step - loss: 0.4199 - f1_m: 0.9290 - val_loss: 0.4015 - val_f1_m: 0.9033\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 800us/step - loss: 0.3201 - f1_m: 0.9560 - val_loss: 0.3051 - val_f1_m: 0.9414\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 812us/step - loss: 0.2552 - f1_m: 0.9695 - val_loss: 0.2343 - val_f1_m: 0.9658\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 811us/step - loss: 0.2134 - f1_m: 0.9755 - val_loss: 0.2146 - val_f1_m: 0.9541\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 825us/step - loss: 0.1837 - f1_m: 0.9787 - val_loss: 0.1642 - val_f1_m: 0.9814\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 834us/step - loss: 0.1625 - f1_m: 0.9822 - val_loss: 0.1789 - val_f1_m: 0.9502\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 809us/step - loss: 0.1462 - f1_m: 0.9820 - val_loss: 0.1602 - val_f1_m: 0.9541\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 785us/step - loss: 0.1336 - f1_m: 0.9827 - val_loss: 0.1254 - val_f1_m: 0.9795\n",
      "0.9827499985694885\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.6523 - f1_m: 0.6850 - val_loss: 0.6778 - val_f1_m: 0.5459\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 808us/step - loss: 0.5372 - f1_m: 0.8675 - val_loss: 0.5590 - val_f1_m: 0.7568\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 810us/step - loss: 0.4054 - f1_m: 0.9262 - val_loss: 0.4365 - val_f1_m: 0.8428\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 788us/step - loss: 0.3088 - f1_m: 0.9540 - val_loss: 0.3312 - val_f1_m: 0.9072\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 799us/step - loss: 0.2466 - f1_m: 0.9655 - val_loss: 0.2618 - val_f1_m: 0.9424\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 787us/step - loss: 0.2063 - f1_m: 0.9705 - val_loss: 0.2290 - val_f1_m: 0.9473\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 802us/step - loss: 0.1786 - f1_m: 0.9778 - val_loss: 0.2210 - val_f1_m: 0.9346\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 798us/step - loss: 0.1584 - f1_m: 0.9780 - val_loss: 0.1927 - val_f1_m: 0.9434\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 810us/step - loss: 0.1433 - f1_m: 0.9797 - val_loss: 0.1797 - val_f1_m: 0.9443\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 786us/step - loss: 0.1311 - f1_m: 0.9815 - val_loss: 0.1716 - val_f1_m: 0.9424\n",
      "0.9815000295639038\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.6532 - f1_m: 0.6555 - val_loss: 0.7016 - val_f1_m: 0.4316\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 823us/step - loss: 0.5472 - f1_m: 0.8285 - val_loss: 0.5507 - val_f1_m: 0.7861\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 799us/step - loss: 0.4128 - f1_m: 0.9265 - val_loss: 0.4012 - val_f1_m: 0.8896\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 807us/step - loss: 0.3121 - f1_m: 0.9530 - val_loss: 0.3049 - val_f1_m: 0.9307\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 812us/step - loss: 0.2487 - f1_m: 0.9638 - val_loss: 0.2174 - val_f1_m: 0.9756\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 786us/step - loss: 0.2080 - f1_m: 0.9697 - val_loss: 0.1946 - val_f1_m: 0.9688\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 796us/step - loss: 0.1805 - f1_m: 0.9740 - val_loss: 0.1736 - val_f1_m: 0.9668\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 798us/step - loss: 0.1602 - f1_m: 0.9755 - val_loss: 0.1563 - val_f1_m: 0.9668\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 798us/step - loss: 0.1449 - f1_m: 0.9808 - val_loss: 0.1694 - val_f1_m: 0.9434\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 789us/step - loss: 0.1332 - f1_m: 0.9780 - val_loss: 0.1351 - val_f1_m: 0.9668\n",
      "0.9779999852180481\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.6549 - f1_m: 0.6905 - val_loss: 0.7034 - val_f1_m: 0.4033\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 817us/step - loss: 0.5551 - f1_m: 0.8180 - val_loss: 0.5816 - val_f1_m: 0.7344\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 806us/step - loss: 0.4345 - f1_m: 0.9095 - val_loss: 0.4685 - val_f1_m: 0.8145\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 803us/step - loss: 0.3380 - f1_m: 0.9388 - val_loss: 0.3443 - val_f1_m: 0.9150\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 805us/step - loss: 0.2721 - f1_m: 0.9600 - val_loss: 0.2926 - val_f1_m: 0.9219\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 792us/step - loss: 0.2277 - f1_m: 0.9665 - val_loss: 0.2512 - val_f1_m: 0.9385\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 802us/step - loss: 0.1971 - f1_m: 0.9722 - val_loss: 0.2206 - val_f1_m: 0.9502\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 814us/step - loss: 0.1748 - f1_m: 0.9762 - val_loss: 0.1840 - val_f1_m: 0.9717\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 801us/step - loss: 0.1575 - f1_m: 0.9785 - val_loss: 0.1748 - val_f1_m: 0.9658\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 803us/step - loss: 0.1443 - f1_m: 0.9812 - val_loss: 0.1845 - val_f1_m: 0.9365\n",
      "0.981249988079071\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.6521 - f1_m: 0.6827 - val_loss: 0.6811 - val_f1_m: 0.5303\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 818us/step - loss: 0.5487 - f1_m: 0.8385 - val_loss: 0.5704 - val_f1_m: 0.7666\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 818us/step - loss: 0.4297 - f1_m: 0.9090 - val_loss: 0.4258 - val_f1_m: 0.8955\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 906us/step - loss: 0.3338 - f1_m: 0.9475 - val_loss: 0.3493 - val_f1_m: 0.9033\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 896us/step - loss: 0.2685 - f1_m: 0.9597 - val_loss: 0.2708 - val_f1_m: 0.9473\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 837us/step - loss: 0.2255 - f1_m: 0.9675 - val_loss: 0.2371 - val_f1_m: 0.9473\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 799us/step - loss: 0.1950 - f1_m: 0.9715 - val_loss: 0.2078 - val_f1_m: 0.9561\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 820us/step - loss: 0.1729 - f1_m: 0.9778 - val_loss: 0.1978 - val_f1_m: 0.9453\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 815us/step - loss: 0.1560 - f1_m: 0.9785 - val_loss: 0.1837 - val_f1_m: 0.9463\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 804us/step - loss: 0.1425 - f1_m: 0.9793 - val_loss: 0.1547 - val_f1_m: 0.9639\n",
      "0.9792500138282776\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 1s 1ms/step - loss: 0.6520 - f1_m: 0.6755 - val_loss: 0.7196 - val_f1_m: 0.3486\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 812us/step - loss: 0.5481 - f1_m: 0.8287 - val_loss: 0.5930 - val_f1_m: 0.6787\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 804us/step - loss: 0.4197 - f1_m: 0.9172 - val_loss: 0.4249 - val_f1_m: 0.8545\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 820us/step - loss: 0.3204 - f1_m: 0.9570 - val_loss: 0.3496 - val_f1_m: 0.8711\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 812us/step - loss: 0.2565 - f1_m: 0.9703 - val_loss: 0.2832 - val_f1_m: 0.9170\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 783us/step - loss: 0.2146 - f1_m: 0.9737 - val_loss: 0.2552 - val_f1_m: 0.9180\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - 0s 807us/step - loss: 0.1855 - f1_m: 0.9762 - val_loss: 0.2014 - val_f1_m: 0.9590\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 798us/step - loss: 0.1646 - f1_m: 0.9800 - val_loss: 0.2004 - val_f1_m: 0.9453\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 802us/step - loss: 0.1489 - f1_m: 0.9812 - val_loss: 0.1828 - val_f1_m: 0.9492\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 796us/step - loss: 0.1361 - f1_m: 0.9825 - val_loss: 0.1656 - val_f1_m: 0.9551\n",
      "0.9825000166893005\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.6525 - f1_m: 0.6667 - val_loss: 0.7210 - val_f1_m: 0.3184\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 818us/step - loss: 0.5517 - f1_m: 0.8162 - val_loss: 0.5784 - val_f1_m: 0.7520\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 826us/step - loss: 0.4338 - f1_m: 0.9093 - val_loss: 0.4594 - val_f1_m: 0.8398\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 800us/step - loss: 0.3381 - f1_m: 0.9410 - val_loss: 0.3764 - val_f1_m: 0.8779\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 792us/step - loss: 0.2725 - f1_m: 0.9572 - val_loss: 0.3129 - val_f1_m: 0.9072\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 802us/step - loss: 0.2285 - f1_m: 0.9630 - val_loss: 0.2496 - val_f1_m: 0.9424\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 793us/step - loss: 0.1977 - f1_m: 0.9715 - val_loss: 0.2344 - val_f1_m: 0.9307\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 800us/step - loss: 0.1747 - f1_m: 0.9762 - val_loss: 0.2096 - val_f1_m: 0.9404\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 797us/step - loss: 0.1574 - f1_m: 0.9780 - val_loss: 0.1918 - val_f1_m: 0.9424\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 795us/step - loss: 0.1437 - f1_m: 0.9793 - val_loss: 0.1958 - val_f1_m: 0.9268\n",
      "0.9792500138282776\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.6506 - f1_m: 0.6945 - val_loss: 0.7002 - val_f1_m: 0.4238\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 823us/step - loss: 0.5398 - f1_m: 0.8502 - val_loss: 0.5669 - val_f1_m: 0.7451\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 809us/step - loss: 0.4129 - f1_m: 0.9237 - val_loss: 0.3997 - val_f1_m: 0.9111\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 794us/step - loss: 0.3158 - f1_m: 0.9513 - val_loss: 0.2971 - val_f1_m: 0.9531\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 802us/step - loss: 0.2539 - f1_m: 0.9655 - val_loss: 0.2497 - val_f1_m: 0.9561\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 792us/step - loss: 0.2130 - f1_m: 0.9707 - val_loss: 0.2323 - val_f1_m: 0.9385\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 792us/step - loss: 0.1849 - f1_m: 0.9745 - val_loss: 0.1781 - val_f1_m: 0.9678\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 798us/step - loss: 0.1645 - f1_m: 0.9793 - val_loss: 0.1800 - val_f1_m: 0.9570\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 805us/step - loss: 0.1489 - f1_m: 0.9800 - val_loss: 0.1573 - val_f1_m: 0.9648\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 798us/step - loss: 0.1364 - f1_m: 0.9805 - val_loss: 0.1320 - val_f1_m: 0.9766\n",
      "0.9804999828338623\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.6552 - f1_m: 0.6970 - val_loss: 0.7057 - val_f1_m: 0.3945\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 816us/step - loss: 0.5471 - f1_m: 0.8530 - val_loss: 0.5828 - val_f1_m: 0.7041\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 800us/step - loss: 0.4167 - f1_m: 0.9240 - val_loss: 0.4309 - val_f1_m: 0.8525\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 800us/step - loss: 0.3173 - f1_m: 0.9498 - val_loss: 0.3519 - val_f1_m: 0.8828\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 804us/step - loss: 0.2529 - f1_m: 0.9592 - val_loss: 0.2802 - val_f1_m: 0.9229\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 802us/step - loss: 0.2110 - f1_m: 0.9693 - val_loss: 0.2691 - val_f1_m: 0.9004\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 798us/step - loss: 0.1822 - f1_m: 0.9722 - val_loss: 0.1986 - val_f1_m: 0.9600\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 812us/step - loss: 0.1614 - f1_m: 0.9747 - val_loss: 0.1729 - val_f1_m: 0.9648\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 797us/step - loss: 0.1456 - f1_m: 0.9783 - val_loss: 0.1633 - val_f1_m: 0.9648\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 792us/step - loss: 0.1332 - f1_m: 0.9797 - val_loss: 0.1620 - val_f1_m: 0.9580\n",
      "0.9797499775886536\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.6510 - f1_m: 0.6747 - val_loss: 0.6649 - val_f1_m: 0.6318\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 805us/step - loss: 0.5383 - f1_m: 0.8510 - val_loss: 0.5493 - val_f1_m: 0.7842\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 809us/step - loss: 0.4111 - f1_m: 0.9270 - val_loss: 0.4364 - val_f1_m: 0.8418\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 808us/step - loss: 0.3154 - f1_m: 0.9540 - val_loss: 0.3497 - val_f1_m: 0.8760\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 793us/step - loss: 0.2541 - f1_m: 0.9663 - val_loss: 0.2796 - val_f1_m: 0.9189\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 797us/step - loss: 0.2129 - f1_m: 0.9678 - val_loss: 0.2035 - val_f1_m: 0.9727\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 801us/step - loss: 0.1852 - f1_m: 0.9710 - val_loss: 0.2160 - val_f1_m: 0.9395\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 799us/step - loss: 0.1644 - f1_m: 0.9783 - val_loss: 0.2004 - val_f1_m: 0.9395\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 791us/step - loss: 0.1484 - f1_m: 0.9805 - val_loss: 0.1892 - val_f1_m: 0.9355\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 790us/step - loss: 0.1357 - f1_m: 0.9833 - val_loss: 0.1960 - val_f1_m: 0.9219\n",
      "0.9832500219345093\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.6514 - f1_m: 0.6700 - val_loss: 0.6966 - val_f1_m: 0.4414\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 799us/step - loss: 0.5459 - f1_m: 0.8325 - val_loss: 0.5840 - val_f1_m: 0.7109\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 816us/step - loss: 0.4243 - f1_m: 0.9160 - val_loss: 0.4256 - val_f1_m: 0.8643\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 819us/step - loss: 0.3279 - f1_m: 0.9480 - val_loss: 0.3351 - val_f1_m: 0.9004\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 805us/step - loss: 0.2635 - f1_m: 0.9675 - val_loss: 0.2760 - val_f1_m: 0.9170\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 801us/step - loss: 0.2206 - f1_m: 0.9707 - val_loss: 0.2415 - val_f1_m: 0.9199\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 807us/step - loss: 0.1904 - f1_m: 0.9772 - val_loss: 0.1930 - val_f1_m: 0.9600\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 793us/step - loss: 0.1686 - f1_m: 0.9795 - val_loss: 0.1675 - val_f1_m: 0.9727\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 798us/step - loss: 0.1518 - f1_m: 0.9815 - val_loss: 0.1528 - val_f1_m: 0.9736\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 811us/step - loss: 0.1386 - f1_m: 0.9835 - val_loss: 0.1651 - val_f1_m: 0.9424\n",
      "0.9835000038146973\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.6509 - f1_m: 0.6792 - val_loss: 0.7063 - val_f1_m: 0.3916\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 811us/step - loss: 0.5482 - f1_m: 0.8322 - val_loss: 0.6293 - val_f1_m: 0.5996\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 835us/step - loss: 0.4319 - f1_m: 0.9052 - val_loss: 0.4413 - val_f1_m: 0.8838\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 813us/step - loss: 0.3382 - f1_m: 0.9448 - val_loss: 0.3656 - val_f1_m: 0.8945\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 810us/step - loss: 0.2737 - f1_m: 0.9567 - val_loss: 0.2973 - val_f1_m: 0.9258\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 863us/step - loss: 0.2301 - f1_m: 0.9657 - val_loss: 0.2465 - val_f1_m: 0.9561\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 852us/step - loss: 0.1992 - f1_m: 0.9728 - val_loss: 0.2020 - val_f1_m: 0.9736\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 847us/step - loss: 0.1769 - f1_m: 0.9758 - val_loss: 0.2061 - val_f1_m: 0.9482\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 833us/step - loss: 0.1596 - f1_m: 0.9793 - val_loss: 0.1767 - val_f1_m: 0.9658\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 849us/step - loss: 0.1466 - f1_m: 0.9790 - val_loss: 0.1384 - val_f1_m: 0.9863\n",
      "0.9789999723434448\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.6507 - f1_m: 0.7035 - val_loss: 0.7225 - val_f1_m: 0.3223\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 788us/step - loss: 0.5362 - f1_m: 0.8322 - val_loss: 0.5197 - val_f1_m: 0.8701\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 811us/step - loss: 0.4060 - f1_m: 0.9262 - val_loss: 0.3923 - val_f1_m: 0.9180\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 808us/step - loss: 0.3101 - f1_m: 0.9557 - val_loss: 0.3339 - val_f1_m: 0.9014\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 784us/step - loss: 0.2479 - f1_m: 0.9605 - val_loss: 0.2594 - val_f1_m: 0.9473\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 794us/step - loss: 0.2074 - f1_m: 0.9685 - val_loss: 0.2062 - val_f1_m: 0.9756\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 804us/step - loss: 0.1800 - f1_m: 0.9720 - val_loss: 0.1979 - val_f1_m: 0.9570\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 805us/step - loss: 0.1593 - f1_m: 0.9745 - val_loss: 0.1782 - val_f1_m: 0.9570\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 807us/step - loss: 0.1443 - f1_m: 0.9772 - val_loss: 0.1636 - val_f1_m: 0.9580\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 789us/step - loss: 0.1325 - f1_m: 0.9790 - val_loss: 0.1482 - val_f1_m: 0.9609\n",
      "0.9789999723434448\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.6520 - f1_m: 0.6847 - val_loss: 0.6986 - val_f1_m: 0.4375\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 809us/step - loss: 0.5422 - f1_m: 0.8552 - val_loss: 0.5754 - val_f1_m: 0.7344\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 833us/step - loss: 0.4137 - f1_m: 0.9215 - val_loss: 0.4116 - val_f1_m: 0.8857\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 790us/step - loss: 0.3156 - f1_m: 0.9535 - val_loss: 0.3258 - val_f1_m: 0.9219\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 800us/step - loss: 0.2516 - f1_m: 0.9643 - val_loss: 0.2657 - val_f1_m: 0.9395\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 794us/step - loss: 0.2099 - f1_m: 0.9728 - val_loss: 0.2351 - val_f1_m: 0.9365\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 804us/step - loss: 0.1812 - f1_m: 0.9735 - val_loss: 0.1929 - val_f1_m: 0.9648\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 783us/step - loss: 0.1603 - f1_m: 0.9780 - val_loss: 0.2041 - val_f1_m: 0.9297\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 791us/step - loss: 0.1448 - f1_m: 0.9793 - val_loss: 0.2025 - val_f1_m: 0.9150\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 795us/step - loss: 0.1326 - f1_m: 0.9800 - val_loss: 0.1759 - val_f1_m: 0.9316\n",
      "0.9800000190734863\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.6535 - f1_m: 0.6607 - val_loss: 0.6855 - val_f1_m: 0.4932\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 819us/step - loss: 0.5418 - f1_m: 0.8520 - val_loss: 0.5529 - val_f1_m: 0.7686\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 803us/step - loss: 0.4113 - f1_m: 0.9260 - val_loss: 0.3918 - val_f1_m: 0.9170\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 791us/step - loss: 0.3129 - f1_m: 0.9545 - val_loss: 0.3258 - val_f1_m: 0.9062\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 795us/step - loss: 0.2496 - f1_m: 0.9685 - val_loss: 0.2643 - val_f1_m: 0.9346\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 817us/step - loss: 0.2084 - f1_m: 0.9695 - val_loss: 0.2145 - val_f1_m: 0.9521\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 804us/step - loss: 0.1797 - f1_m: 0.9778 - val_loss: 0.2083 - val_f1_m: 0.9404\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 794us/step - loss: 0.1590 - f1_m: 0.9772 - val_loss: 0.1658 - val_f1_m: 0.9668\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 802us/step - loss: 0.1439 - f1_m: 0.9815 - val_loss: 0.1584 - val_f1_m: 0.9590\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 798us/step - loss: 0.1313 - f1_m: 0.9825 - val_loss: 0.1340 - val_f1_m: 0.9736\n",
      "0.9825000166893005\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.6511 - f1_m: 0.6670 - val_loss: 0.7048 - val_f1_m: 0.4180\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 818us/step - loss: 0.5473 - f1_m: 0.8227 - val_loss: 0.5753 - val_f1_m: 0.7266\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 815us/step - loss: 0.4192 - f1_m: 0.9250 - val_loss: 0.4209 - val_f1_m: 0.8613\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 813us/step - loss: 0.3184 - f1_m: 0.9500 - val_loss: 0.3416 - val_f1_m: 0.8818\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 800us/step - loss: 0.2541 - f1_m: 0.9647 - val_loss: 0.2874 - val_f1_m: 0.8994\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 810us/step - loss: 0.2122 - f1_m: 0.9688 - val_loss: 0.2394 - val_f1_m: 0.9248\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 804us/step - loss: 0.1833 - f1_m: 0.9735 - val_loss: 0.2161 - val_f1_m: 0.9316\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 798us/step - loss: 0.1624 - f1_m: 0.9760 - val_loss: 0.2082 - val_f1_m: 0.9180\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 799us/step - loss: 0.1469 - f1_m: 0.9787 - val_loss: 0.2046 - val_f1_m: 0.9082\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 797us/step - loss: 0.1343 - f1_m: 0.9787 - val_loss: 0.1856 - val_f1_m: 0.9219\n",
      "0.9787499904632568\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.6498 - f1_m: 0.6710 - val_loss: 0.7010 - val_f1_m: 0.4297\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 817us/step - loss: 0.5432 - f1_m: 0.8257 - val_loss: 0.5563 - val_f1_m: 0.7734\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 820us/step - loss: 0.4219 - f1_m: 0.9235 - val_loss: 0.4373 - val_f1_m: 0.8437\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 801us/step - loss: 0.3251 - f1_m: 0.9477 - val_loss: 0.3213 - val_f1_m: 0.9277\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 805us/step - loss: 0.2613 - f1_m: 0.9607 - val_loss: 0.2847 - val_f1_m: 0.9141\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 800us/step - loss: 0.2186 - f1_m: 0.9697 - val_loss: 0.2423 - val_f1_m: 0.9316\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 800us/step - loss: 0.1888 - f1_m: 0.9745 - val_loss: 0.1916 - val_f1_m: 0.9668\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 812us/step - loss: 0.1670 - f1_m: 0.9758 - val_loss: 0.1885 - val_f1_m: 0.9541\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 810us/step - loss: 0.1505 - f1_m: 0.9775 - val_loss: 0.1775 - val_f1_m: 0.9473\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 805us/step - loss: 0.1369 - f1_m: 0.9803 - val_loss: 0.1736 - val_f1_m: 0.9385\n",
      "0.9802500009536743\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.6549 - f1_m: 0.7145 - val_loss: 0.7252 - val_f1_m: 0.2900\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 809us/step - loss: 0.5547 - f1_m: 0.8262 - val_loss: 0.5932 - val_f1_m: 0.7109\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 829us/step - loss: 0.4327 - f1_m: 0.9070 - val_loss: 0.4470 - val_f1_m: 0.8613\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 801us/step - loss: 0.3348 - f1_m: 0.9490 - val_loss: 0.3810 - val_f1_m: 0.8604\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - 0s 809us/step - loss: 0.2687 - f1_m: 0.9600 - val_loss: 0.2909 - val_f1_m: 0.9326\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 811us/step - loss: 0.2247 - f1_m: 0.9675 - val_loss: 0.2757 - val_f1_m: 0.9121\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 794us/step - loss: 0.1936 - f1_m: 0.9740 - val_loss: 0.2348 - val_f1_m: 0.9326\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 799us/step - loss: 0.1711 - f1_m: 0.9765 - val_loss: 0.2134 - val_f1_m: 0.9395\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 798us/step - loss: 0.1536 - f1_m: 0.9785 - val_loss: 0.1879 - val_f1_m: 0.9512\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 794us/step - loss: 0.1404 - f1_m: 0.9800 - val_loss: 0.1774 - val_f1_m: 0.9492\n",
      "0.9800000190734863\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.6573 - f1_m: 0.6412 - val_loss: 0.7114 - val_f1_m: 0.3818\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 823us/step - loss: 0.5610 - f1_m: 0.8145 - val_loss: 0.5699 - val_f1_m: 0.7871\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 810us/step - loss: 0.4437 - f1_m: 0.9042 - val_loss: 0.4510 - val_f1_m: 0.8525\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 804us/step - loss: 0.3463 - f1_m: 0.9415 - val_loss: 0.3452 - val_f1_m: 0.9170\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 799us/step - loss: 0.2784 - f1_m: 0.9572 - val_loss: 0.3212 - val_f1_m: 0.8760\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 798us/step - loss: 0.2329 - f1_m: 0.9657 - val_loss: 0.2593 - val_f1_m: 0.9199\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 801us/step - loss: 0.2019 - f1_m: 0.9695 - val_loss: 0.2369 - val_f1_m: 0.9209\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 794us/step - loss: 0.1784 - f1_m: 0.9740 - val_loss: 0.2187 - val_f1_m: 0.9209\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 792us/step - loss: 0.1607 - f1_m: 0.9755 - val_loss: 0.1908 - val_f1_m: 0.9326\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 788us/step - loss: 0.1468 - f1_m: 0.9775 - val_loss: 0.1992 - val_f1_m: 0.9180\n",
      "0.9775000214576721\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 1s 1ms/step - loss: 0.6519 - f1_m: 0.6817 - val_loss: 0.7221 - val_f1_m: 0.3174\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 802us/step - loss: 0.5501 - f1_m: 0.8212 - val_loss: 0.6148 - val_f1_m: 0.6396\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 803us/step - loss: 0.4315 - f1_m: 0.9105 - val_loss: 0.4494 - val_f1_m: 0.8457\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 790us/step - loss: 0.3371 - f1_m: 0.9452 - val_loss: 0.3290 - val_f1_m: 0.9424\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 791us/step - loss: 0.2724 - f1_m: 0.9680 - val_loss: 0.3331 - val_f1_m: 0.8584\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 794us/step - loss: 0.2281 - f1_m: 0.9710 - val_loss: 0.2582 - val_f1_m: 0.9268\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 780us/step - loss: 0.1972 - f1_m: 0.9755 - val_loss: 0.2173 - val_f1_m: 0.9502\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 793us/step - loss: 0.1742 - f1_m: 0.9787 - val_loss: 0.2085 - val_f1_m: 0.9385\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 802us/step - loss: 0.1564 - f1_m: 0.9797 - val_loss: 0.1988 - val_f1_m: 0.9336\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 799us/step - loss: 0.1423 - f1_m: 0.9815 - val_loss: 0.1777 - val_f1_m: 0.9453\n",
      "0.9815000295639038\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.6499 - f1_m: 0.6832 - val_loss: 0.6715 - val_f1_m: 0.5781\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 820us/step - loss: 0.5354 - f1_m: 0.8697 - val_loss: 0.5607 - val_f1_m: 0.7441\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 814us/step - loss: 0.4074 - f1_m: 0.9270 - val_loss: 0.4258 - val_f1_m: 0.8555\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 798us/step - loss: 0.3107 - f1_m: 0.9500 - val_loss: 0.3330 - val_f1_m: 0.9033\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 828us/step - loss: 0.2484 - f1_m: 0.9657 - val_loss: 0.2614 - val_f1_m: 0.9424\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 808us/step - loss: 0.2077 - f1_m: 0.9712 - val_loss: 0.2084 - val_f1_m: 0.9648\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 804us/step - loss: 0.1802 - f1_m: 0.9768 - val_loss: 0.1805 - val_f1_m: 0.9678\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 810us/step - loss: 0.1600 - f1_m: 0.9768 - val_loss: 0.1569 - val_f1_m: 0.9727\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 809us/step - loss: 0.1447 - f1_m: 0.9780 - val_loss: 0.1369 - val_f1_m: 0.9795\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 793us/step - loss: 0.1320 - f1_m: 0.9805 - val_loss: 0.1453 - val_f1_m: 0.9658\n",
      "0.9804999828338623\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.6528 - f1_m: 0.6667 - val_loss: 0.7146 - val_f1_m: 0.3574\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 799us/step - loss: 0.5538 - f1_m: 0.8080 - val_loss: 0.5681 - val_f1_m: 0.7666\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 811us/step - loss: 0.4362 - f1_m: 0.9165 - val_loss: 0.4691 - val_f1_m: 0.8145\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 790us/step - loss: 0.3399 - f1_m: 0.9467 - val_loss: 0.3824 - val_f1_m: 0.8477\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 801us/step - loss: 0.2734 - f1_m: 0.9603 - val_loss: 0.2886 - val_f1_m: 0.9141\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 815us/step - loss: 0.2283 - f1_m: 0.9728 - val_loss: 0.2592 - val_f1_m: 0.9102\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 810us/step - loss: 0.1973 - f1_m: 0.9718 - val_loss: 0.2311 - val_f1_m: 0.9170\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 801us/step - loss: 0.1743 - f1_m: 0.9765 - val_loss: 0.1908 - val_f1_m: 0.9482\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 807us/step - loss: 0.1568 - f1_m: 0.9810 - val_loss: 0.1811 - val_f1_m: 0.9404\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 799us/step - loss: 0.1430 - f1_m: 0.9825 - val_loss: 0.1556 - val_f1_m: 0.9570\n",
      "0.9825000166893005\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.6558 - f1_m: 0.6735 - val_loss: 0.6841 - val_f1_m: 0.5205\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 806us/step - loss: 0.5531 - f1_m: 0.8355 - val_loss: 0.6026 - val_f1_m: 0.6875\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 822us/step - loss: 0.4308 - f1_m: 0.9060 - val_loss: 0.4106 - val_f1_m: 0.9150\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 808us/step - loss: 0.3338 - f1_m: 0.9460 - val_loss: 0.3491 - val_f1_m: 0.8984\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 804us/step - loss: 0.2684 - f1_m: 0.9630 - val_loss: 0.2824 - val_f1_m: 0.9229\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 816us/step - loss: 0.2249 - f1_m: 0.9712 - val_loss: 0.2454 - val_f1_m: 0.9287\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 807us/step - loss: 0.1939 - f1_m: 0.9778 - val_loss: 0.2302 - val_f1_m: 0.9229\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 813us/step - loss: 0.1721 - f1_m: 0.9745 - val_loss: 0.1865 - val_f1_m: 0.9502\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 786us/step - loss: 0.1540 - f1_m: 0.9803 - val_loss: 0.1374 - val_f1_m: 0.9863\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 798us/step - loss: 0.1407 - f1_m: 0.9837 - val_loss: 0.1466 - val_f1_m: 0.9707\n",
      "0.9837499856948853\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.6550 - f1_m: 0.6710 - val_loss: 0.7070 - val_f1_m: 0.3828\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 805us/step - loss: 0.5568 - f1_m: 0.8140 - val_loss: 0.5807 - val_f1_m: 0.7432\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 816us/step - loss: 0.4407 - f1_m: 0.9112 - val_loss: 0.4666 - val_f1_m: 0.8271\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 803us/step - loss: 0.3442 - f1_m: 0.9430 - val_loss: 0.3702 - val_f1_m: 0.8828\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 801us/step - loss: 0.2769 - f1_m: 0.9597 - val_loss: 0.2909 - val_f1_m: 0.9307\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 832us/step - loss: 0.2310 - f1_m: 0.9710 - val_loss: 0.2464 - val_f1_m: 0.9473\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 840us/step - loss: 0.1994 - f1_m: 0.9758 - val_loss: 0.2377 - val_f1_m: 0.9248\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 841us/step - loss: 0.1761 - f1_m: 0.9795 - val_loss: 0.1956 - val_f1_m: 0.9580\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 835us/step - loss: 0.1588 - f1_m: 0.9803 - val_loss: 0.1727 - val_f1_m: 0.9678\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 877us/step - loss: 0.1448 - f1_m: 0.9833 - val_loss: 0.1682 - val_f1_m: 0.9609\n",
      "0.9832500219345093\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.6515 - f1_m: 0.6830 - val_loss: 0.7029 - val_f1_m: 0.4258\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 808us/step - loss: 0.5387 - f1_m: 0.8440 - val_loss: 0.5511 - val_f1_m: 0.7773\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 815us/step - loss: 0.4103 - f1_m: 0.9215 - val_loss: 0.3971 - val_f1_m: 0.9014\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 802us/step - loss: 0.3137 - f1_m: 0.9532 - val_loss: 0.3435 - val_f1_m: 0.8838\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 800us/step - loss: 0.2510 - f1_m: 0.9613 - val_loss: 0.2825 - val_f1_m: 0.9023\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 807us/step - loss: 0.2096 - f1_m: 0.9695 - val_loss: 0.2436 - val_f1_m: 0.9131\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 796us/step - loss: 0.1815 - f1_m: 0.9728 - val_loss: 0.2123 - val_f1_m: 0.9355\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 798us/step - loss: 0.1606 - f1_m: 0.9755 - val_loss: 0.1859 - val_f1_m: 0.9482\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 794us/step - loss: 0.1452 - f1_m: 0.9775 - val_loss: 0.1830 - val_f1_m: 0.9375\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 788us/step - loss: 0.1335 - f1_m: 0.9778 - val_loss: 0.1510 - val_f1_m: 0.9600\n",
      "0.9777500033378601\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.6505 - f1_m: 0.6775 - val_loss: 0.7234 - val_f1_m: 0.3223\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 822us/step - loss: 0.5471 - f1_m: 0.8350 - val_loss: 0.5890 - val_f1_m: 0.7031\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 819us/step - loss: 0.4273 - f1_m: 0.9142 - val_loss: 0.4682 - val_f1_m: 0.7998\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 804us/step - loss: 0.3325 - f1_m: 0.9405 - val_loss: 0.3335 - val_f1_m: 0.9189\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 802us/step - loss: 0.2688 - f1_m: 0.9613 - val_loss: 0.2903 - val_f1_m: 0.9121\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 796us/step - loss: 0.2254 - f1_m: 0.9650 - val_loss: 0.2543 - val_f1_m: 0.9180\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 805us/step - loss: 0.1949 - f1_m: 0.9697 - val_loss: 0.1958 - val_f1_m: 0.9619\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 807us/step - loss: 0.1731 - f1_m: 0.9747 - val_loss: 0.2059 - val_f1_m: 0.9336\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 798us/step - loss: 0.1555 - f1_m: 0.9758 - val_loss: 0.1859 - val_f1_m: 0.9414\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 790us/step - loss: 0.1420 - f1_m: 0.9768 - val_loss: 0.1700 - val_f1_m: 0.9463\n",
      "0.9767500162124634\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.6495 - f1_m: 0.6805 - val_loss: 0.7149 - val_f1_m: 0.3721\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 819us/step - loss: 0.5349 - f1_m: 0.8492 - val_loss: 0.5660 - val_f1_m: 0.7412\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 825us/step - loss: 0.4054 - f1_m: 0.9330 - val_loss: 0.3963 - val_f1_m: 0.9102\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 803us/step - loss: 0.3087 - f1_m: 0.9557 - val_loss: 0.3070 - val_f1_m: 0.9346\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 796us/step - loss: 0.2473 - f1_m: 0.9703 - val_loss: 0.2786 - val_f1_m: 0.9160\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 836us/step - loss: 0.2068 - f1_m: 0.9725 - val_loss: 0.2464 - val_f1_m: 0.9180\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 823us/step - loss: 0.1792 - f1_m: 0.9755 - val_loss: 0.2048 - val_f1_m: 0.9434\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 811us/step - loss: 0.1590 - f1_m: 0.9785 - val_loss: 0.1884 - val_f1_m: 0.9434\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 801us/step - loss: 0.1435 - f1_m: 0.9808 - val_loss: 0.1674 - val_f1_m: 0.9531\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 794us/step - loss: 0.1315 - f1_m: 0.9803 - val_loss: 0.1489 - val_f1_m: 0.9629\n",
      "0.9802500009536743\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.6512 - f1_m: 0.6665 - val_loss: 0.6903 - val_f1_m: 0.4805\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 823us/step - loss: 0.5377 - f1_m: 0.8625 - val_loss: 0.5423 - val_f1_m: 0.8076\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 823us/step - loss: 0.4070 - f1_m: 0.9352 - val_loss: 0.4342 - val_f1_m: 0.8389\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 813us/step - loss: 0.3092 - f1_m: 0.9528 - val_loss: 0.3114 - val_f1_m: 0.9365\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 804us/step - loss: 0.2466 - f1_m: 0.9643 - val_loss: 0.2669 - val_f1_m: 0.9355\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 800us/step - loss: 0.2054 - f1_m: 0.9725 - val_loss: 0.2263 - val_f1_m: 0.9424\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 803us/step - loss: 0.1773 - f1_m: 0.9745 - val_loss: 0.2056 - val_f1_m: 0.9414\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 794us/step - loss: 0.1568 - f1_m: 0.9790 - val_loss: 0.1976 - val_f1_m: 0.9375\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 803us/step - loss: 0.1415 - f1_m: 0.9778 - val_loss: 0.1718 - val_f1_m: 0.9434\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 791us/step - loss: 0.1295 - f1_m: 0.9815 - val_loss: 0.1581 - val_f1_m: 0.9453\n",
      "0.9815000295639038\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.6506 - f1_m: 0.6857 - val_loss: 0.6952 - val_f1_m: 0.4541\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 816us/step - loss: 0.5395 - f1_m: 0.8555 - val_loss: 0.5641 - val_f1_m: 0.7656\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 815us/step - loss: 0.4134 - f1_m: 0.9255 - val_loss: 0.4349 - val_f1_m: 0.8418\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 798us/step - loss: 0.3177 - f1_m: 0.9433 - val_loss: 0.2906 - val_f1_m: 0.9727\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 804us/step - loss: 0.2560 - f1_m: 0.9615 - val_loss: 0.2538 - val_f1_m: 0.9590\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 804us/step - loss: 0.2150 - f1_m: 0.9660 - val_loss: 0.1813 - val_f1_m: 0.9902\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 800us/step - loss: 0.1864 - f1_m: 0.9737 - val_loss: 0.2047 - val_f1_m: 0.9531\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 788us/step - loss: 0.1660 - f1_m: 0.9747 - val_loss: 0.1599 - val_f1_m: 0.9824\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 800us/step - loss: 0.1503 - f1_m: 0.9772 - val_loss: 0.1458 - val_f1_m: 0.9814\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 809us/step - loss: 0.1381 - f1_m: 0.9805 - val_loss: 0.1352 - val_f1_m: 0.9814\n",
      "0.9804999828338623\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.6531 - f1_m: 0.7077 - val_loss: 0.7015 - val_f1_m: 0.4062\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 812us/step - loss: 0.5413 - f1_m: 0.8537 - val_loss: 0.5738 - val_f1_m: 0.7187\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - 0s 822us/step - loss: 0.4115 - f1_m: 0.9247 - val_loss: 0.4389 - val_f1_m: 0.8398\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 812us/step - loss: 0.3149 - f1_m: 0.9485 - val_loss: 0.3268 - val_f1_m: 0.9248\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 789us/step - loss: 0.2538 - f1_m: 0.9603 - val_loss: 0.2772 - val_f1_m: 0.9346\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 794us/step - loss: 0.2134 - f1_m: 0.9710 - val_loss: 0.2339 - val_f1_m: 0.9502\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 802us/step - loss: 0.1853 - f1_m: 0.9712 - val_loss: 0.1945 - val_f1_m: 0.9648\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 785us/step - loss: 0.1640 - f1_m: 0.9745 - val_loss: 0.2039 - val_f1_m: 0.9404\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 807us/step - loss: 0.1484 - f1_m: 0.9775 - val_loss: 0.1862 - val_f1_m: 0.9482\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 806us/step - loss: 0.1359 - f1_m: 0.9795 - val_loss: 0.1653 - val_f1_m: 0.9551\n",
      "0.9794999957084656\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.6530 - f1_m: 0.6977 - val_loss: 0.6835 - val_f1_m: 0.5273\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 812us/step - loss: 0.5421 - f1_m: 0.8505 - val_loss: 0.5329 - val_f1_m: 0.8369\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 822us/step - loss: 0.4133 - f1_m: 0.9287 - val_loss: 0.4036 - val_f1_m: 0.8838\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 799us/step - loss: 0.3145 - f1_m: 0.9520 - val_loss: 0.3314 - val_f1_m: 0.8936\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 790us/step - loss: 0.2514 - f1_m: 0.9628 - val_loss: 0.2660 - val_f1_m: 0.9189\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 798us/step - loss: 0.2105 - f1_m: 0.9702 - val_loss: 0.2442 - val_f1_m: 0.9111\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 795us/step - loss: 0.1821 - f1_m: 0.9722 - val_loss: 0.1830 - val_f1_m: 0.9619\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 796us/step - loss: 0.1611 - f1_m: 0.9795 - val_loss: 0.1825 - val_f1_m: 0.9463\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 797us/step - loss: 0.1455 - f1_m: 0.9783 - val_loss: 0.1403 - val_f1_m: 0.9736\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 800us/step - loss: 0.1330 - f1_m: 0.9808 - val_loss: 0.1434 - val_f1_m: 0.9658\n",
      "0.9807500243186951\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.6517 - f1_m: 0.6680 - val_loss: 0.7097 - val_f1_m: 0.3867\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 814us/step - loss: 0.5416 - f1_m: 0.8380 - val_loss: 0.5870 - val_f1_m: 0.6963\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 805us/step - loss: 0.4160 - f1_m: 0.9270 - val_loss: 0.4238 - val_f1_m: 0.8633\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 807us/step - loss: 0.3203 - f1_m: 0.9495 - val_loss: 0.3336 - val_f1_m: 0.9062\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 795us/step - loss: 0.2571 - f1_m: 0.9647 - val_loss: 0.2694 - val_f1_m: 0.9404\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 792us/step - loss: 0.2156 - f1_m: 0.9685 - val_loss: 0.2341 - val_f1_m: 0.9453\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 789us/step - loss: 0.1872 - f1_m: 0.9737 - val_loss: 0.1933 - val_f1_m: 0.9580\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 797us/step - loss: 0.1659 - f1_m: 0.9765 - val_loss: 0.1969 - val_f1_m: 0.9424\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 792us/step - loss: 0.1503 - f1_m: 0.9770 - val_loss: 0.1628 - val_f1_m: 0.9580\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 802us/step - loss: 0.1381 - f1_m: 0.9778 - val_loss: 0.1381 - val_f1_m: 0.9746\n",
      "0.9777500033378601\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.6517 - f1_m: 0.6727 - val_loss: 0.7226 - val_f1_m: 0.3203\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 813us/step - loss: 0.5484 - f1_m: 0.8290 - val_loss: 0.5640 - val_f1_m: 0.7852\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 812us/step - loss: 0.4194 - f1_m: 0.9230 - val_loss: 0.4178 - val_f1_m: 0.8867\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 799us/step - loss: 0.3161 - f1_m: 0.9548 - val_loss: 0.3163 - val_f1_m: 0.9375\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 808us/step - loss: 0.2497 - f1_m: 0.9680 - val_loss: 0.2547 - val_f1_m: 0.9551\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 793us/step - loss: 0.2068 - f1_m: 0.9743 - val_loss: 0.2303 - val_f1_m: 0.9443\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 791us/step - loss: 0.1777 - f1_m: 0.9818 - val_loss: 0.2336 - val_f1_m: 0.9111\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 801us/step - loss: 0.1565 - f1_m: 0.9803 - val_loss: 0.1862 - val_f1_m: 0.9512\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 801us/step - loss: 0.1409 - f1_m: 0.9818 - val_loss: 0.1460 - val_f1_m: 0.9746\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 808us/step - loss: 0.1285 - f1_m: 0.9858 - val_loss: 0.1612 - val_f1_m: 0.9541\n",
      "0.9857500195503235\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 1s 1ms/step - loss: 0.6537 - f1_m: 0.6790 - val_loss: 0.7061 - val_f1_m: 0.4082\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 816us/step - loss: 0.5501 - f1_m: 0.8250 - val_loss: 0.5699 - val_f1_m: 0.7617\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 826us/step - loss: 0.4307 - f1_m: 0.9030 - val_loss: 0.4614 - val_f1_m: 0.8154\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 799us/step - loss: 0.3353 - f1_m: 0.9427 - val_loss: 0.3360 - val_f1_m: 0.9238\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 791us/step - loss: 0.2709 - f1_m: 0.9540 - val_loss: 0.2988 - val_f1_m: 0.9121\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 792us/step - loss: 0.2274 - f1_m: 0.9610 - val_loss: 0.2542 - val_f1_m: 0.9268\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 797us/step - loss: 0.1973 - f1_m: 0.9680 - val_loss: 0.2299 - val_f1_m: 0.9307\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 794us/step - loss: 0.1751 - f1_m: 0.9747 - val_loss: 0.1979 - val_f1_m: 0.9443\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 805us/step - loss: 0.1580 - f1_m: 0.9750 - val_loss: 0.1829 - val_f1_m: 0.9463\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 788us/step - loss: 0.1448 - f1_m: 0.9795 - val_loss: 0.1627 - val_f1_m: 0.9561\n",
      "0.9794999957084656\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.6513 - f1_m: 0.6867 - val_loss: 0.7007 - val_f1_m: 0.4277\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 797us/step - loss: 0.5371 - f1_m: 0.8665 - val_loss: 0.5799 - val_f1_m: 0.7002\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 801us/step - loss: 0.4089 - f1_m: 0.9195 - val_loss: 0.4061 - val_f1_m: 0.8838\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 808us/step - loss: 0.3137 - f1_m: 0.9517 - val_loss: 0.3276 - val_f1_m: 0.9062\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 798us/step - loss: 0.2523 - f1_m: 0.9632 - val_loss: 0.2683 - val_f1_m: 0.9316\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 795us/step - loss: 0.2113 - f1_m: 0.9700 - val_loss: 0.2321 - val_f1_m: 0.9385\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 794us/step - loss: 0.1830 - f1_m: 0.9720 - val_loss: 0.2049 - val_f1_m: 0.9434\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 809us/step - loss: 0.1620 - f1_m: 0.9775 - val_loss: 0.1874 - val_f1_m: 0.9453\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 825us/step - loss: 0.1465 - f1_m: 0.9778 - val_loss: 0.1464 - val_f1_m: 0.9756\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 802us/step - loss: 0.1341 - f1_m: 0.9795 - val_loss: 0.1361 - val_f1_m: 0.9756\n",
      "0.9794999957084656\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.6523 - f1_m: 0.6727 - val_loss: 0.7086 - val_f1_m: 0.3828\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 801us/step - loss: 0.5492 - f1_m: 0.8110 - val_loss: 0.5787 - val_f1_m: 0.7227\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 794us/step - loss: 0.4309 - f1_m: 0.9125 - val_loss: 0.4284 - val_f1_m: 0.8936\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 803us/step - loss: 0.3367 - f1_m: 0.9452 - val_loss: 0.3437 - val_f1_m: 0.9248\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 793us/step - loss: 0.2718 - f1_m: 0.9597 - val_loss: 0.3117 - val_f1_m: 0.9033\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 816us/step - loss: 0.2283 - f1_m: 0.9672 - val_loss: 0.2572 - val_f1_m: 0.9307\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 852us/step - loss: 0.1976 - f1_m: 0.9703 - val_loss: 0.2147 - val_f1_m: 0.9551\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 839us/step - loss: 0.1753 - f1_m: 0.9765 - val_loss: 0.1813 - val_f1_m: 0.9678\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 838us/step - loss: 0.1579 - f1_m: 0.9810 - val_loss: 0.1684 - val_f1_m: 0.9648\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 882us/step - loss: 0.1441 - f1_m: 0.9818 - val_loss: 0.1641 - val_f1_m: 0.9590\n",
      "0.9817500114440918\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.6508 - f1_m: 0.6712 - val_loss: 0.7360 - val_f1_m: 0.2734\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 808us/step - loss: 0.5465 - f1_m: 0.8175 - val_loss: 0.5704 - val_f1_m: 0.7549\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 819us/step - loss: 0.4250 - f1_m: 0.9212 - val_loss: 0.4762 - val_f1_m: 0.7949\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 807us/step - loss: 0.3296 - f1_m: 0.9465 - val_loss: 0.3771 - val_f1_m: 0.8555\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 817us/step - loss: 0.2652 - f1_m: 0.9603 - val_loss: 0.3015 - val_f1_m: 0.8945\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 795us/step - loss: 0.2227 - f1_m: 0.9678 - val_loss: 0.2517 - val_f1_m: 0.9248\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 797us/step - loss: 0.1928 - f1_m: 0.9732 - val_loss: 0.2077 - val_f1_m: 0.9551\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 798us/step - loss: 0.1714 - f1_m: 0.9762 - val_loss: 0.1850 - val_f1_m: 0.9590\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 796us/step - loss: 0.1545 - f1_m: 0.9775 - val_loss: 0.1536 - val_f1_m: 0.9717\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 794us/step - loss: 0.1417 - f1_m: 0.9805 - val_loss: 0.1927 - val_f1_m: 0.9189\n",
      "0.9804999828338623\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.6493 - f1_m: 0.6892 - val_loss: 0.6922 - val_f1_m: 0.4746\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 796us/step - loss: 0.5357 - f1_m: 0.8540 - val_loss: 0.5476 - val_f1_m: 0.7881\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 823us/step - loss: 0.4073 - f1_m: 0.9260 - val_loss: 0.3818 - val_f1_m: 0.9229\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 792us/step - loss: 0.3109 - f1_m: 0.9585 - val_loss: 0.3318 - val_f1_m: 0.8916\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 809us/step - loss: 0.2480 - f1_m: 0.9660 - val_loss: 0.2781 - val_f1_m: 0.9033\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 803us/step - loss: 0.2064 - f1_m: 0.9707 - val_loss: 0.1907 - val_f1_m: 0.9697\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 801us/step - loss: 0.1784 - f1_m: 0.9765 - val_loss: 0.1912 - val_f1_m: 0.9531\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 803us/step - loss: 0.1574 - f1_m: 0.9800 - val_loss: 0.1763 - val_f1_m: 0.9531\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 795us/step - loss: 0.1417 - f1_m: 0.9787 - val_loss: 0.1423 - val_f1_m: 0.9688\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 795us/step - loss: 0.1297 - f1_m: 0.9822 - val_loss: 0.1376 - val_f1_m: 0.9639\n",
      "0.9822499752044678\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.6503 - f1_m: 0.6625 - val_loss: 0.7185 - val_f1_m: 0.3359\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 813us/step - loss: 0.5416 - f1_m: 0.8342 - val_loss: 0.5757 - val_f1_m: 0.7236\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 823us/step - loss: 0.4113 - f1_m: 0.9200 - val_loss: 0.3958 - val_f1_m: 0.9199\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 802us/step - loss: 0.3143 - f1_m: 0.9615 - val_loss: 0.3429 - val_f1_m: 0.9023\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 796us/step - loss: 0.2519 - f1_m: 0.9675 - val_loss: 0.2925 - val_f1_m: 0.9121\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 799us/step - loss: 0.2113 - f1_m: 0.9735 - val_loss: 0.2302 - val_f1_m: 0.9482\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 807us/step - loss: 0.1825 - f1_m: 0.9787 - val_loss: 0.2282 - val_f1_m: 0.9248\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 806us/step - loss: 0.1621 - f1_m: 0.9797 - val_loss: 0.2166 - val_f1_m: 0.9229\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 806us/step - loss: 0.1465 - f1_m: 0.9785 - val_loss: 0.1662 - val_f1_m: 0.9561\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 793us/step - loss: 0.1341 - f1_m: 0.9820 - val_loss: 0.1500 - val_f1_m: 0.9609\n",
      "0.9819999933242798\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.6528 - f1_m: 0.6842 - val_loss: 0.7056 - val_f1_m: 0.4102\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 814us/step - loss: 0.5480 - f1_m: 0.8340 - val_loss: 0.5781 - val_f1_m: 0.7256\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 825us/step - loss: 0.4249 - f1_m: 0.9182 - val_loss: 0.4529 - val_f1_m: 0.8223\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 801us/step - loss: 0.3281 - f1_m: 0.9485 - val_loss: 0.3800 - val_f1_m: 0.8477\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 810us/step - loss: 0.2626 - f1_m: 0.9613 - val_loss: 0.2920 - val_f1_m: 0.9238\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 782us/step - loss: 0.2188 - f1_m: 0.9695 - val_loss: 0.2513 - val_f1_m: 0.9375\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 810us/step - loss: 0.1888 - f1_m: 0.9743 - val_loss: 0.2384 - val_f1_m: 0.9229\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 806us/step - loss: 0.1670 - f1_m: 0.9753 - val_loss: 0.2127 - val_f1_m: 0.9395\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 802us/step - loss: 0.1507 - f1_m: 0.9772 - val_loss: 0.1773 - val_f1_m: 0.9648\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 816us/step - loss: 0.1377 - f1_m: 0.9793 - val_loss: 0.1611 - val_f1_m: 0.9678\n",
      "0.9792500138282776\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.6532 - f1_m: 0.6655 - val_loss: 0.7134 - val_f1_m: 0.3750\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 811us/step - loss: 0.5502 - f1_m: 0.8112 - val_loss: 0.5708 - val_f1_m: 0.7461\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 825us/step - loss: 0.4308 - f1_m: 0.9018 - val_loss: 0.4129 - val_f1_m: 0.9053\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 798us/step - loss: 0.3365 - f1_m: 0.9438 - val_loss: 0.3529 - val_f1_m: 0.8828\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 794us/step - loss: 0.2717 - f1_m: 0.9563 - val_loss: 0.3000 - val_f1_m: 0.8936\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 805us/step - loss: 0.2287 - f1_m: 0.9613 - val_loss: 0.2599 - val_f1_m: 0.9082\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 811us/step - loss: 0.1980 - f1_m: 0.9690 - val_loss: 0.2075 - val_f1_m: 0.9512\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 805us/step - loss: 0.1759 - f1_m: 0.9730 - val_loss: 0.1871 - val_f1_m: 0.9551\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 820us/step - loss: 0.1591 - f1_m: 0.9730 - val_loss: 0.1836 - val_f1_m: 0.9424\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 806us/step - loss: 0.1458 - f1_m: 0.9780 - val_loss: 0.1619 - val_f1_m: 0.9580\n",
      "0.9779999852180481\n",
      "if any\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - 0s 1ms/step - loss: 0.6507 - f1_m: 0.6730 - val_loss: 0.7226 - val_f1_m: 0.3213\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 823us/step - loss: 0.5464 - f1_m: 0.8212 - val_loss: 0.5677 - val_f1_m: 0.7568\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 815us/step - loss: 0.4260 - f1_m: 0.9110 - val_loss: 0.4470 - val_f1_m: 0.8496\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 797us/step - loss: 0.3305 - f1_m: 0.9482 - val_loss: 0.3906 - val_f1_m: 0.8369\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 789us/step - loss: 0.2672 - f1_m: 0.9570 - val_loss: 0.2871 - val_f1_m: 0.9248\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 802us/step - loss: 0.2243 - f1_m: 0.9675 - val_loss: 0.2235 - val_f1_m: 0.9590\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 796us/step - loss: 0.1952 - f1_m: 0.9743 - val_loss: 0.2230 - val_f1_m: 0.9375\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 800us/step - loss: 0.1732 - f1_m: 0.9770 - val_loss: 0.1828 - val_f1_m: 0.9600\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 794us/step - loss: 0.1566 - f1_m: 0.9790 - val_loss: 0.1834 - val_f1_m: 0.9424\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 799us/step - loss: 0.1433 - f1_m: 0.9800 - val_loss: 0.1788 - val_f1_m: 0.9365\n",
      "0.9800000190734863\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.6522 - f1_m: 0.6825 - val_loss: 0.6988 - val_f1_m: 0.4453\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 809us/step - loss: 0.5385 - f1_m: 0.8447 - val_loss: 0.5341 - val_f1_m: 0.8252\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 811us/step - loss: 0.4113 - f1_m: 0.9337 - val_loss: 0.4231 - val_f1_m: 0.8604\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 794us/step - loss: 0.3140 - f1_m: 0.9548 - val_loss: 0.3619 - val_f1_m: 0.8604\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 798us/step - loss: 0.2512 - f1_m: 0.9622 - val_loss: 0.2640 - val_f1_m: 0.9355\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 796us/step - loss: 0.2093 - f1_m: 0.9707 - val_loss: 0.2406 - val_f1_m: 0.9268\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 795us/step - loss: 0.1809 - f1_m: 0.9785 - val_loss: 0.2087 - val_f1_m: 0.9414\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 801us/step - loss: 0.1599 - f1_m: 0.9770 - val_loss: 0.1835 - val_f1_m: 0.9502\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 807us/step - loss: 0.1439 - f1_m: 0.9815 - val_loss: 0.1776 - val_f1_m: 0.9434\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 796us/step - loss: 0.1314 - f1_m: 0.9820 - val_loss: 0.1566 - val_f1_m: 0.9551\n",
      "0.9819999933242798\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.6535 - f1_m: 0.6725 - val_loss: 0.7157 - val_f1_m: 0.3525\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 802us/step - loss: 0.5505 - f1_m: 0.8277 - val_loss: 0.5934 - val_f1_m: 0.6914\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 807us/step - loss: 0.4203 - f1_m: 0.9265 - val_loss: 0.4300 - val_f1_m: 0.8594\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 795us/step - loss: 0.3194 - f1_m: 0.9550 - val_loss: 0.3588 - val_f1_m: 0.8711\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 807us/step - loss: 0.2551 - f1_m: 0.9663 - val_loss: 0.2775 - val_f1_m: 0.9268\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 796us/step - loss: 0.2133 - f1_m: 0.9720 - val_loss: 0.2267 - val_f1_m: 0.9580\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 811us/step - loss: 0.1846 - f1_m: 0.9732 - val_loss: 0.1929 - val_f1_m: 0.9668\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 805us/step - loss: 0.1639 - f1_m: 0.9787 - val_loss: 0.1950 - val_f1_m: 0.9414\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 795us/step - loss: 0.1475 - f1_m: 0.9785 - val_loss: 0.1648 - val_f1_m: 0.9668\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 787us/step - loss: 0.1352 - f1_m: 0.9808 - val_loss: 0.1471 - val_f1_m: 0.9727\n",
      "0.9807500243186951\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.6512 - f1_m: 0.6727 - val_loss: 0.6898 - val_f1_m: 0.4697\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 808us/step - loss: 0.5398 - f1_m: 0.8450 - val_loss: 0.5265 - val_f1_m: 0.8428\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 831us/step - loss: 0.4127 - f1_m: 0.9310 - val_loss: 0.4546 - val_f1_m: 0.8047\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 802us/step - loss: 0.3157 - f1_m: 0.9498 - val_loss: 0.3266 - val_f1_m: 0.9209\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 803us/step - loss: 0.2532 - f1_m: 0.9605 - val_loss: 0.2965 - val_f1_m: 0.9033\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 816us/step - loss: 0.2123 - f1_m: 0.9668 - val_loss: 0.2542 - val_f1_m: 0.9219\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 806us/step - loss: 0.1841 - f1_m: 0.9712 - val_loss: 0.2143 - val_f1_m: 0.9463\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 801us/step - loss: 0.1637 - f1_m: 0.9750 - val_loss: 0.1928 - val_f1_m: 0.9521\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 805us/step - loss: 0.1476 - f1_m: 0.9755 - val_loss: 0.1637 - val_f1_m: 0.9746\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 811us/step - loss: 0.1354 - f1_m: 0.9760 - val_loss: 0.1410 - val_f1_m: 0.9775\n",
      "0.9760000109672546\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.6528 - f1_m: 0.6752 - val_loss: 0.7034 - val_f1_m: 0.4053\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 811us/step - loss: 0.5470 - f1_m: 0.8302 - val_loss: 0.5725 - val_f1_m: 0.7432\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 822us/step - loss: 0.4252 - f1_m: 0.9132 - val_loss: 0.4149 - val_f1_m: 0.9131\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 794us/step - loss: 0.3296 - f1_m: 0.9480 - val_loss: 0.3558 - val_f1_m: 0.9014\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 811us/step - loss: 0.2661 - f1_m: 0.9580 - val_loss: 0.2697 - val_f1_m: 0.9570\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 794us/step - loss: 0.2237 - f1_m: 0.9703 - val_loss: 0.2506 - val_f1_m: 0.9414\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 803us/step - loss: 0.1943 - f1_m: 0.9712 - val_loss: 0.2146 - val_f1_m: 0.9551\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 804us/step - loss: 0.1724 - f1_m: 0.9762 - val_loss: 0.1890 - val_f1_m: 0.9600\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 790us/step - loss: 0.1554 - f1_m: 0.9783 - val_loss: 0.1747 - val_f1_m: 0.9600\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 802us/step - loss: 0.1428 - f1_m: 0.9765 - val_loss: 0.1555 - val_f1_m: 0.9688\n",
      "0.9764999747276306\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.6531 - f1_m: 0.6842 - val_loss: 0.7122 - val_f1_m: 0.3564\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 822us/step - loss: 0.5479 - f1_m: 0.8292 - val_loss: 0.5597 - val_f1_m: 0.7637\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 819us/step - loss: 0.4163 - f1_m: 0.9367 - val_loss: 0.4260 - val_f1_m: 0.8525\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 804us/step - loss: 0.3126 - f1_m: 0.9565 - val_loss: 0.3269 - val_f1_m: 0.9004\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 812us/step - loss: 0.2464 - f1_m: 0.9695 - val_loss: 0.2667 - val_f1_m: 0.9307\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 807us/step - loss: 0.2045 - f1_m: 0.9762 - val_loss: 0.2363 - val_f1_m: 0.9326\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 809us/step - loss: 0.1757 - f1_m: 0.9793 - val_loss: 0.2274 - val_f1_m: 0.9189\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 804us/step - loss: 0.1551 - f1_m: 0.9787 - val_loss: 0.2080 - val_f1_m: 0.9219\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 815us/step - loss: 0.1398 - f1_m: 0.9825 - val_loss: 0.1797 - val_f1_m: 0.9414\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 818us/step - loss: 0.1275 - f1_m: 0.9830 - val_loss: 0.1716 - val_f1_m: 0.9385\n",
      "0.9829999804496765\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 1s 1ms/step - loss: 0.6525 - f1_m: 0.6792 - val_loss: 0.6988 - val_f1_m: 0.4219\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 798us/step - loss: 0.5513 - f1_m: 0.8272 - val_loss: 0.5878 - val_f1_m: 0.6963\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 799us/step - loss: 0.4329 - f1_m: 0.9085 - val_loss: 0.4765 - val_f1_m: 0.7832\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 794us/step - loss: 0.3293 - f1_m: 0.9392 - val_loss: 0.3227 - val_f1_m: 0.9307\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 795us/step - loss: 0.2585 - f1_m: 0.9680 - val_loss: 0.3078 - val_f1_m: 0.8691\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 817us/step - loss: 0.2155 - f1_m: 0.9682 - val_loss: 0.2265 - val_f1_m: 0.9482\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 840us/step - loss: 0.1860 - f1_m: 0.9705 - val_loss: 0.2030 - val_f1_m: 0.9492\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 846us/step - loss: 0.1648 - f1_m: 0.9762 - val_loss: 0.1656 - val_f1_m: 0.9727\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 845us/step - loss: 0.1492 - f1_m: 0.9772 - val_loss: 0.1488 - val_f1_m: 0.9746\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 857us/step - loss: 0.1372 - f1_m: 0.9780 - val_loss: 0.1405 - val_f1_m: 0.9746\n",
      "0.9779999852180481\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.6526 - f1_m: 0.7287 - val_loss: 0.7002 - val_f1_m: 0.4316\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 804us/step - loss: 0.5414 - f1_m: 0.8475 - val_loss: 0.5531 - val_f1_m: 0.7695\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 816us/step - loss: 0.4119 - f1_m: 0.9305 - val_loss: 0.4111 - val_f1_m: 0.8887\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 791us/step - loss: 0.3145 - f1_m: 0.9507 - val_loss: 0.3289 - val_f1_m: 0.9219\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 792us/step - loss: 0.2513 - f1_m: 0.9647 - val_loss: 0.3167 - val_f1_m: 0.8662\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 802us/step - loss: 0.2100 - f1_m: 0.9645 - val_loss: 0.2199 - val_f1_m: 0.9590\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 801us/step - loss: 0.1819 - f1_m: 0.9722 - val_loss: 0.2010 - val_f1_m: 0.9551\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 791us/step - loss: 0.1614 - f1_m: 0.9743 - val_loss: 0.1876 - val_f1_m: 0.9531\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 804us/step - loss: 0.1455 - f1_m: 0.9780 - val_loss: 0.1575 - val_f1_m: 0.9707\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 783us/step - loss: 0.1338 - f1_m: 0.9800 - val_loss: 0.1637 - val_f1_m: 0.9531\n",
      "0.9800000190734863\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.6523 - f1_m: 0.7090 - val_loss: 0.7171 - val_f1_m: 0.3232\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 821us/step - loss: 0.5508 - f1_m: 0.8147 - val_loss: 0.5884 - val_f1_m: 0.7344\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 818us/step - loss: 0.4309 - f1_m: 0.9202 - val_loss: 0.4525 - val_f1_m: 0.8457\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 803us/step - loss: 0.3332 - f1_m: 0.9477 - val_loss: 0.3832 - val_f1_m: 0.8535\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 807us/step - loss: 0.2667 - f1_m: 0.9603 - val_loss: 0.2984 - val_f1_m: 0.9180\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 796us/step - loss: 0.2220 - f1_m: 0.9710 - val_loss: 0.2991 - val_f1_m: 0.8711\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 795us/step - loss: 0.1914 - f1_m: 0.9747 - val_loss: 0.2352 - val_f1_m: 0.9307\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 792us/step - loss: 0.1688 - f1_m: 0.9760 - val_loss: 0.2286 - val_f1_m: 0.9160\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 794us/step - loss: 0.1518 - f1_m: 0.9790 - val_loss: 0.1855 - val_f1_m: 0.9551\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 790us/step - loss: 0.1386 - f1_m: 0.9805 - val_loss: 0.1751 - val_f1_m: 0.9551\n",
      "0.9804999828338623\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.6515 - f1_m: 0.6765 - val_loss: 0.7246 - val_f1_m: 0.3203\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 815us/step - loss: 0.5451 - f1_m: 0.8235 - val_loss: 0.5486 - val_f1_m: 0.7871\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 829us/step - loss: 0.4141 - f1_m: 0.9345 - val_loss: 0.4286 - val_f1_m: 0.8447\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 801us/step - loss: 0.3137 - f1_m: 0.9562 - val_loss: 0.3410 - val_f1_m: 0.8818\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 797us/step - loss: 0.2491 - f1_m: 0.9678 - val_loss: 0.2640 - val_f1_m: 0.9355\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 794us/step - loss: 0.2086 - f1_m: 0.9712 - val_loss: 0.2228 - val_f1_m: 0.9463\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 802us/step - loss: 0.1796 - f1_m: 0.9780 - val_loss: 0.2173 - val_f1_m: 0.9297\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 795us/step - loss: 0.1593 - f1_m: 0.9762 - val_loss: 0.1690 - val_f1_m: 0.9639\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 802us/step - loss: 0.1438 - f1_m: 0.9793 - val_loss: 0.1864 - val_f1_m: 0.9326\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 808us/step - loss: 0.1313 - f1_m: 0.9805 - val_loss: 0.1796 - val_f1_m: 0.9287\n",
      "0.9804999828338623\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.6542 - f1_m: 0.6997 - val_loss: 0.7162 - val_f1_m: 0.3545\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 811us/step - loss: 0.5466 - f1_m: 0.8447 - val_loss: 0.5603 - val_f1_m: 0.7480\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 813us/step - loss: 0.4155 - f1_m: 0.9200 - val_loss: 0.4074 - val_f1_m: 0.8926\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 809us/step - loss: 0.3183 - f1_m: 0.9527 - val_loss: 0.3193 - val_f1_m: 0.9229\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 804us/step - loss: 0.2552 - f1_m: 0.9690 - val_loss: 0.2793 - val_f1_m: 0.9180\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 796us/step - loss: 0.2148 - f1_m: 0.9715 - val_loss: 0.2322 - val_f1_m: 0.9385\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 808us/step - loss: 0.1863 - f1_m: 0.9747 - val_loss: 0.1914 - val_f1_m: 0.9619\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 800us/step - loss: 0.1656 - f1_m: 0.9793 - val_loss: 0.1722 - val_f1_m: 0.9629\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 798us/step - loss: 0.1498 - f1_m: 0.9800 - val_loss: 0.1620 - val_f1_m: 0.9600\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 809us/step - loss: 0.1373 - f1_m: 0.9815 - val_loss: 0.1430 - val_f1_m: 0.9688\n",
      "0.9815000295639038\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.6566 - f1_m: 0.6850 - val_loss: 0.6994 - val_f1_m: 0.4170\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 800us/step - loss: 0.5548 - f1_m: 0.8447 - val_loss: 0.5969 - val_f1_m: 0.6924\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 815us/step - loss: 0.4330 - f1_m: 0.9207 - val_loss: 0.4674 - val_f1_m: 0.8145\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 807us/step - loss: 0.3349 - f1_m: 0.9463 - val_loss: 0.3664 - val_f1_m: 0.8926\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 791us/step - loss: 0.2689 - f1_m: 0.9613 - val_loss: 0.3065 - val_f1_m: 0.9121\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 805us/step - loss: 0.2244 - f1_m: 0.9697 - val_loss: 0.2562 - val_f1_m: 0.9395\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 810us/step - loss: 0.1938 - f1_m: 0.9725 - val_loss: 0.2103 - val_f1_m: 0.9688\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 798us/step - loss: 0.1710 - f1_m: 0.9758 - val_loss: 0.2007 - val_f1_m: 0.9600\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - 0s 814us/step - loss: 0.1542 - f1_m: 0.9765 - val_loss: 0.1777 - val_f1_m: 0.9697\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 797us/step - loss: 0.1407 - f1_m: 0.9793 - val_loss: 0.1780 - val_f1_m: 0.9570\n",
      "0.9792500138282776\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.6508 - f1_m: 0.6650 - val_loss: 0.7119 - val_f1_m: 0.3721\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 820us/step - loss: 0.5367 - f1_m: 0.8410 - val_loss: 0.5099 - val_f1_m: 0.8652\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 818us/step - loss: 0.4064 - f1_m: 0.9335 - val_loss: 0.4089 - val_f1_m: 0.8711\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 811us/step - loss: 0.3099 - f1_m: 0.9570 - val_loss: 0.3149 - val_f1_m: 0.9180\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 796us/step - loss: 0.2491 - f1_m: 0.9680 - val_loss: 0.2517 - val_f1_m: 0.9434\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 791us/step - loss: 0.2089 - f1_m: 0.9735 - val_loss: 0.2346 - val_f1_m: 0.9229\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 792us/step - loss: 0.1813 - f1_m: 0.9745 - val_loss: 0.1995 - val_f1_m: 0.9414\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 784us/step - loss: 0.1612 - f1_m: 0.9800 - val_loss: 0.2046 - val_f1_m: 0.9170\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 787us/step - loss: 0.1460 - f1_m: 0.9778 - val_loss: 0.1718 - val_f1_m: 0.9414\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 792us/step - loss: 0.1337 - f1_m: 0.9830 - val_loss: 0.1657 - val_f1_m: 0.9395\n",
      "0.9829999804496765\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.6527 - f1_m: 0.6787 - val_loss: 0.7126 - val_f1_m: 0.3545\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 804us/step - loss: 0.5457 - f1_m: 0.8355 - val_loss: 0.5490 - val_f1_m: 0.7920\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 802us/step - loss: 0.4175 - f1_m: 0.9237 - val_loss: 0.4369 - val_f1_m: 0.8408\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 806us/step - loss: 0.3193 - f1_m: 0.9463 - val_loss: 0.3197 - val_f1_m: 0.9414\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 797us/step - loss: 0.2561 - f1_m: 0.9635 - val_loss: 0.2856 - val_f1_m: 0.9189\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 790us/step - loss: 0.2146 - f1_m: 0.9697 - val_loss: 0.2278 - val_f1_m: 0.9561\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 791us/step - loss: 0.1861 - f1_m: 0.9753 - val_loss: 0.2165 - val_f1_m: 0.9453\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 795us/step - loss: 0.1656 - f1_m: 0.9762 - val_loss: 0.1739 - val_f1_m: 0.9639\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 800us/step - loss: 0.1498 - f1_m: 0.9778 - val_loss: 0.1851 - val_f1_m: 0.9473\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 800us/step - loss: 0.1374 - f1_m: 0.9810 - val_loss: 0.1591 - val_f1_m: 0.9580\n",
      "0.9810000061988831\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.6531 - f1_m: 0.6770 - val_loss: 0.6907 - val_f1_m: 0.4727\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 805us/step - loss: 0.5537 - f1_m: 0.8267 - val_loss: 0.5408 - val_f1_m: 0.8271\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 826us/step - loss: 0.4322 - f1_m: 0.9262 - val_loss: 0.4419 - val_f1_m: 0.8252\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 800us/step - loss: 0.3229 - f1_m: 0.9517 - val_loss: 0.3240 - val_f1_m: 0.9014\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 800us/step - loss: 0.2521 - f1_m: 0.9628 - val_loss: 0.2676 - val_f1_m: 0.9189\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 808us/step - loss: 0.2087 - f1_m: 0.9720 - val_loss: 0.2466 - val_f1_m: 0.9082\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 808us/step - loss: 0.1797 - f1_m: 0.9772 - val_loss: 0.2034 - val_f1_m: 0.9434\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 800us/step - loss: 0.1590 - f1_m: 0.9765 - val_loss: 0.2026 - val_f1_m: 0.9199\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 807us/step - loss: 0.1432 - f1_m: 0.9827 - val_loss: 0.1894 - val_f1_m: 0.9268\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 795us/step - loss: 0.1311 - f1_m: 0.9812 - val_loss: 0.1590 - val_f1_m: 0.9482\n",
      "0.981249988079071\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.6524 - f1_m: 0.6635 - val_loss: 0.6835 - val_f1_m: 0.5225\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 814us/step - loss: 0.5397 - f1_m: 0.8577 - val_loss: 0.5350 - val_f1_m: 0.8311\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 816us/step - loss: 0.4100 - f1_m: 0.9257 - val_loss: 0.4075 - val_f1_m: 0.9023\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 796us/step - loss: 0.3118 - f1_m: 0.9530 - val_loss: 0.3306 - val_f1_m: 0.9102\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 799us/step - loss: 0.2487 - f1_m: 0.9643 - val_loss: 0.2809 - val_f1_m: 0.9141\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 800us/step - loss: 0.2080 - f1_m: 0.9690 - val_loss: 0.2361 - val_f1_m: 0.9346\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 804us/step - loss: 0.1801 - f1_m: 0.9765 - val_loss: 0.2029 - val_f1_m: 0.9482\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 800us/step - loss: 0.1595 - f1_m: 0.9787 - val_loss: 0.1785 - val_f1_m: 0.9570\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 809us/step - loss: 0.1439 - f1_m: 0.9783 - val_loss: 0.1393 - val_f1_m: 0.9834\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 790us/step - loss: 0.1327 - f1_m: 0.9808 - val_loss: 0.1526 - val_f1_m: 0.9600\n",
      "0.9807500243186951\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.6511 - f1_m: 0.6835 - val_loss: 0.6931 - val_f1_m: 0.4697\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 821us/step - loss: 0.5396 - f1_m: 0.8425 - val_loss: 0.5531 - val_f1_m: 0.7764\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 825us/step - loss: 0.4104 - f1_m: 0.9317 - val_loss: 0.4492 - val_f1_m: 0.8184\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 803us/step - loss: 0.3129 - f1_m: 0.9498 - val_loss: 0.3559 - val_f1_m: 0.8701\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 793us/step - loss: 0.2506 - f1_m: 0.9597 - val_loss: 0.2802 - val_f1_m: 0.9150\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 800us/step - loss: 0.2098 - f1_m: 0.9690 - val_loss: 0.2448 - val_f1_m: 0.9219\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 812us/step - loss: 0.1819 - f1_m: 0.9753 - val_loss: 0.2177 - val_f1_m: 0.9248\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 788us/step - loss: 0.1608 - f1_m: 0.9750 - val_loss: 0.1774 - val_f1_m: 0.9512\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 805us/step - loss: 0.1452 - f1_m: 0.9785 - val_loss: 0.1628 - val_f1_m: 0.9551\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 804us/step - loss: 0.1327 - f1_m: 0.9795 - val_loss: 0.1859 - val_f1_m: 0.9180\n",
      "0.9794999957084656\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.6540 - f1_m: 0.6812 - val_loss: 0.6904 - val_f1_m: 0.4648\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 803us/step - loss: 0.5528 - f1_m: 0.8235 - val_loss: 0.5868 - val_f1_m: 0.7227\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 806us/step - loss: 0.4331 - f1_m: 0.9115 - val_loss: 0.4812 - val_f1_m: 0.7959\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 811us/step - loss: 0.3370 - f1_m: 0.9423 - val_loss: 0.3970 - val_f1_m: 0.8389\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 811us/step - loss: 0.2719 - f1_m: 0.9550 - val_loss: 0.2742 - val_f1_m: 0.9492\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 807us/step - loss: 0.2279 - f1_m: 0.9632 - val_loss: 0.2504 - val_f1_m: 0.9365\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 806us/step - loss: 0.1972 - f1_m: 0.9685 - val_loss: 0.2093 - val_f1_m: 0.9580\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 800us/step - loss: 0.1747 - f1_m: 0.9743 - val_loss: 0.2226 - val_f1_m: 0.9268\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 802us/step - loss: 0.1574 - f1_m: 0.9743 - val_loss: 0.1861 - val_f1_m: 0.9443\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 782us/step - loss: 0.1442 - f1_m: 0.9740 - val_loss: 0.1912 - val_f1_m: 0.9346\n",
      "0.9739999771118164\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.6513 - f1_m: 0.6925 - val_loss: 0.7003 - val_f1_m: 0.4092\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 807us/step - loss: 0.5373 - f1_m: 0.8552 - val_loss: 0.5631 - val_f1_m: 0.7520\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 812us/step - loss: 0.4063 - f1_m: 0.9310 - val_loss: 0.4129 - val_f1_m: 0.8955\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 788us/step - loss: 0.3085 - f1_m: 0.9557 - val_loss: 0.3472 - val_f1_m: 0.8936\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 800us/step - loss: 0.2457 - f1_m: 0.9655 - val_loss: 0.2558 - val_f1_m: 0.9521\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 797us/step - loss: 0.2053 - f1_m: 0.9728 - val_loss: 0.2497 - val_f1_m: 0.9248\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 798us/step - loss: 0.1776 - f1_m: 0.9740 - val_loss: 0.1941 - val_f1_m: 0.9619\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 808us/step - loss: 0.1575 - f1_m: 0.9760 - val_loss: 0.1827 - val_f1_m: 0.9541\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 833us/step - loss: 0.1424 - f1_m: 0.9787 - val_loss: 0.1704 - val_f1_m: 0.9531\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 837us/step - loss: 0.1306 - f1_m: 0.9797 - val_loss: 0.1686 - val_f1_m: 0.9453\n",
      "0.9797499775886536\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.6512 - f1_m: 0.6720 - val_loss: 0.6802 - val_f1_m: 0.5352\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 915us/step - loss: 0.5385 - f1_m: 0.8595 - val_loss: 0.5471 - val_f1_m: 0.8115\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 829us/step - loss: 0.4085 - f1_m: 0.9312 - val_loss: 0.3999 - val_f1_m: 0.9131\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 804us/step - loss: 0.3114 - f1_m: 0.9565 - val_loss: 0.3254 - val_f1_m: 0.9199\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 805us/step - loss: 0.2493 - f1_m: 0.9657 - val_loss: 0.2489 - val_f1_m: 0.9502\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 793us/step - loss: 0.2087 - f1_m: 0.9743 - val_loss: 0.2450 - val_f1_m: 0.9268\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 798us/step - loss: 0.1804 - f1_m: 0.9753 - val_loss: 0.1953 - val_f1_m: 0.9521\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 796us/step - loss: 0.1596 - f1_m: 0.9778 - val_loss: 0.1704 - val_f1_m: 0.9590\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 806us/step - loss: 0.1438 - f1_m: 0.9805 - val_loss: 0.1558 - val_f1_m: 0.9600\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 799us/step - loss: 0.1315 - f1_m: 0.9815 - val_loss: 0.1466 - val_f1_m: 0.9600\n",
      "0.9815000295639038\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 1s 1ms/step - loss: 0.6535 - f1_m: 0.6635 - val_loss: 0.7131 - val_f1_m: 0.3623\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 803us/step - loss: 0.5500 - f1_m: 0.8195 - val_loss: 0.5775 - val_f1_m: 0.7383\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 806us/step - loss: 0.4304 - f1_m: 0.9137 - val_loss: 0.4250 - val_f1_m: 0.8750\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 802us/step - loss: 0.3339 - f1_m: 0.9440 - val_loss: 0.3369 - val_f1_m: 0.9033\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 800us/step - loss: 0.2679 - f1_m: 0.9592 - val_loss: 0.2707 - val_f1_m: 0.9336\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 801us/step - loss: 0.2237 - f1_m: 0.9670 - val_loss: 0.2103 - val_f1_m: 0.9658\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 791us/step - loss: 0.1931 - f1_m: 0.9753 - val_loss: 0.2213 - val_f1_m: 0.9189\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 795us/step - loss: 0.1710 - f1_m: 0.9775 - val_loss: 0.1799 - val_f1_m: 0.9512\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 802us/step - loss: 0.1540 - f1_m: 0.9800 - val_loss: 0.1600 - val_f1_m: 0.9609\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 808us/step - loss: 0.1406 - f1_m: 0.9810 - val_loss: 0.1385 - val_f1_m: 0.9746\n",
      "0.9810000061988831\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.6528 - f1_m: 0.6835 - val_loss: 0.6959 - val_f1_m: 0.4580\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 812us/step - loss: 0.5513 - f1_m: 0.8257 - val_loss: 0.5883 - val_f1_m: 0.7100\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 805us/step - loss: 0.4329 - f1_m: 0.9197 - val_loss: 0.4588 - val_f1_m: 0.8223\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 794us/step - loss: 0.3365 - f1_m: 0.9460 - val_loss: 0.3424 - val_f1_m: 0.9102\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 784us/step - loss: 0.2714 - f1_m: 0.9650 - val_loss: 0.2650 - val_f1_m: 0.9551\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 795us/step - loss: 0.2271 - f1_m: 0.9703 - val_loss: 0.2313 - val_f1_m: 0.9551\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 805us/step - loss: 0.1962 - f1_m: 0.9760 - val_loss: 0.2253 - val_f1_m: 0.9346\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 804us/step - loss: 0.1733 - f1_m: 0.9787 - val_loss: 0.1916 - val_f1_m: 0.9551\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 787us/step - loss: 0.1561 - f1_m: 0.9800 - val_loss: 0.1627 - val_f1_m: 0.9688\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 802us/step - loss: 0.1424 - f1_m: 0.9793 - val_loss: 0.1515 - val_f1_m: 0.9688\n",
      "0.9792500138282776\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.6482 - f1_m: 0.7000 - val_loss: 0.6885 - val_f1_m: 0.5020\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 801us/step - loss: 0.5296 - f1_m: 0.8515 - val_loss: 0.5385 - val_f1_m: 0.8018\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 818us/step - loss: 0.3985 - f1_m: 0.9380 - val_loss: 0.4553 - val_f1_m: 0.8057\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 798us/step - loss: 0.3034 - f1_m: 0.9510 - val_loss: 0.3216 - val_f1_m: 0.9102\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 807us/step - loss: 0.2423 - f1_m: 0.9617 - val_loss: 0.2625 - val_f1_m: 0.9365\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 791us/step - loss: 0.2029 - f1_m: 0.9725 - val_loss: 0.2356 - val_f1_m: 0.9346\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 800us/step - loss: 0.1753 - f1_m: 0.9762 - val_loss: 0.1932 - val_f1_m: 0.9609\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 798us/step - loss: 0.1561 - f1_m: 0.9787 - val_loss: 0.1808 - val_f1_m: 0.9590\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 805us/step - loss: 0.1406 - f1_m: 0.9783 - val_loss: 0.1442 - val_f1_m: 0.9785\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 794us/step - loss: 0.1292 - f1_m: 0.9812 - val_loss: 0.1421 - val_f1_m: 0.9766\n",
      "0.981249988079071\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.6524 - f1_m: 0.7010 - val_loss: 0.6974 - val_f1_m: 0.4375\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 783us/step - loss: 0.5398 - f1_m: 0.8585 - val_loss: 0.5869 - val_f1_m: 0.6914\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 809us/step - loss: 0.4077 - f1_m: 0.9240 - val_loss: 0.3976 - val_f1_m: 0.8984\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 790us/step - loss: 0.3093 - f1_m: 0.9580 - val_loss: 0.3379 - val_f1_m: 0.8877\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 798us/step - loss: 0.2472 - f1_m: 0.9668 - val_loss: 0.2796 - val_f1_m: 0.9053\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 789us/step - loss: 0.2067 - f1_m: 0.9718 - val_loss: 0.2030 - val_f1_m: 0.9707\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - 0s 803us/step - loss: 0.1784 - f1_m: 0.9768 - val_loss: 0.1873 - val_f1_m: 0.9600\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 799us/step - loss: 0.1585 - f1_m: 0.9797 - val_loss: 0.1929 - val_f1_m: 0.9326\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 784us/step - loss: 0.1430 - f1_m: 0.9810 - val_loss: 0.1690 - val_f1_m: 0.9463\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 783us/step - loss: 0.1308 - f1_m: 0.9820 - val_loss: 0.1515 - val_f1_m: 0.9639\n",
      "0.9819999933242798\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.6515 - f1_m: 0.6910 - val_loss: 0.7048 - val_f1_m: 0.4092\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 808us/step - loss: 0.5406 - f1_m: 0.8380 - val_loss: 0.5393 - val_f1_m: 0.8203\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 805us/step - loss: 0.4133 - f1_m: 0.9315 - val_loss: 0.4216 - val_f1_m: 0.8662\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 782us/step - loss: 0.3165 - f1_m: 0.9505 - val_loss: 0.2937 - val_f1_m: 0.9688\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 786us/step - loss: 0.2544 - f1_m: 0.9630 - val_loss: 0.2575 - val_f1_m: 0.9580\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 783us/step - loss: 0.2127 - f1_m: 0.9725 - val_loss: 0.2343 - val_f1_m: 0.9482\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 800us/step - loss: 0.1836 - f1_m: 0.9753 - val_loss: 0.2181 - val_f1_m: 0.9424\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 797us/step - loss: 0.1620 - f1_m: 0.9800 - val_loss: 0.2035 - val_f1_m: 0.9414\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 784us/step - loss: 0.1462 - f1_m: 0.9783 - val_loss: 0.1667 - val_f1_m: 0.9668\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 804us/step - loss: 0.1332 - f1_m: 0.9803 - val_loss: 0.1954 - val_f1_m: 0.9209\n",
      "0.9802500009536743\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.6514 - f1_m: 0.6800 - val_loss: 0.6966 - val_f1_m: 0.4443\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 802us/step - loss: 0.5468 - f1_m: 0.8272 - val_loss: 0.5514 - val_f1_m: 0.7998\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 805us/step - loss: 0.4269 - f1_m: 0.9218 - val_loss: 0.4126 - val_f1_m: 0.9092\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 788us/step - loss: 0.3312 - f1_m: 0.9522 - val_loss: 0.3226 - val_f1_m: 0.9502\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 789us/step - loss: 0.2661 - f1_m: 0.9655 - val_loss: 0.3056 - val_f1_m: 0.9004\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 792us/step - loss: 0.2229 - f1_m: 0.9695 - val_loss: 0.2281 - val_f1_m: 0.9639\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 791us/step - loss: 0.1925 - f1_m: 0.9775 - val_loss: 0.2198 - val_f1_m: 0.9414\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 787us/step - loss: 0.1702 - f1_m: 0.9780 - val_loss: 0.2021 - val_f1_m: 0.9424\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 779us/step - loss: 0.1532 - f1_m: 0.9800 - val_loss: 0.1687 - val_f1_m: 0.9697\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 816us/step - loss: 0.1397 - f1_m: 0.9822 - val_loss: 0.1410 - val_f1_m: 0.9795\n",
      "0.9822499752044678\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.6528 - f1_m: 0.6910 - val_loss: 0.7095 - val_f1_m: 0.3691\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 815us/step - loss: 0.5403 - f1_m: 0.8517 - val_loss: 0.5401 - val_f1_m: 0.7998\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 798us/step - loss: 0.4089 - f1_m: 0.9335 - val_loss: 0.4116 - val_f1_m: 0.8906\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 791us/step - loss: 0.3107 - f1_m: 0.9567 - val_loss: 0.3530 - val_f1_m: 0.8789\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 783us/step - loss: 0.2472 - f1_m: 0.9655 - val_loss: 0.2908 - val_f1_m: 0.9062\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 787us/step - loss: 0.2058 - f1_m: 0.9695 - val_loss: 0.2313 - val_f1_m: 0.9453\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 802us/step - loss: 0.1773 - f1_m: 0.9732 - val_loss: 0.2040 - val_f1_m: 0.9521\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 792us/step - loss: 0.1564 - f1_m: 0.9750 - val_loss: 0.1719 - val_f1_m: 0.9746\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 790us/step - loss: 0.1411 - f1_m: 0.9785 - val_loss: 0.1647 - val_f1_m: 0.9668\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 822us/step - loss: 0.1292 - f1_m: 0.9790 - val_loss: 0.1744 - val_f1_m: 0.9443\n",
      "0.9789999723434448\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.6537 - f1_m: 0.6630 - val_loss: 0.6771 - val_f1_m: 0.5781\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 815us/step - loss: 0.5531 - f1_m: 0.8315 - val_loss: 0.5632 - val_f1_m: 0.7871\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 823us/step - loss: 0.4337 - f1_m: 0.9197 - val_loss: 0.4516 - val_f1_m: 0.8525\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 807us/step - loss: 0.3369 - f1_m: 0.9487 - val_loss: 0.3556 - val_f1_m: 0.8896\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 802us/step - loss: 0.2706 - f1_m: 0.9645 - val_loss: 0.3093 - val_f1_m: 0.8887\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 807us/step - loss: 0.2266 - f1_m: 0.9678 - val_loss: 0.2429 - val_f1_m: 0.9463\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 797us/step - loss: 0.1956 - f1_m: 0.9750 - val_loss: 0.2113 - val_f1_m: 0.9570\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 809us/step - loss: 0.1730 - f1_m: 0.9765 - val_loss: 0.1868 - val_f1_m: 0.9639\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 804us/step - loss: 0.1557 - f1_m: 0.9805 - val_loss: 0.1819 - val_f1_m: 0.9541\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 796us/step - loss: 0.1423 - f1_m: 0.9790 - val_loss: 0.1554 - val_f1_m: 0.9736\n",
      "0.9789999723434448\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.6517 - f1_m: 0.7035 - val_loss: 0.7204 - val_f1_m: 0.3418\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 810us/step - loss: 0.5437 - f1_m: 0.8372 - val_loss: 0.5602 - val_f1_m: 0.7588\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 827us/step - loss: 0.4116 - f1_m: 0.9323 - val_loss: 0.4399 - val_f1_m: 0.8350\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 812us/step - loss: 0.3125 - f1_m: 0.9545 - val_loss: 0.3051 - val_f1_m: 0.9443\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 804us/step - loss: 0.2490 - f1_m: 0.9657 - val_loss: 0.2621 - val_f1_m: 0.9434\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 814us/step - loss: 0.2076 - f1_m: 0.9705 - val_loss: 0.2401 - val_f1_m: 0.9287\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 811us/step - loss: 0.1793 - f1_m: 0.9735 - val_loss: 0.1811 - val_f1_m: 0.9736\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 798us/step - loss: 0.1588 - f1_m: 0.9768 - val_loss: 0.1700 - val_f1_m: 0.9688\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 790us/step - loss: 0.1437 - f1_m: 0.9775 - val_loss: 0.1735 - val_f1_m: 0.9541\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 817us/step - loss: 0.1315 - f1_m: 0.9787 - val_loss: 0.1476 - val_f1_m: 0.9707\n",
      "0.9787499904632568\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.6532 - f1_m: 0.6752 - val_loss: 0.6949 - val_f1_m: 0.4473\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 822us/step - loss: 0.5525 - f1_m: 0.8237 - val_loss: 0.5763 - val_f1_m: 0.7432\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 825us/step - loss: 0.4336 - f1_m: 0.9122 - val_loss: 0.4559 - val_f1_m: 0.8301\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 807us/step - loss: 0.3367 - f1_m: 0.9460 - val_loss: 0.3785 - val_f1_m: 0.8555\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 816us/step - loss: 0.2706 - f1_m: 0.9603 - val_loss: 0.2837 - val_f1_m: 0.9375\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 833us/step - loss: 0.2270 - f1_m: 0.9682 - val_loss: 0.2574 - val_f1_m: 0.9346\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 806us/step - loss: 0.1965 - f1_m: 0.9732 - val_loss: 0.2421 - val_f1_m: 0.9219\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 809us/step - loss: 0.1751 - f1_m: 0.9775 - val_loss: 0.2134 - val_f1_m: 0.9375\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 796us/step - loss: 0.1580 - f1_m: 0.9770 - val_loss: 0.1841 - val_f1_m: 0.9551\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 788us/step - loss: 0.1453 - f1_m: 0.9795 - val_loss: 0.1589 - val_f1_m: 0.9697\n",
      "0.9794999957084656\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.6507 - f1_m: 0.6847 - val_loss: 0.7280 - val_f1_m: 0.3164\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 799us/step - loss: 0.5359 - f1_m: 0.8407 - val_loss: 0.5378 - val_f1_m: 0.7979\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 833us/step - loss: 0.4073 - f1_m: 0.9212 - val_loss: 0.3963 - val_f1_m: 0.9053\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 796us/step - loss: 0.3124 - f1_m: 0.9560 - val_loss: 0.3066 - val_f1_m: 0.9326\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 792us/step - loss: 0.2517 - f1_m: 0.9613 - val_loss: 0.2662 - val_f1_m: 0.9277\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 802us/step - loss: 0.2116 - f1_m: 0.9712 - val_loss: 0.2171 - val_f1_m: 0.9531\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 786us/step - loss: 0.1838 - f1_m: 0.9740 - val_loss: 0.1989 - val_f1_m: 0.9502\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 798us/step - loss: 0.1635 - f1_m: 0.9755 - val_loss: 0.1628 - val_f1_m: 0.9746\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 853us/step - loss: 0.1481 - f1_m: 0.9785 - val_loss: 0.1441 - val_f1_m: 0.9805\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 852us/step - loss: 0.1360 - f1_m: 0.9820 - val_loss: 0.1384 - val_f1_m: 0.9766\n",
      "0.9819999933242798\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.6497 - f1_m: 0.6717 - val_loss: 0.7044 - val_f1_m: 0.3916\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 884us/step - loss: 0.5427 - f1_m: 0.8380 - val_loss: 0.5507 - val_f1_m: 0.8076\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 833us/step - loss: 0.4128 - f1_m: 0.9310 - val_loss: 0.4508 - val_f1_m: 0.8301\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 797us/step - loss: 0.3130 - f1_m: 0.9517 - val_loss: 0.3843 - val_f1_m: 0.8340\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 801us/step - loss: 0.2498 - f1_m: 0.9597 - val_loss: 0.2892 - val_f1_m: 0.9219\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 799us/step - loss: 0.2084 - f1_m: 0.9693 - val_loss: 0.2718 - val_f1_m: 0.8945\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 795us/step - loss: 0.1803 - f1_m: 0.9690 - val_loss: 0.2161 - val_f1_m: 0.9443\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 807us/step - loss: 0.1600 - f1_m: 0.9750 - val_loss: 0.2120 - val_f1_m: 0.9307\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 807us/step - loss: 0.1444 - f1_m: 0.9772 - val_loss: 0.1889 - val_f1_m: 0.9414\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 798us/step - loss: 0.1323 - f1_m: 0.9780 - val_loss: 0.1754 - val_f1_m: 0.9463\n",
      "0.9779999852180481\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.6540 - f1_m: 0.6527 - val_loss: 0.6985 - val_f1_m: 0.4492\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 822us/step - loss: 0.5494 - f1_m: 0.8438 - val_loss: 0.5468 - val_f1_m: 0.8057\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 813us/step - loss: 0.4197 - f1_m: 0.9215 - val_loss: 0.4127 - val_f1_m: 0.8916\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 808us/step - loss: 0.3201 - f1_m: 0.9507 - val_loss: 0.3332 - val_f1_m: 0.9102\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 801us/step - loss: 0.2548 - f1_m: 0.9625 - val_loss: 0.2761 - val_f1_m: 0.9258\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 796us/step - loss: 0.2115 - f1_m: 0.9722 - val_loss: 0.2294 - val_f1_m: 0.9414\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 796us/step - loss: 0.1822 - f1_m: 0.9740 - val_loss: 0.2195 - val_f1_m: 0.9326\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 805us/step - loss: 0.1610 - f1_m: 0.9743 - val_loss: 0.1742 - val_f1_m: 0.9590\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 802us/step - loss: 0.1453 - f1_m: 0.9795 - val_loss: 0.1667 - val_f1_m: 0.9531\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 802us/step - loss: 0.1328 - f1_m: 0.9820 - val_loss: 0.1668 - val_f1_m: 0.9463\n",
      "0.9819999933242798\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.6519 - f1_m: 0.6622 - val_loss: 0.6977 - val_f1_m: 0.4385\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 821us/step - loss: 0.5495 - f1_m: 0.8220 - val_loss: 0.5536 - val_f1_m: 0.7998\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 829us/step - loss: 0.4324 - f1_m: 0.9172 - val_loss: 0.4251 - val_f1_m: 0.8926\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 798us/step - loss: 0.3394 - f1_m: 0.9425 - val_loss: 0.3485 - val_f1_m: 0.9082\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 796us/step - loss: 0.2740 - f1_m: 0.9567 - val_loss: 0.2973 - val_f1_m: 0.9170\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 798us/step - loss: 0.2305 - f1_m: 0.9643 - val_loss: 0.2357 - val_f1_m: 0.9521\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 806us/step - loss: 0.1992 - f1_m: 0.9695 - val_loss: 0.2138 - val_f1_m: 0.9492\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 792us/step - loss: 0.1767 - f1_m: 0.9707 - val_loss: 0.1792 - val_f1_m: 0.9678\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 817us/step - loss: 0.1594 - f1_m: 0.9775 - val_loss: 0.1728 - val_f1_m: 0.9590\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 810us/step - loss: 0.1456 - f1_m: 0.9768 - val_loss: 0.1447 - val_f1_m: 0.9756\n",
      "0.9767500162124634\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 1s 1ms/step - loss: 0.6532 - f1_m: 0.6862 - val_loss: 0.6900 - val_f1_m: 0.4648\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 823us/step - loss: 0.5450 - f1_m: 0.8437 - val_loss: 0.5769 - val_f1_m: 0.7090\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 863us/step - loss: 0.4198 - f1_m: 0.9305 - val_loss: 0.4740 - val_f1_m: 0.7734\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 854us/step - loss: 0.3233 - f1_m: 0.9515 - val_loss: 0.3403 - val_f1_m: 0.9033\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 835us/step - loss: 0.2582 - f1_m: 0.9643 - val_loss: 0.2880 - val_f1_m: 0.9258\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 810us/step - loss: 0.2151 - f1_m: 0.9725 - val_loss: 0.2633 - val_f1_m: 0.9102\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 806us/step - loss: 0.1857 - f1_m: 0.9730 - val_loss: 0.2069 - val_f1_m: 0.9580\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 802us/step - loss: 0.1641 - f1_m: 0.9772 - val_loss: 0.1851 - val_f1_m: 0.9648\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 815us/step - loss: 0.1473 - f1_m: 0.9797 - val_loss: 0.1837 - val_f1_m: 0.9502\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 814us/step - loss: 0.1348 - f1_m: 0.9815 - val_loss: 0.1710 - val_f1_m: 0.9521\n",
      "0.9815000295639038\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.6530 - f1_m: 0.6927 - val_loss: 0.6964 - val_f1_m: 0.4463\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 828us/step - loss: 0.5392 - f1_m: 0.8520 - val_loss: 0.5460 - val_f1_m: 0.7920\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 814us/step - loss: 0.4090 - f1_m: 0.9330 - val_loss: 0.4091 - val_f1_m: 0.8740\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 810us/step - loss: 0.3115 - f1_m: 0.9515 - val_loss: 0.3253 - val_f1_m: 0.9014\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - 0s 811us/step - loss: 0.2484 - f1_m: 0.9663 - val_loss: 0.2715 - val_f1_m: 0.9150\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 802us/step - loss: 0.2071 - f1_m: 0.9690 - val_loss: 0.2352 - val_f1_m: 0.9209\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 795us/step - loss: 0.1788 - f1_m: 0.9747 - val_loss: 0.1766 - val_f1_m: 0.9639\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 795us/step - loss: 0.1590 - f1_m: 0.9780 - val_loss: 0.1887 - val_f1_m: 0.9297\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 790us/step - loss: 0.1435 - f1_m: 0.9803 - val_loss: 0.1516 - val_f1_m: 0.9619\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 792us/step - loss: 0.1313 - f1_m: 0.9827 - val_loss: 0.1667 - val_f1_m: 0.9346\n",
      "0.9827499985694885\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.6515 - f1_m: 0.6810 - val_loss: 0.7076 - val_f1_m: 0.3994\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 817us/step - loss: 0.5387 - f1_m: 0.8585 - val_loss: 0.5704 - val_f1_m: 0.7295\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 817us/step - loss: 0.4076 - f1_m: 0.9308 - val_loss: 0.4233 - val_f1_m: 0.8672\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 802us/step - loss: 0.3085 - f1_m: 0.9540 - val_loss: 0.3294 - val_f1_m: 0.9043\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 801us/step - loss: 0.2451 - f1_m: 0.9655 - val_loss: 0.2718 - val_f1_m: 0.9287\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 809us/step - loss: 0.2040 - f1_m: 0.9737 - val_loss: 0.2251 - val_f1_m: 0.9482\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 793us/step - loss: 0.1760 - f1_m: 0.9740 - val_loss: 0.2120 - val_f1_m: 0.9385\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 798us/step - loss: 0.1557 - f1_m: 0.9780 - val_loss: 0.1942 - val_f1_m: 0.9424\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 790us/step - loss: 0.1401 - f1_m: 0.9808 - val_loss: 0.1886 - val_f1_m: 0.9287\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 796us/step - loss: 0.1277 - f1_m: 0.9827 - val_loss: 0.1850 - val_f1_m: 0.9248\n",
      "0.9827499985694885\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.6530 - f1_m: 0.6650 - val_loss: 0.6970 - val_f1_m: 0.4512\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 799us/step - loss: 0.5477 - f1_m: 0.8400 - val_loss: 0.6135 - val_f1_m: 0.6396\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 810us/step - loss: 0.4292 - f1_m: 0.9057 - val_loss: 0.4370 - val_f1_m: 0.8633\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 788us/step - loss: 0.3332 - f1_m: 0.9472 - val_loss: 0.3357 - val_f1_m: 0.9199\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 789us/step - loss: 0.2681 - f1_m: 0.9632 - val_loss: 0.2983 - val_f1_m: 0.9033\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 802us/step - loss: 0.2243 - f1_m: 0.9690 - val_loss: 0.2508 - val_f1_m: 0.9277\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 798us/step - loss: 0.1934 - f1_m: 0.9743 - val_loss: 0.1868 - val_f1_m: 0.9775\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 797us/step - loss: 0.1715 - f1_m: 0.9783 - val_loss: 0.1941 - val_f1_m: 0.9473\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 793us/step - loss: 0.1544 - f1_m: 0.9797 - val_loss: 0.1665 - val_f1_m: 0.9678\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 804us/step - loss: 0.1409 - f1_m: 0.9825 - val_loss: 0.1689 - val_f1_m: 0.9492\n",
      "0.9825000166893005\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.6514 - f1_m: 0.6555 - val_loss: 0.6898 - val_f1_m: 0.4844\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 798us/step - loss: 0.5479 - f1_m: 0.8300 - val_loss: 0.6013 - val_f1_m: 0.6699\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 822us/step - loss: 0.4306 - f1_m: 0.9090 - val_loss: 0.4317 - val_f1_m: 0.8857\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 796us/step - loss: 0.3371 - f1_m: 0.9407 - val_loss: 0.3617 - val_f1_m: 0.8916\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 804us/step - loss: 0.2730 - f1_m: 0.9588 - val_loss: 0.2783 - val_f1_m: 0.9502\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 803us/step - loss: 0.2295 - f1_m: 0.9707 - val_loss: 0.2496 - val_f1_m: 0.9404\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 799us/step - loss: 0.1989 - f1_m: 0.9710 - val_loss: 0.2349 - val_f1_m: 0.9307\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 788us/step - loss: 0.1763 - f1_m: 0.9747 - val_loss: 0.1875 - val_f1_m: 0.9707\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 802us/step - loss: 0.1589 - f1_m: 0.9778 - val_loss: 0.1823 - val_f1_m: 0.9580\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 801us/step - loss: 0.1451 - f1_m: 0.9785 - val_loss: 0.1731 - val_f1_m: 0.9570\n",
      "0.9785000085830688\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.6521 - f1_m: 0.6995 - val_loss: 0.7258 - val_f1_m: 0.2881\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 815us/step - loss: 0.5501 - f1_m: 0.8167 - val_loss: 0.5853 - val_f1_m: 0.7227\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 807us/step - loss: 0.4311 - f1_m: 0.9230 - val_loss: 0.4863 - val_f1_m: 0.7949\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 793us/step - loss: 0.3351 - f1_m: 0.9455 - val_loss: 0.3775 - val_f1_m: 0.8691\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 784us/step - loss: 0.2691 - f1_m: 0.9588 - val_loss: 0.3034 - val_f1_m: 0.9248\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 811us/step - loss: 0.2248 - f1_m: 0.9665 - val_loss: 0.2479 - val_f1_m: 0.9453\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 817us/step - loss: 0.1936 - f1_m: 0.9743 - val_loss: 0.2143 - val_f1_m: 0.9580\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 792us/step - loss: 0.1708 - f1_m: 0.9755 - val_loss: 0.2045 - val_f1_m: 0.9434\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 791us/step - loss: 0.1538 - f1_m: 0.9755 - val_loss: 0.1633 - val_f1_m: 0.9766\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 803us/step - loss: 0.1405 - f1_m: 0.9822 - val_loss: 0.1484 - val_f1_m: 0.9775\n",
      "0.9822499752044678\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.6533 - f1_m: 0.6962 - val_loss: 0.7100 - val_f1_m: 0.3682\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 808us/step - loss: 0.5529 - f1_m: 0.8117 - val_loss: 0.5768 - val_f1_m: 0.7529\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 806us/step - loss: 0.4325 - f1_m: 0.9178 - val_loss: 0.4734 - val_f1_m: 0.8145\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 795us/step - loss: 0.3342 - f1_m: 0.9485 - val_loss: 0.3885 - val_f1_m: 0.8584\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 798us/step - loss: 0.2678 - f1_m: 0.9588 - val_loss: 0.3088 - val_f1_m: 0.9062\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 804us/step - loss: 0.2237 - f1_m: 0.9682 - val_loss: 0.2511 - val_f1_m: 0.9414\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 802us/step - loss: 0.1928 - f1_m: 0.9705 - val_loss: 0.2152 - val_f1_m: 0.9551\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 810us/step - loss: 0.1700 - f1_m: 0.9765 - val_loss: 0.2049 - val_f1_m: 0.9424\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 799us/step - loss: 0.1529 - f1_m: 0.9787 - val_loss: 0.2037 - val_f1_m: 0.9355\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 791us/step - loss: 0.1395 - f1_m: 0.9818 - val_loss: 0.1845 - val_f1_m: 0.9385\n",
      "0.9817500114440918\n",
      "if any\n"
     ]
    }
   ],
   "source": [
    "#get details of models in the epsilon integrally private range\n",
    "epsilon = 0.00001\n",
    "#A function to return integrally private model weights with the number of recurrence\n",
    "#def get_IP_models(data, epsilon=0.01, target=2):\n",
    "\n",
    "\"\"\"\n",
    "df1=Positive.sample(positiveN)\n",
    "Positive.drop(df1.index, inplace=True)\n",
    "df2=Negative.sample(negativeN)\n",
    "Negative.drop(df2.index, inplace=True)\n",
    "test_data=df1.append(df2, ignore_index=True)\n",
    "test_data=test_data.sample(frac = 1) #This is to shuffel the training and testing data\n",
    "test_data=test_data.sample(frac = 1)\n",
    "test_data=test_data.sample(frac = 1)\n",
    "X_test=test_data.drop(columns=[target_variable])\n",
    "y_test=to_categorical(test_data[target_variable])\n",
    "\"\"\"\n",
    "\n",
    "# adding dense layer\n",
    "Models=[]\n",
    "val_acc=[]\n",
    "train_acc=[]\n",
    "test_acc=[]\n",
    "val_loss=[]\n",
    "train_loss=[]\n",
    "ind=0\n",
    "add_weights=[]\n",
    "while ind<len(samples):\n",
    "  \"\"\"\"\n",
    "  print(positiveN, negativeN)\n",
    "  df1=Positive.sample(min(positiveN, len(Positive)))\n",
    "  Positive.drop(df1.index, inplace=True)\n",
    "  df2=Negative.sample(min(negativeN, len(Negative)))\n",
    "  Negative.drop(df2.index, inplace=True)\n",
    "  train_data=df1.append(df2, ignore_index=True)\n",
    "  train_data=train_data.sample(frac = 1) #shuffel test data 3 times\n",
    "  train_data=train_data.sample(frac = 1) #shuffel test data 3 times\n",
    "  train_data=train_data.sample(frac = 1) #shuffel test data 3 times\n",
    "\n",
    "  #all models have different initialization\n",
    "  # define the sequential model\n",
    "  \"\"\"\n",
    "  train_data=samples[ind]\n",
    "  ind=ind+1\n",
    "  \"\"\"initial_model = keras.Sequential()\n",
    "\n",
    "    # adding dense layer\n",
    "  initial_model.add(Dense(5, input_dim=X_test.shape[1], kernel_initializer='normal', activation='relu'))\n",
    "  initial_model.add(Dense(10, activation='relu'))\n",
    "  initial_model.add(Dense(5, activation='relu'))\n",
    "\n",
    "    # adding dense layer with softmax activation/output layer\n",
    "  initial_model.add(Dense(2, activation='softmax'))\n",
    "  #initial_model.summary()\"\"\"\n",
    "  ann_model=get_initial_model(data.shape[1]-1, 2) #same intial weights\n",
    "  ann_model.set_weights(initial_model.get_weights())\n",
    "  X_train=train_data.drop(columns=[target_variable])\n",
    "  #train_data[target_variable]=train_data[target_variable]-1 #only for skin_nonskin dataset\n",
    "  y_train=to_categorical(train_data[target_variable])\n",
    "  #print(y_train)\n",
    "  ann_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[f1_m]) # metrics=['accuracy']\n",
    "  history = ann_model.fit(X_train, y_train, epochs=10, validation_split=0.2, verbose=1)\n",
    "  print(history.history['f1_m'][-1])\n",
    "  #ann_model.set_weights(update_weights(ann_model.get_weights()))\n",
    "  #pred_test=ann_model.predict(X_test)\n",
    "  present=False\n",
    "  for i in range(len(Models)):\n",
    "    if (check_models(Models[i][0], ann_model.get_weights())):\n",
    "      print(\"if any\")\n",
    "      Models[i][1]=Models[i][1]+1\n",
    "      add_weights[i].append(ann_model.get_weights())\n",
    "      val_acc[i].append(history.history['val_f1_m'])\n",
    "      train_acc[i].append(history.history['f1_m'])\n",
    "      #test_acc[i].append(f1_m(y_test, pred_test).numpy())\n",
    "      val_loss[i].append(history.history['val_loss'])\n",
    "      train_loss[i].append(history.history['loss'])\n",
    "      present=True\n",
    "      break;\n",
    "  if present==False:\n",
    "    add_weights.append([ann_model.get_weights()])\n",
    "    Models.append([ann_model.get_weights(), 1])\n",
    "    val_acc.append([history.history['val_f1_m']])\n",
    "    train_acc.append([history.history['f1_m']])\n",
    "    #test_acc.append([f1_m(y_test, pred_test).numpy()])\n",
    "    val_loss.append([history.history['val_loss']])\n",
    "    train_loss.append([history.history['loss']])\n",
    "add_weights, Models, val_acc, train_acc, train_loss, val_acc, val_loss = add_weights, Models, val_acc, train_acc, train_loss, val_acc, val_loss\n",
    "\n",
    "#add_weights, Models, val_acc, train_acc, train_loss, val_acc, val_loss = get_IP_models(data_init, epsilon=0.001, target=2)\n",
    "#Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa33efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Models)\n",
    "\n",
    "#here use only top 5-10 integrally private models.\n",
    "#add_weights=add_weights[top_5]\n",
    "A=np.argsort(np.array(Models).T[1])[::-1][:5]\n",
    "print(np.array(Models).T[1])\n",
    "print(A)\n",
    "recommended_models=[]\n",
    "for i in range(len(A)):\n",
    "    recommended_models.append(add_weights[A[i]])\n",
    "#add_weights=add_weights[A]\n",
    "\n",
    "\n",
    "# Now trying to generate Streaming settings for the dataset\n",
    "# lets find the outputs from all the \n",
    "mean_model_weights, mean_model_acc, mean_model_loss, mean_model_test_acc, mean_model_test_loss = epsilon_mean_recommendation(recommended_models, data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359dc37e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 409us/step\n",
      "313/313 [==============================] - 0s 392us/step\n",
      "313/313 [==============================] - 0s 395us/step\n",
      "10000\n",
      "One lap done.\n",
      "313/313 [==============================] - 0s 401us/step\n",
      "313/313 [==============================] - 0s 398us/step\n",
      "313/313 [==============================] - 0s 415us/step\n",
      "10000\n",
      "One lap done.\n",
      "313/313 [==============================] - 0s 405us/step\n",
      "313/313 [==============================] - 0s 395us/step\n",
      "313/313 [==============================] - 0s 401us/step\n",
      "10000\n",
      "drift has been detecte models must be retrained\n",
      "finished\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5703 - f1_m: 0.6987 - val_loss: 0.2758 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 803us/step - loss: 0.4027 - f1_m: 0.7752 - val_loss: 0.2031 - val_f1_m: 0.9990\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 796us/step - loss: 0.3052 - f1_m: 0.9057 - val_loss: 0.1572 - val_f1_m: 0.9912\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 785us/step - loss: 0.2404 - f1_m: 0.9515 - val_loss: 0.1294 - val_f1_m: 0.9912\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 771us/step - loss: 0.1957 - f1_m: 0.9688 - val_loss: 0.1152 - val_f1_m: 0.9893\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 769us/step - loss: 0.1639 - f1_m: 0.9812 - val_loss: 0.1016 - val_f1_m: 0.9902\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 784us/step - loss: 0.1408 - f1_m: 0.9883 - val_loss: 0.0791 - val_f1_m: 0.9971\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 778us/step - loss: 0.1239 - f1_m: 0.9910 - val_loss: 0.0774 - val_f1_m: 0.9951\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 783us/step - loss: 0.1107 - f1_m: 0.9915 - val_loss: 0.0652 - val_f1_m: 0.9971\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 791us/step - loss: 0.1000 - f1_m: 0.9918 - val_loss: 0.0584 - val_f1_m: 0.9971\n",
      "0.9917500019073486\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5689 - f1_m: 0.6982 - val_loss: 0.2754 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 801us/step - loss: 0.4019 - f1_m: 0.7755 - val_loss: 0.2126 - val_f1_m: 0.9971\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 781us/step - loss: 0.3032 - f1_m: 0.9097 - val_loss: 0.1587 - val_f1_m: 0.9941\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 779us/step - loss: 0.2389 - f1_m: 0.9492 - val_loss: 0.1349 - val_f1_m: 0.9854\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 772us/step - loss: 0.1945 - f1_m: 0.9697 - val_loss: 0.1197 - val_f1_m: 0.9814\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 784us/step - loss: 0.1631 - f1_m: 0.9800 - val_loss: 0.0948 - val_f1_m: 0.9922\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 783us/step - loss: 0.1405 - f1_m: 0.9840 - val_loss: 0.0795 - val_f1_m: 0.9971\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 777us/step - loss: 0.1232 - f1_m: 0.9883 - val_loss: 0.0829 - val_f1_m: 0.9863\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 786us/step - loss: 0.1101 - f1_m: 0.9915 - val_loss: 0.0763 - val_f1_m: 0.9863\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 780us/step - loss: 0.0996 - f1_m: 0.9920 - val_loss: 0.0638 - val_f1_m: 0.9951\n",
      "0.9919999837875366\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5711 - f1_m: 0.6975 - val_loss: 0.2751 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 844us/step - loss: 0.4004 - f1_m: 0.7737 - val_loss: 0.2220 - val_f1_m: 0.9893\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 795us/step - loss: 0.3021 - f1_m: 0.9175 - val_loss: 0.1618 - val_f1_m: 0.9863\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 789us/step - loss: 0.2373 - f1_m: 0.9488 - val_loss: 0.1419 - val_f1_m: 0.9824\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 796us/step - loss: 0.1936 - f1_m: 0.9672 - val_loss: 0.1195 - val_f1_m: 0.9834\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 790us/step - loss: 0.1621 - f1_m: 0.9803 - val_loss: 0.0993 - val_f1_m: 0.9893\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 802us/step - loss: 0.1402 - f1_m: 0.9870 - val_loss: 0.0906 - val_f1_m: 0.9893\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 824us/step - loss: 0.1231 - f1_m: 0.9908 - val_loss: 0.0782 - val_f1_m: 0.9922\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 787us/step - loss: 0.1101 - f1_m: 0.9910 - val_loss: 0.0758 - val_f1_m: 0.9902\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 776us/step - loss: 0.0995 - f1_m: 0.9923 - val_loss: 0.0660 - val_f1_m: 0.9932\n",
      "0.9922500252723694\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5764 - f1_m: 0.6967 - val_loss: 0.2783 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 798us/step - loss: 0.4141 - f1_m: 0.7607 - val_loss: 0.1876 - val_f1_m: 0.9990\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 801us/step - loss: 0.3156 - f1_m: 0.8902 - val_loss: 0.1567 - val_f1_m: 0.9893\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 787us/step - loss: 0.2498 - f1_m: 0.9452 - val_loss: 0.1187 - val_f1_m: 0.9902\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 778us/step - loss: 0.2037 - f1_m: 0.9665 - val_loss: 0.1037 - val_f1_m: 0.9883\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 778us/step - loss: 0.1709 - f1_m: 0.9803 - val_loss: 0.0818 - val_f1_m: 0.9961\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 781us/step - loss: 0.1472 - f1_m: 0.9858 - val_loss: 0.0727 - val_f1_m: 0.9941\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 784us/step - loss: 0.1289 - f1_m: 0.9895 - val_loss: 0.0718 - val_f1_m: 0.9883\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 788us/step - loss: 0.1157 - f1_m: 0.9915 - val_loss: 0.0531 - val_f1_m: 0.9980\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 774us/step - loss: 0.1046 - f1_m: 0.9923 - val_loss: 0.0543 - val_f1_m: 0.9951\n",
      "0.9922500252723694\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5729 - f1_m: 0.6972 - val_loss: 0.2873 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 791us/step - loss: 0.4004 - f1_m: 0.7782 - val_loss: 0.1990 - val_f1_m: 0.9990\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 790us/step - loss: 0.3013 - f1_m: 0.9125 - val_loss: 0.1541 - val_f1_m: 0.9922\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 774us/step - loss: 0.2366 - f1_m: 0.9512 - val_loss: 0.1408 - val_f1_m: 0.9854\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 784us/step - loss: 0.1924 - f1_m: 0.9690 - val_loss: 0.1203 - val_f1_m: 0.9893\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 780us/step - loss: 0.1611 - f1_m: 0.9795 - val_loss: 0.0861 - val_f1_m: 0.9941\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 780us/step - loss: 0.1392 - f1_m: 0.9855 - val_loss: 0.0806 - val_f1_m: 0.9941\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 778us/step - loss: 0.1222 - f1_m: 0.9905 - val_loss: 0.0810 - val_f1_m: 0.9922\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 774us/step - loss: 0.1092 - f1_m: 0.9912 - val_loss: 0.0757 - val_f1_m: 0.9922\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 771us/step - loss: 0.0985 - f1_m: 0.9930 - val_loss: 0.0727 - val_f1_m: 0.9893\n",
      "0.9929999709129333\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5729 - f1_m: 0.6957 - val_loss: 0.2789 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 793us/step - loss: 0.4023 - f1_m: 0.7737 - val_loss: 0.1932 - val_f1_m: 1.0000\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 792us/step - loss: 0.3035 - f1_m: 0.9060 - val_loss: 0.1552 - val_f1_m: 0.9951\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 776us/step - loss: 0.2386 - f1_m: 0.9507 - val_loss: 0.1272 - val_f1_m: 0.9922\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 773us/step - loss: 0.1940 - f1_m: 0.9725 - val_loss: 0.1115 - val_f1_m: 0.9893\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 784us/step - loss: 0.1625 - f1_m: 0.9800 - val_loss: 0.0976 - val_f1_m: 0.9893\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 4ms/step - loss: 0.1397 - f1_m: 0.9862 - val_loss: 0.0789 - val_f1_m: 0.9980\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.1227 - f1_m: 0.9877 - val_loss: 0.0737 - val_f1_m: 0.9941\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.1095 - f1_m: 0.9887 - val_loss: 0.0736 - val_f1_m: 0.9834\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.0993 - f1_m: 0.9895 - val_loss: 0.0646 - val_f1_m: 0.9883\n",
      "0.9894999861717224\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.5729 - f1_m: 0.6990 - val_loss: 0.2852 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 824us/step - loss: 0.4039 - f1_m: 0.7715 - val_loss: 0.2072 - val_f1_m: 0.9990\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 854us/step - loss: 0.3046 - f1_m: 0.9102 - val_loss: 0.1544 - val_f1_m: 0.9980\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 826us/step - loss: 0.2389 - f1_m: 0.9495 - val_loss: 0.1280 - val_f1_m: 0.9971\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 848us/step - loss: 0.1932 - f1_m: 0.9685 - val_loss: 0.1256 - val_f1_m: 0.9893\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 801us/step - loss: 0.1607 - f1_m: 0.9812 - val_loss: 0.0895 - val_f1_m: 0.9980\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 785us/step - loss: 0.1377 - f1_m: 0.9860 - val_loss: 0.0939 - val_f1_m: 0.9912\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 787us/step - loss: 0.1203 - f1_m: 0.9900 - val_loss: 0.0827 - val_f1_m: 0.9912\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 787us/step - loss: 0.1072 - f1_m: 0.9920 - val_loss: 0.0648 - val_f1_m: 0.9980\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 777us/step - loss: 0.0971 - f1_m: 0.9905 - val_loss: 0.0683 - val_f1_m: 0.9932\n",
      "0.9904999732971191\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5731 - f1_m: 0.6952 - val_loss: 0.2793 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 796us/step - loss: 0.4043 - f1_m: 0.7760 - val_loss: 0.1915 - val_f1_m: 0.9990\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 787us/step - loss: 0.3048 - f1_m: 0.9067 - val_loss: 0.1506 - val_f1_m: 0.9961\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 782us/step - loss: 0.2399 - f1_m: 0.9482 - val_loss: 0.1217 - val_f1_m: 0.9951\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 774us/step - loss: 0.1950 - f1_m: 0.9685 - val_loss: 0.0975 - val_f1_m: 0.9971\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 777us/step - loss: 0.1635 - f1_m: 0.9755 - val_loss: 0.0953 - val_f1_m: 0.9912\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 778us/step - loss: 0.1405 - f1_m: 0.9850 - val_loss: 0.0749 - val_f1_m: 0.9961\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 783us/step - loss: 0.1232 - f1_m: 0.9883 - val_loss: 0.0664 - val_f1_m: 0.9961\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 771us/step - loss: 0.1098 - f1_m: 0.9893 - val_loss: 0.0652 - val_f1_m: 0.9912\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 768us/step - loss: 0.0994 - f1_m: 0.9885 - val_loss: 0.0576 - val_f1_m: 0.9922\n",
      "0.9884999990463257\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5692 - f1_m: 0.6982 - val_loss: 0.2780 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 799us/step - loss: 0.4022 - f1_m: 0.7712 - val_loss: 0.2055 - val_f1_m: 1.0000\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 797us/step - loss: 0.3044 - f1_m: 0.9050 - val_loss: 0.1641 - val_f1_m: 0.9912\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 774us/step - loss: 0.2406 - f1_m: 0.9498 - val_loss: 0.1340 - val_f1_m: 0.9893\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 795us/step - loss: 0.1960 - f1_m: 0.9695 - val_loss: 0.1098 - val_f1_m: 0.9922\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 777us/step - loss: 0.1647 - f1_m: 0.9787 - val_loss: 0.0881 - val_f1_m: 1.0000\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 774us/step - loss: 0.1422 - f1_m: 0.9845 - val_loss: 0.0889 - val_f1_m: 0.9922\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 795us/step - loss: 0.1255 - f1_m: 0.9887 - val_loss: 0.0799 - val_f1_m: 0.9912\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 774us/step - loss: 0.1126 - f1_m: 0.9883 - val_loss: 0.0613 - val_f1_m: 1.0000\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 773us/step - loss: 0.1028 - f1_m: 0.9880 - val_loss: 0.0593 - val_f1_m: 0.9990\n",
      "0.9879999756813049\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5714 - f1_m: 0.6982 - val_loss: 0.2808 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 785us/step - loss: 0.4041 - f1_m: 0.7700 - val_loss: 0.1794 - val_f1_m: 0.9990\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 786us/step - loss: 0.3046 - f1_m: 0.9002 - val_loss: 0.1555 - val_f1_m: 0.9814\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 777us/step - loss: 0.2399 - f1_m: 0.9477 - val_loss: 0.1268 - val_f1_m: 0.9834\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 786us/step - loss: 0.1960 - f1_m: 0.9672 - val_loss: 0.1162 - val_f1_m: 0.9775\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 819us/step - loss: 0.1649 - f1_m: 0.9758 - val_loss: 0.0982 - val_f1_m: 0.9834\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 779us/step - loss: 0.1429 - f1_m: 0.9833 - val_loss: 0.0799 - val_f1_m: 0.9912\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 798us/step - loss: 0.1256 - f1_m: 0.9890 - val_loss: 0.0743 - val_f1_m: 0.9902\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 782us/step - loss: 0.1125 - f1_m: 0.9925 - val_loss: 0.0734 - val_f1_m: 0.9863\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 767us/step - loss: 0.1024 - f1_m: 0.9910 - val_loss: 0.0670 - val_f1_m: 0.9863\n",
      "0.9909999966621399\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5731 - f1_m: 0.6960 - val_loss: 0.2859 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 791us/step - loss: 0.4000 - f1_m: 0.7765 - val_loss: 0.2173 - val_f1_m: 0.9980\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 795us/step - loss: 0.2975 - f1_m: 0.9145 - val_loss: 0.1639 - val_f1_m: 0.9932\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 773us/step - loss: 0.2322 - f1_m: 0.9553 - val_loss: 0.1411 - val_f1_m: 0.9844\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 768us/step - loss: 0.1885 - f1_m: 0.9722 - val_loss: 0.1252 - val_f1_m: 0.9814\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 782us/step - loss: 0.1580 - f1_m: 0.9827 - val_loss: 0.1006 - val_f1_m: 0.9893\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 769us/step - loss: 0.1360 - f1_m: 0.9847 - val_loss: 0.0872 - val_f1_m: 0.9922\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 778us/step - loss: 0.1195 - f1_m: 0.9875 - val_loss: 0.0784 - val_f1_m: 0.9932\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 776us/step - loss: 0.1070 - f1_m: 0.9890 - val_loss: 0.0641 - val_f1_m: 0.9961\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 784us/step - loss: 0.0969 - f1_m: 0.9900 - val_loss: 0.0679 - val_f1_m: 0.9893\n",
      "0.9900000095367432\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.5731 - f1_m: 0.6967 - val_loss: 0.2798 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 14s 114ms/step - loss: 0.4021 - f1_m: 0.7672 - val_loss: 0.1999 - val_f1_m: 0.9990\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 839us/step - loss: 0.3023 - f1_m: 0.9110 - val_loss: 0.1469 - val_f1_m: 0.9971\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 822us/step - loss: 0.2373 - f1_m: 0.9545 - val_loss: 0.1150 - val_f1_m: 0.9971\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 872us/step - loss: 0.1922 - f1_m: 0.9693 - val_loss: 0.1069 - val_f1_m: 0.9951\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 849us/step - loss: 0.1610 - f1_m: 0.9815 - val_loss: 0.0841 - val_f1_m: 0.9980\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 826us/step - loss: 0.1381 - f1_m: 0.9877 - val_loss: 0.0864 - val_f1_m: 0.9922\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 883us/step - loss: 0.1214 - f1_m: 0.9912 - val_loss: 0.0709 - val_f1_m: 0.9961\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 799us/step - loss: 0.1081 - f1_m: 0.9930 - val_loss: 0.0603 - val_f1_m: 0.9971\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 793us/step - loss: 0.0980 - f1_m: 0.9910 - val_loss: 0.0553 - val_f1_m: 0.9951\n",
      "0.9909999966621399\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5728 - f1_m: 0.6972 - val_loss: 0.2736 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 792us/step - loss: 0.4042 - f1_m: 0.7747 - val_loss: 0.1865 - val_f1_m: 1.0000\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 789us/step - loss: 0.3037 - f1_m: 0.9045 - val_loss: 0.1563 - val_f1_m: 0.9922\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 770us/step - loss: 0.2382 - f1_m: 0.9482 - val_loss: 0.1156 - val_f1_m: 0.9922\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 780us/step - loss: 0.1939 - f1_m: 0.9655 - val_loss: 0.1060 - val_f1_m: 0.9902\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 781us/step - loss: 0.1626 - f1_m: 0.9755 - val_loss: 0.0788 - val_f1_m: 0.9941\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 783us/step - loss: 0.1403 - f1_m: 0.9820 - val_loss: 0.0727 - val_f1_m: 0.9941\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 773us/step - loss: 0.1235 - f1_m: 0.9860 - val_loss: 0.0672 - val_f1_m: 0.9922\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 778us/step - loss: 0.1106 - f1_m: 0.9887 - val_loss: 0.0543 - val_f1_m: 0.9941\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 770us/step - loss: 0.1001 - f1_m: 0.9905 - val_loss: 0.0593 - val_f1_m: 0.9912\n",
      "0.9904999732971191\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5697 - f1_m: 0.6990 - val_loss: 0.2811 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 786us/step - loss: 0.3997 - f1_m: 0.7772 - val_loss: 0.1949 - val_f1_m: 1.0000\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 776us/step - loss: 0.3000 - f1_m: 0.9090 - val_loss: 0.1513 - val_f1_m: 0.9971\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 778us/step - loss: 0.2355 - f1_m: 0.9500 - val_loss: 0.1318 - val_f1_m: 0.9922\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 787us/step - loss: 0.1919 - f1_m: 0.9672 - val_loss: 0.1119 - val_f1_m: 0.9902\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 781us/step - loss: 0.1612 - f1_m: 0.9768 - val_loss: 0.1064 - val_f1_m: 0.9863\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 780us/step - loss: 0.1393 - f1_m: 0.9850 - val_loss: 0.0841 - val_f1_m: 0.9932\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 788us/step - loss: 0.1228 - f1_m: 0.9872 - val_loss: 0.0806 - val_f1_m: 0.9912\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 778us/step - loss: 0.1101 - f1_m: 0.9893 - val_loss: 0.0695 - val_f1_m: 0.9941\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 774us/step - loss: 0.1004 - f1_m: 0.9905 - val_loss: 0.0582 - val_f1_m: 0.9961\n",
      "0.9904999732971191\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5735 - f1_m: 0.6960 - val_loss: 0.2881 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 815us/step - loss: 0.4036 - f1_m: 0.7727 - val_loss: 0.2186 - val_f1_m: 0.9971\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 791us/step - loss: 0.3016 - f1_m: 0.9057 - val_loss: 0.1797 - val_f1_m: 0.9844\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 766us/step - loss: 0.2361 - f1_m: 0.9517 - val_loss: 0.1547 - val_f1_m: 0.9766\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 778us/step - loss: 0.1920 - f1_m: 0.9685 - val_loss: 0.1395 - val_f1_m: 0.9707\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 767us/step - loss: 0.1609 - f1_m: 0.9785 - val_loss: 0.1083 - val_f1_m: 0.9873\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 788us/step - loss: 0.1390 - f1_m: 0.9827 - val_loss: 0.0942 - val_f1_m: 0.9873\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 767us/step - loss: 0.1220 - f1_m: 0.9860 - val_loss: 0.0816 - val_f1_m: 0.9912\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 769us/step - loss: 0.1091 - f1_m: 0.9900 - val_loss: 0.0805 - val_f1_m: 0.9863\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 774us/step - loss: 0.0989 - f1_m: 0.9915 - val_loss: 0.0715 - val_f1_m: 0.9883\n",
      "0.9915000200271606\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5746 - f1_m: 0.6952 - val_loss: 0.2865 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 785us/step - loss: 0.4059 - f1_m: 0.7707 - val_loss: 0.2033 - val_f1_m: 0.9990\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 807us/step - loss: 0.3084 - f1_m: 0.8997 - val_loss: 0.1646 - val_f1_m: 0.9863\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 776us/step - loss: 0.2439 - f1_m: 0.9467 - val_loss: 0.1261 - val_f1_m: 0.9893\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 783us/step - loss: 0.1990 - f1_m: 0.9693 - val_loss: 0.1049 - val_f1_m: 0.9922\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 776us/step - loss: 0.1670 - f1_m: 0.9850 - val_loss: 0.0862 - val_f1_m: 0.9971\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 776us/step - loss: 0.1438 - f1_m: 0.9860 - val_loss: 0.0748 - val_f1_m: 0.9980\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 767us/step - loss: 0.1261 - f1_m: 0.9898 - val_loss: 0.0704 - val_f1_m: 0.9961\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 768us/step - loss: 0.1123 - f1_m: 0.9912 - val_loss: 0.0626 - val_f1_m: 0.9971\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 761us/step - loss: 0.1013 - f1_m: 0.9910 - val_loss: 0.0582 - val_f1_m: 0.9961\n",
      "0.9909999966621399\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5712 - f1_m: 0.6982 - val_loss: 0.2834 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 783us/step - loss: 0.4074 - f1_m: 0.7662 - val_loss: 0.2055 - val_f1_m: 0.9980\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 778us/step - loss: 0.3088 - f1_m: 0.9032 - val_loss: 0.1665 - val_f1_m: 0.9883\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 770us/step - loss: 0.2435 - f1_m: 0.9500 - val_loss: 0.1287 - val_f1_m: 0.9912\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 778us/step - loss: 0.1982 - f1_m: 0.9665 - val_loss: 0.1144 - val_f1_m: 0.9902\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 773us/step - loss: 0.1659 - f1_m: 0.9783 - val_loss: 0.1040 - val_f1_m: 0.9873\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 757us/step - loss: 0.1424 - f1_m: 0.9872 - val_loss: 0.0812 - val_f1_m: 0.9941\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 770us/step - loss: 0.1250 - f1_m: 0.9883 - val_loss: 0.0802 - val_f1_m: 0.9902\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 767us/step - loss: 0.1117 - f1_m: 0.9883 - val_loss: 0.0705 - val_f1_m: 0.9902\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 814us/step - loss: 0.1008 - f1_m: 0.9915 - val_loss: 0.0719 - val_f1_m: 0.9863\n",
      "0.9915000200271606\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 1s 2ms/step - loss: 0.5716 - f1_m: 0.6980 - val_loss: 0.2873 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 901us/step - loss: 0.4027 - f1_m: 0.7705 - val_loss: 0.2058 - val_f1_m: 0.9980\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 897us/step - loss: 0.3041 - f1_m: 0.9057 - val_loss: 0.1651 - val_f1_m: 0.9883\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - 0s 789us/step - loss: 0.2395 - f1_m: 0.9485 - val_loss: 0.1404 - val_f1_m: 0.9834\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 802us/step - loss: 0.1951 - f1_m: 0.9685 - val_loss: 0.1251 - val_f1_m: 0.9805\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 794us/step - loss: 0.1647 - f1_m: 0.9780 - val_loss: 0.1044 - val_f1_m: 0.9883\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 793us/step - loss: 0.1421 - f1_m: 0.9862 - val_loss: 0.0895 - val_f1_m: 0.9932\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 855us/step - loss: 0.1252 - f1_m: 0.9890 - val_loss: 0.0863 - val_f1_m: 0.9902\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 853us/step - loss: 0.1124 - f1_m: 0.9908 - val_loss: 0.0815 - val_f1_m: 0.9873\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 838us/step - loss: 0.1023 - f1_m: 0.9883 - val_loss: 0.0688 - val_f1_m: 0.9941\n",
      "0.9882500171661377\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5702 - f1_m: 0.6967 - val_loss: 0.2836 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 797us/step - loss: 0.4000 - f1_m: 0.7802 - val_loss: 0.2125 - val_f1_m: 0.9990\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 792us/step - loss: 0.3031 - f1_m: 0.9067 - val_loss: 0.1817 - val_f1_m: 0.9785\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 779us/step - loss: 0.2410 - f1_m: 0.9485 - val_loss: 0.1293 - val_f1_m: 0.9961\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 770us/step - loss: 0.1970 - f1_m: 0.9663 - val_loss: 0.1182 - val_f1_m: 0.9922\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 801us/step - loss: 0.1662 - f1_m: 0.9728 - val_loss: 0.1048 - val_f1_m: 0.9912\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 781us/step - loss: 0.1435 - f1_m: 0.9810 - val_loss: 0.0939 - val_f1_m: 0.9922\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 786us/step - loss: 0.1265 - f1_m: 0.9862 - val_loss: 0.0741 - val_f1_m: 0.9980\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 781us/step - loss: 0.1133 - f1_m: 0.9875 - val_loss: 0.0701 - val_f1_m: 0.9971\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 780us/step - loss: 0.1030 - f1_m: 0.9898 - val_loss: 0.0663 - val_f1_m: 0.9961\n",
      "0.9897500276565552\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5707 - f1_m: 0.6967 - val_loss: 0.2716 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 782us/step - loss: 0.4032 - f1_m: 0.7647 - val_loss: 0.1963 - val_f1_m: 0.9990\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 778us/step - loss: 0.3033 - f1_m: 0.9095 - val_loss: 0.1473 - val_f1_m: 0.9951\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 788us/step - loss: 0.2368 - f1_m: 0.9520 - val_loss: 0.1327 - val_f1_m: 0.9863\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 771us/step - loss: 0.1916 - f1_m: 0.9747 - val_loss: 0.0954 - val_f1_m: 0.9971\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 783us/step - loss: 0.1598 - f1_m: 0.9835 - val_loss: 0.0883 - val_f1_m: 0.9941\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 782us/step - loss: 0.1367 - f1_m: 0.9880 - val_loss: 0.0787 - val_f1_m: 0.9941\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 777us/step - loss: 0.1195 - f1_m: 0.9923 - val_loss: 0.0748 - val_f1_m: 0.9912\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 784us/step - loss: 0.1064 - f1_m: 0.9935 - val_loss: 0.0665 - val_f1_m: 0.9922\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 781us/step - loss: 0.0959 - f1_m: 0.9935 - val_loss: 0.0570 - val_f1_m: 0.9951\n",
      "0.9934999942779541\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5713 - f1_m: 0.6942 - val_loss: 0.2859 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 778us/step - loss: 0.4002 - f1_m: 0.7760 - val_loss: 0.2162 - val_f1_m: 0.9980\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 802us/step - loss: 0.3014 - f1_m: 0.9115 - val_loss: 0.1609 - val_f1_m: 0.9883\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 785us/step - loss: 0.2385 - f1_m: 0.9502 - val_loss: 0.1273 - val_f1_m: 0.9883\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 782us/step - loss: 0.1957 - f1_m: 0.9680 - val_loss: 0.1116 - val_f1_m: 0.9873\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 784us/step - loss: 0.1657 - f1_m: 0.9760 - val_loss: 0.1046 - val_f1_m: 0.9854\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 784us/step - loss: 0.1440 - f1_m: 0.9818 - val_loss: 0.0979 - val_f1_m: 0.9824\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 784us/step - loss: 0.1271 - f1_m: 0.9872 - val_loss: 0.0752 - val_f1_m: 0.9941\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 783us/step - loss: 0.1141 - f1_m: 0.9885 - val_loss: 0.0639 - val_f1_m: 0.9980\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 771us/step - loss: 0.1038 - f1_m: 0.9910 - val_loss: 0.0616 - val_f1_m: 0.9941\n",
      "0.9909999966621399\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.5744 - f1_m: 0.6987 - val_loss: 0.2819 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 799us/step - loss: 0.4012 - f1_m: 0.7747 - val_loss: 0.2114 - val_f1_m: 0.9951\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 796us/step - loss: 0.3014 - f1_m: 0.9150 - val_loss: 0.1566 - val_f1_m: 0.9893\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 779us/step - loss: 0.2365 - f1_m: 0.9575 - val_loss: 0.1220 - val_f1_m: 0.9922\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 786us/step - loss: 0.1922 - f1_m: 0.9725 - val_loss: 0.1078 - val_f1_m: 0.9912\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 783us/step - loss: 0.1608 - f1_m: 0.9825 - val_loss: 0.0877 - val_f1_m: 0.9951\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 802us/step - loss: 0.1380 - f1_m: 0.9870 - val_loss: 0.0808 - val_f1_m: 0.9951\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 790us/step - loss: 0.1208 - f1_m: 0.9910 - val_loss: 0.0737 - val_f1_m: 0.9951\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 776us/step - loss: 0.1076 - f1_m: 0.9927 - val_loss: 0.0658 - val_f1_m: 0.9961\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 798us/step - loss: 0.0967 - f1_m: 0.9927 - val_loss: 0.0538 - val_f1_m: 0.9990\n",
      "0.9927499890327454\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5728 - f1_m: 0.6980 - val_loss: 0.2787 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 781us/step - loss: 0.4057 - f1_m: 0.7700 - val_loss: 0.2002 - val_f1_m: 0.9980\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 793us/step - loss: 0.3063 - f1_m: 0.9005 - val_loss: 0.1587 - val_f1_m: 0.9922\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 866us/step - loss: 0.2411 - f1_m: 0.9492 - val_loss: 0.1324 - val_f1_m: 0.9902\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 856us/step - loss: 0.1960 - f1_m: 0.9685 - val_loss: 0.1211 - val_f1_m: 0.9854\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 781us/step - loss: 0.1641 - f1_m: 0.9805 - val_loss: 0.1117 - val_f1_m: 0.9834\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 813us/step - loss: 0.1411 - f1_m: 0.9868 - val_loss: 0.0853 - val_f1_m: 0.9951\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 841us/step - loss: 0.1235 - f1_m: 0.9905 - val_loss: 0.0844 - val_f1_m: 0.9922\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 803us/step - loss: 0.1101 - f1_m: 0.9933 - val_loss: 0.0752 - val_f1_m: 0.9951\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 783us/step - loss: 0.0995 - f1_m: 0.9923 - val_loss: 0.0706 - val_f1_m: 0.9932\n",
      "0.9922500252723694\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5724 - f1_m: 0.6980 - val_loss: 0.2892 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 793us/step - loss: 0.4045 - f1_m: 0.7745 - val_loss: 0.2000 - val_f1_m: 0.9990\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 791us/step - loss: 0.3066 - f1_m: 0.9005 - val_loss: 0.1560 - val_f1_m: 0.9932\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 794us/step - loss: 0.2427 - f1_m: 0.9455 - val_loss: 0.1339 - val_f1_m: 0.9883\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 784us/step - loss: 0.1984 - f1_m: 0.9657 - val_loss: 0.1155 - val_f1_m: 0.9873\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 788us/step - loss: 0.1676 - f1_m: 0.9777 - val_loss: 0.0882 - val_f1_m: 0.9961\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 784us/step - loss: 0.1445 - f1_m: 0.9822 - val_loss: 0.0836 - val_f1_m: 0.9912\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 776us/step - loss: 0.1273 - f1_m: 0.9850 - val_loss: 0.0649 - val_f1_m: 0.9971\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 787us/step - loss: 0.1142 - f1_m: 0.9880 - val_loss: 0.0701 - val_f1_m: 0.9902\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 786us/step - loss: 0.1038 - f1_m: 0.9887 - val_loss: 0.0560 - val_f1_m: 0.9961\n",
      "0.9887499809265137\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5747 - f1_m: 0.6965 - val_loss: 0.2925 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 800us/step - loss: 0.4026 - f1_m: 0.7730 - val_loss: 0.2019 - val_f1_m: 0.9990\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 788us/step - loss: 0.3030 - f1_m: 0.9007 - val_loss: 0.1610 - val_f1_m: 0.9902\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 781us/step - loss: 0.2385 - f1_m: 0.9555 - val_loss: 0.1239 - val_f1_m: 0.9941\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 770us/step - loss: 0.1938 - f1_m: 0.9705 - val_loss: 0.1111 - val_f1_m: 0.9902\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 800us/step - loss: 0.1629 - f1_m: 0.9818 - val_loss: 0.0939 - val_f1_m: 0.9922\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 789us/step - loss: 0.1398 - f1_m: 0.9872 - val_loss: 0.0785 - val_f1_m: 0.9961\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 780us/step - loss: 0.1226 - f1_m: 0.9915 - val_loss: 0.0699 - val_f1_m: 0.9971\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 822us/step - loss: 0.1094 - f1_m: 0.9912 - val_loss: 0.0665 - val_f1_m: 0.9941\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 787us/step - loss: 0.0989 - f1_m: 0.9895 - val_loss: 0.0711 - val_f1_m: 0.9834\n",
      "0.9894999861717224\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5720 - f1_m: 0.6982 - val_loss: 0.2834 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 794us/step - loss: 0.4021 - f1_m: 0.7730 - val_loss: 0.2091 - val_f1_m: 0.9971\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 787us/step - loss: 0.3020 - f1_m: 0.9112 - val_loss: 0.1568 - val_f1_m: 0.9912\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 796us/step - loss: 0.2361 - f1_m: 0.9545 - val_loss: 0.1259 - val_f1_m: 0.9922\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 810us/step - loss: 0.1915 - f1_m: 0.9693 - val_loss: 0.1123 - val_f1_m: 0.9902\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 780us/step - loss: 0.1596 - f1_m: 0.9847 - val_loss: 0.0810 - val_f1_m: 0.9951\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 783us/step - loss: 0.1374 - f1_m: 0.9877 - val_loss: 0.0759 - val_f1_m: 0.9951\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 811us/step - loss: 0.1204 - f1_m: 0.9920 - val_loss: 0.0723 - val_f1_m: 0.9932\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 766us/step - loss: 0.1073 - f1_m: 0.9925 - val_loss: 0.0708 - val_f1_m: 0.9932\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 794us/step - loss: 0.0973 - f1_m: 0.9915 - val_loss: 0.0648 - val_f1_m: 0.9922\n",
      "0.9915000200271606\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5720 - f1_m: 0.6977 - val_loss: 0.2826 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 823us/step - loss: 0.4000 - f1_m: 0.7782 - val_loss: 0.2068 - val_f1_m: 0.9961\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 811us/step - loss: 0.2995 - f1_m: 0.9168 - val_loss: 0.1586 - val_f1_m: 0.9902\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 783us/step - loss: 0.2341 - f1_m: 0.9552 - val_loss: 0.1385 - val_f1_m: 0.9863\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 792us/step - loss: 0.1894 - f1_m: 0.9735 - val_loss: 0.1145 - val_f1_m: 0.9883\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 789us/step - loss: 0.1578 - f1_m: 0.9805 - val_loss: 0.1075 - val_f1_m: 0.9863\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 783us/step - loss: 0.1353 - f1_m: 0.9875 - val_loss: 0.0939 - val_f1_m: 0.9883\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 791us/step - loss: 0.1182 - f1_m: 0.9908 - val_loss: 0.0753 - val_f1_m: 0.9951\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 781us/step - loss: 0.1050 - f1_m: 0.9920 - val_loss: 0.0709 - val_f1_m: 0.9951\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 778us/step - loss: 0.0946 - f1_m: 0.9935 - val_loss: 0.0747 - val_f1_m: 0.9873\n",
      "0.9934999942779541\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 1s 3ms/step - loss: 0.5710 - f1_m: 0.6960 - val_loss: 0.2779 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 810us/step - loss: 0.4000 - f1_m: 0.7762 - val_loss: 0.2006 - val_f1_m: 0.9961\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 804us/step - loss: 0.3014 - f1_m: 0.9045 - val_loss: 0.1615 - val_f1_m: 0.9873\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 756us/step - loss: 0.2364 - f1_m: 0.9550 - val_loss: 0.1324 - val_f1_m: 0.9814\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 793us/step - loss: 0.1920 - f1_m: 0.9732 - val_loss: 0.1068 - val_f1_m: 0.9844\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 777us/step - loss: 0.1610 - f1_m: 0.9827 - val_loss: 0.0977 - val_f1_m: 0.9844\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 774us/step - loss: 0.1382 - f1_m: 0.9883 - val_loss: 0.0850 - val_f1_m: 0.9834\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 817us/step - loss: 0.1215 - f1_m: 0.9912 - val_loss: 0.0681 - val_f1_m: 0.9971\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 817us/step - loss: 0.1083 - f1_m: 0.9910 - val_loss: 0.0705 - val_f1_m: 0.9873\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 781us/step - loss: 0.0979 - f1_m: 0.9905 - val_loss: 0.0656 - val_f1_m: 0.9863\n",
      "0.9904999732971191\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5704 - f1_m: 0.6985 - val_loss: 0.2709 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 784us/step - loss: 0.4039 - f1_m: 0.7710 - val_loss: 0.2009 - val_f1_m: 1.0000\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 809us/step - loss: 0.3055 - f1_m: 0.9050 - val_loss: 0.1680 - val_f1_m: 0.9854\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 776us/step - loss: 0.2411 - f1_m: 0.9503 - val_loss: 0.1386 - val_f1_m: 0.9844\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 772us/step - loss: 0.1965 - f1_m: 0.9690 - val_loss: 0.1191 - val_f1_m: 0.9883\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 770us/step - loss: 0.1647 - f1_m: 0.9822 - val_loss: 0.1060 - val_f1_m: 0.9902\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 759us/step - loss: 0.1418 - f1_m: 0.9870 - val_loss: 0.0873 - val_f1_m: 0.9961\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 779us/step - loss: 0.1247 - f1_m: 0.9900 - val_loss: 0.0845 - val_f1_m: 0.9912\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 779us/step - loss: 0.1114 - f1_m: 0.9912 - val_loss: 0.0760 - val_f1_m: 0.9932\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 771us/step - loss: 0.1010 - f1_m: 0.9900 - val_loss: 0.0756 - val_f1_m: 0.9883\n",
      "0.9900000095367432\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5696 - f1_m: 0.6980 - val_loss: 0.2840 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - 0s 793us/step - loss: 0.4023 - f1_m: 0.7695 - val_loss: 0.2033 - val_f1_m: 0.9990\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 797us/step - loss: 0.3034 - f1_m: 0.9095 - val_loss: 0.1660 - val_f1_m: 0.9922\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 790us/step - loss: 0.2386 - f1_m: 0.9545 - val_loss: 0.1302 - val_f1_m: 0.9941\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 792us/step - loss: 0.1938 - f1_m: 0.9707 - val_loss: 0.1121 - val_f1_m: 0.9932\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 807us/step - loss: 0.1621 - f1_m: 0.9820 - val_loss: 0.1034 - val_f1_m: 0.9912\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 779us/step - loss: 0.1392 - f1_m: 0.9875 - val_loss: 0.0805 - val_f1_m: 0.9971\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 779us/step - loss: 0.1222 - f1_m: 0.9902 - val_loss: 0.0757 - val_f1_m: 0.9961\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 787us/step - loss: 0.1090 - f1_m: 0.9890 - val_loss: 0.0757 - val_f1_m: 0.9902\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 789us/step - loss: 0.0986 - f1_m: 0.9918 - val_loss: 0.0597 - val_f1_m: 0.9961\n",
      "0.9917500019073486\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5713 - f1_m: 0.6977 - val_loss: 0.2736 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 796us/step - loss: 0.4040 - f1_m: 0.7732 - val_loss: 0.1993 - val_f1_m: 0.9951\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 769us/step - loss: 0.3041 - f1_m: 0.9043 - val_loss: 0.1420 - val_f1_m: 0.9932\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 759us/step - loss: 0.2395 - f1_m: 0.9490 - val_loss: 0.1189 - val_f1_m: 0.9893\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 776us/step - loss: 0.1957 - f1_m: 0.9687 - val_loss: 0.1021 - val_f1_m: 0.9922\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 802us/step - loss: 0.1648 - f1_m: 0.9783 - val_loss: 0.0897 - val_f1_m: 0.9922\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 788us/step - loss: 0.1421 - f1_m: 0.9855 - val_loss: 0.0788 - val_f1_m: 0.9932\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 786us/step - loss: 0.1251 - f1_m: 0.9883 - val_loss: 0.0690 - val_f1_m: 0.9941\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 790us/step - loss: 0.1119 - f1_m: 0.9900 - val_loss: 0.0709 - val_f1_m: 0.9883\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 776us/step - loss: 0.1017 - f1_m: 0.9920 - val_loss: 0.0625 - val_f1_m: 0.9932\n",
      "0.9919999837875366\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5717 - f1_m: 0.6950 - val_loss: 0.2870 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 792us/step - loss: 0.4023 - f1_m: 0.7745 - val_loss: 0.1952 - val_f1_m: 1.0000\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 790us/step - loss: 0.3042 - f1_m: 0.9020 - val_loss: 0.1550 - val_f1_m: 0.9951\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 796us/step - loss: 0.2394 - f1_m: 0.9510 - val_loss: 0.1307 - val_f1_m: 0.9902\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 798us/step - loss: 0.1946 - f1_m: 0.9712 - val_loss: 0.1040 - val_f1_m: 0.9961\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 793us/step - loss: 0.1633 - f1_m: 0.9808 - val_loss: 0.0932 - val_f1_m: 0.9951\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 853us/step - loss: 0.1401 - f1_m: 0.9883 - val_loss: 0.0875 - val_f1_m: 0.9941\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 805us/step - loss: 0.1232 - f1_m: 0.9883 - val_loss: 0.0670 - val_f1_m: 1.0000\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 826us/step - loss: 0.1099 - f1_m: 0.9925 - val_loss: 0.0683 - val_f1_m: 0.9951\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 819us/step - loss: 0.0994 - f1_m: 0.9912 - val_loss: 0.0629 - val_f1_m: 0.9951\n",
      "0.9912499785423279\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5739 - f1_m: 0.6965 - val_loss: 0.2712 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 799us/step - loss: 0.4058 - f1_m: 0.7660 - val_loss: 0.2013 - val_f1_m: 1.0000\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 792us/step - loss: 0.3060 - f1_m: 0.9035 - val_loss: 0.1584 - val_f1_m: 0.9961\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 776us/step - loss: 0.2410 - f1_m: 0.9492 - val_loss: 0.1242 - val_f1_m: 0.9990\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 790us/step - loss: 0.1961 - f1_m: 0.9665 - val_loss: 0.1152 - val_f1_m: 0.9912\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 824us/step - loss: 0.1644 - f1_m: 0.9793 - val_loss: 0.0999 - val_f1_m: 0.9922\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 789us/step - loss: 0.1410 - f1_m: 0.9845 - val_loss: 0.0750 - val_f1_m: 0.9990\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 789us/step - loss: 0.1239 - f1_m: 0.9887 - val_loss: 0.0642 - val_f1_m: 0.9990\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 785us/step - loss: 0.1106 - f1_m: 0.9900 - val_loss: 0.0625 - val_f1_m: 0.9980\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 754us/step - loss: 0.0998 - f1_m: 0.9902 - val_loss: 0.0657 - val_f1_m: 0.9922\n",
      "0.9902499914169312\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5707 - f1_m: 0.6970 - val_loss: 0.2830 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 925us/step - loss: 0.4013 - f1_m: 0.7702 - val_loss: 0.2079 - val_f1_m: 0.9971\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 842us/step - loss: 0.3024 - f1_m: 0.9127 - val_loss: 0.1553 - val_f1_m: 0.9961\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 814us/step - loss: 0.2380 - f1_m: 0.9495 - val_loss: 0.1374 - val_f1_m: 0.9902\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 810us/step - loss: 0.1939 - f1_m: 0.9663 - val_loss: 0.1246 - val_f1_m: 0.9854\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 809us/step - loss: 0.1635 - f1_m: 0.9790 - val_loss: 0.0957 - val_f1_m: 0.9971\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 774us/step - loss: 0.1409 - f1_m: 0.9855 - val_loss: 0.0830 - val_f1_m: 0.9980\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 780us/step - loss: 0.1242 - f1_m: 0.9887 - val_loss: 0.0844 - val_f1_m: 0.9873\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 786us/step - loss: 0.1111 - f1_m: 0.9900 - val_loss: 0.0665 - val_f1_m: 0.9980\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 794us/step - loss: 0.1008 - f1_m: 0.9918 - val_loss: 0.0699 - val_f1_m: 0.9932\n",
      "0.9917500019073486\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.5734 - f1_m: 0.6967 - val_loss: 0.2734 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 800us/step - loss: 0.4051 - f1_m: 0.7682 - val_loss: 0.1782 - val_f1_m: 1.0000\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 848us/step - loss: 0.3056 - f1_m: 0.9067 - val_loss: 0.1399 - val_f1_m: 0.9912\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 775us/step - loss: 0.2397 - f1_m: 0.9477 - val_loss: 0.1002 - val_f1_m: 0.9922\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 798us/step - loss: 0.1954 - f1_m: 0.9728 - val_loss: 0.0962 - val_f1_m: 0.9873\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 802us/step - loss: 0.1637 - f1_m: 0.9825 - val_loss: 0.0773 - val_f1_m: 0.9902\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 787us/step - loss: 0.1410 - f1_m: 0.9860 - val_loss: 0.0692 - val_f1_m: 0.9893\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 775us/step - loss: 0.1241 - f1_m: 0.9908 - val_loss: 0.0581 - val_f1_m: 0.9922\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 786us/step - loss: 0.1111 - f1_m: 0.9902 - val_loss: 0.0560 - val_f1_m: 0.9863\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 788us/step - loss: 0.1007 - f1_m: 0.9918 - val_loss: 0.0473 - val_f1_m: 0.9902\n",
      "0.9917500019073486\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5716 - f1_m: 0.6977 - val_loss: 0.2834 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 796us/step - loss: 0.4004 - f1_m: 0.7760 - val_loss: 0.2174 - val_f1_m: 0.9961\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 767us/step - loss: 0.3014 - f1_m: 0.9107 - val_loss: 0.1505 - val_f1_m: 0.9971\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 752us/step - loss: 0.2375 - f1_m: 0.9537 - val_loss: 0.1331 - val_f1_m: 0.9844\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 931us/step - loss: 0.1932 - f1_m: 0.9700 - val_loss: 0.1261 - val_f1_m: 0.9756\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 997us/step - loss: 0.1619 - f1_m: 0.9812 - val_loss: 0.0955 - val_f1_m: 0.9912\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.1393 - f1_m: 0.9868 - val_loss: 0.0939 - val_f1_m: 0.9844\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 881us/step - loss: 0.1222 - f1_m: 0.9912 - val_loss: 0.0810 - val_f1_m: 0.9883\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 934us/step - loss: 0.1092 - f1_m: 0.9925 - val_loss: 0.0693 - val_f1_m: 0.9912\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 996us/step - loss: 0.0992 - f1_m: 0.9920 - val_loss: 0.0617 - val_f1_m: 0.9941\n",
      "0.9919999837875366\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.5713 - f1_m: 0.7005 - val_loss: 0.2698 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 834us/step - loss: 0.4053 - f1_m: 0.7795 - val_loss: 0.1858 - val_f1_m: 1.0000\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 965us/step - loss: 0.3063 - f1_m: 0.9015 - val_loss: 0.1489 - val_f1_m: 0.9932\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 798us/step - loss: 0.2414 - f1_m: 0.9520 - val_loss: 0.1214 - val_f1_m: 0.9922\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 799us/step - loss: 0.1968 - f1_m: 0.9695 - val_loss: 0.1127 - val_f1_m: 0.9863\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 807us/step - loss: 0.1656 - f1_m: 0.9785 - val_loss: 0.0953 - val_f1_m: 0.9902\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 810us/step - loss: 0.1425 - f1_m: 0.9865 - val_loss: 0.0777 - val_f1_m: 0.9980\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 797us/step - loss: 0.1253 - f1_m: 0.9893 - val_loss: 0.0634 - val_f1_m: 1.0000\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 773us/step - loss: 0.1120 - f1_m: 0.9908 - val_loss: 0.0664 - val_f1_m: 0.9912\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 768us/step - loss: 0.1018 - f1_m: 0.9895 - val_loss: 0.0498 - val_f1_m: 1.0000\n",
      "0.9894999861717224\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5732 - f1_m: 0.6940 - val_loss: 0.2787 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 794us/step - loss: 0.4022 - f1_m: 0.7725 - val_loss: 0.1956 - val_f1_m: 0.9990\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 794us/step - loss: 0.3026 - f1_m: 0.9078 - val_loss: 0.1532 - val_f1_m: 0.9893\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 778us/step - loss: 0.2382 - f1_m: 0.9475 - val_loss: 0.1476 - val_f1_m: 0.9727\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 788us/step - loss: 0.1938 - f1_m: 0.9707 - val_loss: 0.1196 - val_f1_m: 0.9805\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 850us/step - loss: 0.1624 - f1_m: 0.9805 - val_loss: 0.0994 - val_f1_m: 0.9873\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 774us/step - loss: 0.1393 - f1_m: 0.9887 - val_loss: 0.0791 - val_f1_m: 0.9971\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 782us/step - loss: 0.1226 - f1_m: 0.9902 - val_loss: 0.0781 - val_f1_m: 0.9932\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 791us/step - loss: 0.1092 - f1_m: 0.9912 - val_loss: 0.0790 - val_f1_m: 0.9844\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 786us/step - loss: 0.0985 - f1_m: 0.9918 - val_loss: 0.0648 - val_f1_m: 0.9961\n",
      "0.9917500019073486\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5750 - f1_m: 0.6967 - val_loss: 0.2912 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 909us/step - loss: 0.4050 - f1_m: 0.7670 - val_loss: 0.2281 - val_f1_m: 0.9824\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 886us/step - loss: 0.3072 - f1_m: 0.9115 - val_loss: 0.1680 - val_f1_m: 0.9824\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 872us/step - loss: 0.2434 - f1_m: 0.9502 - val_loss: 0.1352 - val_f1_m: 0.9824\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 797us/step - loss: 0.1986 - f1_m: 0.9670 - val_loss: 0.1214 - val_f1_m: 0.9805\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 804us/step - loss: 0.1667 - f1_m: 0.9797 - val_loss: 0.0904 - val_f1_m: 0.9902\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 790us/step - loss: 0.1437 - f1_m: 0.9833 - val_loss: 0.0897 - val_f1_m: 0.9854\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 776us/step - loss: 0.1262 - f1_m: 0.9865 - val_loss: 0.0814 - val_f1_m: 0.9863\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 787us/step - loss: 0.1128 - f1_m: 0.9895 - val_loss: 0.0669 - val_f1_m: 0.9912\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 825us/step - loss: 0.1025 - f1_m: 0.9887 - val_loss: 0.0616 - val_f1_m: 0.9912\n",
      "0.9887499809265137\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.5711 - f1_m: 0.6957 - val_loss: 0.2792 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 916us/step - loss: 0.4026 - f1_m: 0.7707 - val_loss: 0.1990 - val_f1_m: 1.0000\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 829us/step - loss: 0.3046 - f1_m: 0.9065 - val_loss: 0.1531 - val_f1_m: 0.9961\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 809us/step - loss: 0.2403 - f1_m: 0.9503 - val_loss: 0.1374 - val_f1_m: 0.9883\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 776us/step - loss: 0.1957 - f1_m: 0.9675 - val_loss: 0.1123 - val_f1_m: 0.9893\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 778us/step - loss: 0.1642 - f1_m: 0.9758 - val_loss: 0.0961 - val_f1_m: 0.9902\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 799us/step - loss: 0.1410 - f1_m: 0.9820 - val_loss: 0.0909 - val_f1_m: 0.9883\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 836us/step - loss: 0.1239 - f1_m: 0.9858 - val_loss: 0.0754 - val_f1_m: 0.9922\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 815us/step - loss: 0.1112 - f1_m: 0.9862 - val_loss: 0.0658 - val_f1_m: 0.9922\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 797us/step - loss: 0.1009 - f1_m: 0.9872 - val_loss: 0.0689 - val_f1_m: 0.9902\n",
      "0.9872499704360962\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.5724 - f1_m: 0.6982 - val_loss: 0.2868 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 860us/step - loss: 0.4021 - f1_m: 0.7750 - val_loss: 0.1978 - val_f1_m: 0.9990\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 818us/step - loss: 0.3037 - f1_m: 0.9057 - val_loss: 0.1536 - val_f1_m: 0.9932\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 784us/step - loss: 0.2384 - f1_m: 0.9497 - val_loss: 0.1269 - val_f1_m: 0.9922\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 802us/step - loss: 0.1940 - f1_m: 0.9715 - val_loss: 0.1106 - val_f1_m: 0.9922\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 796us/step - loss: 0.1626 - f1_m: 0.9790 - val_loss: 0.0908 - val_f1_m: 0.9941\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 782us/step - loss: 0.1399 - f1_m: 0.9887 - val_loss: 0.0810 - val_f1_m: 0.9941\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 806us/step - loss: 0.1228 - f1_m: 0.9918 - val_loss: 0.0764 - val_f1_m: 0.9932\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 810us/step - loss: 0.1096 - f1_m: 0.9920 - val_loss: 0.0671 - val_f1_m: 0.9951\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - 0s 805us/step - loss: 0.0987 - f1_m: 0.9948 - val_loss: 0.0610 - val_f1_m: 0.9941\n",
      "0.9947500228881836\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5730 - f1_m: 0.6985 - val_loss: 0.2901 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 790us/step - loss: 0.4013 - f1_m: 0.7692 - val_loss: 0.2062 - val_f1_m: 0.9990\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 796us/step - loss: 0.3035 - f1_m: 0.9020 - val_loss: 0.1793 - val_f1_m: 0.9834\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 780us/step - loss: 0.2394 - f1_m: 0.9503 - val_loss: 0.1297 - val_f1_m: 0.9951\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 774us/step - loss: 0.1948 - f1_m: 0.9660 - val_loss: 0.1208 - val_f1_m: 0.9922\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 805us/step - loss: 0.1632 - f1_m: 0.9785 - val_loss: 0.1026 - val_f1_m: 0.9932\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 766us/step - loss: 0.1406 - f1_m: 0.9858 - val_loss: 0.0880 - val_f1_m: 0.9951\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 771us/step - loss: 0.1235 - f1_m: 0.9883 - val_loss: 0.0741 - val_f1_m: 0.9990\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 781us/step - loss: 0.1103 - f1_m: 0.9905 - val_loss: 0.0800 - val_f1_m: 0.9873\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 773us/step - loss: 0.1001 - f1_m: 0.9893 - val_loss: 0.0731 - val_f1_m: 0.9863\n",
      "0.9892500042915344\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5730 - f1_m: 0.6967 - val_loss: 0.2879 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 784us/step - loss: 0.4058 - f1_m: 0.7700 - val_loss: 0.2014 - val_f1_m: 0.9980\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 801us/step - loss: 0.3083 - f1_m: 0.9042 - val_loss: 0.1510 - val_f1_m: 0.9961\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 786us/step - loss: 0.2432 - f1_m: 0.9488 - val_loss: 0.1384 - val_f1_m: 0.9844\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 795us/step - loss: 0.1983 - f1_m: 0.9697 - val_loss: 0.1114 - val_f1_m: 0.9893\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 790us/step - loss: 0.1662 - f1_m: 0.9808 - val_loss: 0.0939 - val_f1_m: 0.9922\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 781us/step - loss: 0.1432 - f1_m: 0.9887 - val_loss: 0.0870 - val_f1_m: 0.9922\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 784us/step - loss: 0.1253 - f1_m: 0.9915 - val_loss: 0.0600 - val_f1_m: 0.9990\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 789us/step - loss: 0.1124 - f1_m: 0.9937 - val_loss: 0.0695 - val_f1_m: 0.9941\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 785us/step - loss: 0.1019 - f1_m: 0.9937 - val_loss: 0.0628 - val_f1_m: 0.9941\n",
      "0.9937499761581421\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 1s 1ms/step - loss: 0.5708 - f1_m: 0.6945 - val_loss: 0.2780 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 789us/step - loss: 0.4028 - f1_m: 0.7737 - val_loss: 0.1992 - val_f1_m: 0.9980\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 794us/step - loss: 0.3030 - f1_m: 0.9020 - val_loss: 0.1605 - val_f1_m: 0.9902\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 791us/step - loss: 0.2375 - f1_m: 0.9495 - val_loss: 0.1256 - val_f1_m: 0.9932\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 800us/step - loss: 0.1930 - f1_m: 0.9672 - val_loss: 0.1131 - val_f1_m: 0.9883\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 777us/step - loss: 0.1616 - f1_m: 0.9810 - val_loss: 0.1064 - val_f1_m: 0.9844\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 793us/step - loss: 0.1391 - f1_m: 0.9868 - val_loss: 0.0887 - val_f1_m: 0.9932\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 777us/step - loss: 0.1221 - f1_m: 0.9900 - val_loss: 0.0886 - val_f1_m: 0.9834\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 794us/step - loss: 0.1091 - f1_m: 0.9910 - val_loss: 0.0781 - val_f1_m: 0.9883\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 787us/step - loss: 0.0989 - f1_m: 0.9910 - val_loss: 0.0697 - val_f1_m: 0.9902\n",
      "0.9909999966621399\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5778 - f1_m: 0.6960 - val_loss: 0.2879 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 797us/step - loss: 0.4031 - f1_m: 0.7720 - val_loss: 0.2011 - val_f1_m: 0.9971\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 791us/step - loss: 0.3003 - f1_m: 0.9080 - val_loss: 0.1525 - val_f1_m: 0.9893\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 789us/step - loss: 0.2353 - f1_m: 0.9545 - val_loss: 0.1233 - val_f1_m: 0.9883\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 775us/step - loss: 0.1912 - f1_m: 0.9700 - val_loss: 0.1087 - val_f1_m: 0.9863\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 769us/step - loss: 0.1601 - f1_m: 0.9815 - val_loss: 0.0945 - val_f1_m: 0.9873\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 778us/step - loss: 0.1377 - f1_m: 0.9877 - val_loss: 0.0877 - val_f1_m: 0.9863\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 799us/step - loss: 0.1210 - f1_m: 0.9872 - val_loss: 0.0802 - val_f1_m: 0.9863\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 775us/step - loss: 0.1083 - f1_m: 0.9895 - val_loss: 0.0720 - val_f1_m: 0.9902\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 781us/step - loss: 0.0984 - f1_m: 0.9890 - val_loss: 0.0647 - val_f1_m: 0.9912\n",
      "0.9890000224113464\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5699 - f1_m: 0.6945 - val_loss: 0.2704 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 792us/step - loss: 0.4003 - f1_m: 0.7765 - val_loss: 0.2102 - val_f1_m: 0.9922\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 776us/step - loss: 0.3015 - f1_m: 0.9137 - val_loss: 0.1564 - val_f1_m: 0.9893\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 783us/step - loss: 0.2371 - f1_m: 0.9527 - val_loss: 0.1414 - val_f1_m: 0.9814\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 781us/step - loss: 0.1937 - f1_m: 0.9693 - val_loss: 0.1099 - val_f1_m: 0.9893\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 802us/step - loss: 0.1621 - f1_m: 0.9820 - val_loss: 0.1027 - val_f1_m: 0.9834\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 778us/step - loss: 0.1395 - f1_m: 0.9880 - val_loss: 0.0931 - val_f1_m: 0.9834\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 789us/step - loss: 0.1226 - f1_m: 0.9905 - val_loss: 0.0704 - val_f1_m: 0.9951\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 788us/step - loss: 0.1094 - f1_m: 0.9905 - val_loss: 0.0689 - val_f1_m: 0.9902\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 773us/step - loss: 0.0991 - f1_m: 0.9933 - val_loss: 0.0589 - val_f1_m: 0.9932\n",
      "0.9932500123977661\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5716 - f1_m: 0.6967 - val_loss: 0.2778 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 791us/step - loss: 0.4013 - f1_m: 0.7755 - val_loss: 0.1921 - val_f1_m: 1.0000\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 787us/step - loss: 0.3019 - f1_m: 0.9052 - val_loss: 0.1495 - val_f1_m: 0.9971\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 782us/step - loss: 0.2376 - f1_m: 0.9500 - val_loss: 0.1301 - val_f1_m: 0.9922\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 776us/step - loss: 0.1934 - f1_m: 0.9695 - val_loss: 0.1117 - val_f1_m: 0.9932\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 766us/step - loss: 0.1622 - f1_m: 0.9800 - val_loss: 0.0921 - val_f1_m: 0.9971\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 774us/step - loss: 0.1394 - f1_m: 0.9860 - val_loss: 0.0851 - val_f1_m: 0.9971\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 786us/step - loss: 0.1224 - f1_m: 0.9868 - val_loss: 0.0758 - val_f1_m: 0.9980\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 775us/step - loss: 0.1092 - f1_m: 0.9890 - val_loss: 0.0669 - val_f1_m: 0.9971\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 776us/step - loss: 0.0988 - f1_m: 0.9898 - val_loss: 0.0667 - val_f1_m: 0.9912\n",
      "0.9897500276565552\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5727 - f1_m: 0.6995 - val_loss: 0.2757 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 802us/step - loss: 0.4069 - f1_m: 0.7750 - val_loss: 0.1924 - val_f1_m: 0.9971\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 784us/step - loss: 0.3078 - f1_m: 0.9010 - val_loss: 0.1462 - val_f1_m: 0.9951\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 779us/step - loss: 0.2423 - f1_m: 0.9467 - val_loss: 0.1285 - val_f1_m: 0.9922\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 784us/step - loss: 0.1966 - f1_m: 0.9697 - val_loss: 0.1079 - val_f1_m: 0.9922\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 784us/step - loss: 0.1642 - f1_m: 0.9837 - val_loss: 0.0824 - val_f1_m: 0.9990\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 786us/step - loss: 0.1411 - f1_m: 0.9827 - val_loss: 0.0736 - val_f1_m: 0.9990\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 772us/step - loss: 0.1236 - f1_m: 0.9898 - val_loss: 0.0744 - val_f1_m: 0.9990\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 767us/step - loss: 0.1103 - f1_m: 0.9908 - val_loss: 0.0694 - val_f1_m: 0.9971\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 765us/step - loss: 0.0998 - f1_m: 0.9908 - val_loss: 0.0599 - val_f1_m: 0.9980\n",
      "0.9907500147819519\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5738 - f1_m: 0.6977 - val_loss: 0.2938 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 780us/step - loss: 0.4050 - f1_m: 0.7697 - val_loss: 0.1974 - val_f1_m: 1.0000\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 792us/step - loss: 0.3067 - f1_m: 0.9062 - val_loss: 0.1578 - val_f1_m: 0.9951\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 786us/step - loss: 0.2421 - f1_m: 0.9505 - val_loss: 0.1294 - val_f1_m: 0.9912\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 859us/step - loss: 0.1978 - f1_m: 0.9665 - val_loss: 0.1057 - val_f1_m: 0.9941\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 784us/step - loss: 0.1660 - f1_m: 0.9822 - val_loss: 0.1003 - val_f1_m: 0.9883\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 765us/step - loss: 0.1431 - f1_m: 0.9872 - val_loss: 0.0815 - val_f1_m: 0.9941\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 787us/step - loss: 0.1257 - f1_m: 0.9918 - val_loss: 0.0687 - val_f1_m: 0.9990\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 796us/step - loss: 0.1122 - f1_m: 0.9905 - val_loss: 0.0639 - val_f1_m: 0.9990\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 796us/step - loss: 0.1014 - f1_m: 0.9923 - val_loss: 0.0604 - val_f1_m: 0.9961\n",
      "0.9922500252723694\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5696 - f1_m: 0.6955 - val_loss: 0.2649 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 805us/step - loss: 0.4047 - f1_m: 0.7675 - val_loss: 0.1945 - val_f1_m: 0.9980\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 791us/step - loss: 0.3049 - f1_m: 0.8987 - val_loss: 0.1707 - val_f1_m: 0.9785\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 794us/step - loss: 0.2393 - f1_m: 0.9510 - val_loss: 0.1354 - val_f1_m: 0.9824\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 775us/step - loss: 0.1941 - f1_m: 0.9697 - val_loss: 0.1223 - val_f1_m: 0.9785\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 785us/step - loss: 0.1624 - f1_m: 0.9805 - val_loss: 0.0994 - val_f1_m: 0.9854\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 777us/step - loss: 0.1394 - f1_m: 0.9862 - val_loss: 0.0885 - val_f1_m: 0.9854\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 774us/step - loss: 0.1221 - f1_m: 0.9902 - val_loss: 0.0810 - val_f1_m: 0.9844\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 770us/step - loss: 0.1089 - f1_m: 0.9898 - val_loss: 0.0660 - val_f1_m: 0.9932\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 780us/step - loss: 0.0982 - f1_m: 0.9905 - val_loss: 0.0549 - val_f1_m: 0.9961\n",
      "0.9904999732971191\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5732 - f1_m: 0.6972 - val_loss: 0.2710 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 796us/step - loss: 0.4086 - f1_m: 0.7757 - val_loss: 0.1942 - val_f1_m: 1.0000\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 779us/step - loss: 0.3097 - f1_m: 0.9020 - val_loss: 0.1502 - val_f1_m: 0.9941\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 794us/step - loss: 0.2436 - f1_m: 0.9490 - val_loss: 0.1315 - val_f1_m: 0.9883\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 791us/step - loss: 0.1978 - f1_m: 0.9660 - val_loss: 0.1041 - val_f1_m: 0.9922\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 768us/step - loss: 0.1651 - f1_m: 0.9778 - val_loss: 0.0842 - val_f1_m: 0.9961\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 790us/step - loss: 0.1415 - f1_m: 0.9852 - val_loss: 0.0726 - val_f1_m: 0.9971\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 780us/step - loss: 0.1242 - f1_m: 0.9887 - val_loss: 0.0742 - val_f1_m: 0.9941\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 791us/step - loss: 0.1107 - f1_m: 0.9905 - val_loss: 0.0641 - val_f1_m: 0.9971\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 783us/step - loss: 0.1000 - f1_m: 0.9915 - val_loss: 0.0546 - val_f1_m: 0.9971\n",
      "0.9915000200271606\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5701 - f1_m: 0.6985 - val_loss: 0.2696 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 804us/step - loss: 0.4028 - f1_m: 0.7807 - val_loss: 0.1996 - val_f1_m: 0.9961\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 774us/step - loss: 0.3054 - f1_m: 0.8975 - val_loss: 0.1714 - val_f1_m: 0.9775\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 769us/step - loss: 0.2421 - f1_m: 0.9470 - val_loss: 0.1390 - val_f1_m: 0.9834\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 770us/step - loss: 0.1985 - f1_m: 0.9662 - val_loss: 0.1295 - val_f1_m: 0.9727\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 791us/step - loss: 0.1677 - f1_m: 0.9778 - val_loss: 0.1017 - val_f1_m: 0.9873\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 811us/step - loss: 0.1450 - f1_m: 0.9860 - val_loss: 0.1023 - val_f1_m: 0.9834\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 761us/step - loss: 0.1281 - f1_m: 0.9880 - val_loss: 0.0841 - val_f1_m: 0.9883\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 781us/step - loss: 0.1146 - f1_m: 0.9925 - val_loss: 0.0760 - val_f1_m: 0.9893\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 769us/step - loss: 0.1042 - f1_m: 0.9920 - val_loss: 0.0660 - val_f1_m: 0.9941\n",
      "0.9919999837875366\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5725 - f1_m: 0.6977 - val_loss: 0.2757 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 782us/step - loss: 0.4043 - f1_m: 0.7695 - val_loss: 0.2062 - val_f1_m: 0.9980\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 795us/step - loss: 0.3025 - f1_m: 0.9047 - val_loss: 0.1666 - val_f1_m: 0.9863\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 768us/step - loss: 0.2365 - f1_m: 0.9560 - val_loss: 0.1303 - val_f1_m: 0.9883\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 769us/step - loss: 0.1917 - f1_m: 0.9745 - val_loss: 0.1164 - val_f1_m: 0.9863\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 774us/step - loss: 0.1600 - f1_m: 0.9835 - val_loss: 0.0853 - val_f1_m: 0.9971\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 773us/step - loss: 0.1375 - f1_m: 0.9898 - val_loss: 0.0846 - val_f1_m: 0.9922\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - 0s 777us/step - loss: 0.1200 - f1_m: 0.9908 - val_loss: 0.0846 - val_f1_m: 0.9824\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 787us/step - loss: 0.1072 - f1_m: 0.9915 - val_loss: 0.0644 - val_f1_m: 0.9961\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 781us/step - loss: 0.0971 - f1_m: 0.9920 - val_loss: 0.0590 - val_f1_m: 0.9961\n",
      "0.9919999837875366\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5741 - f1_m: 0.6950 - val_loss: 0.2917 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 788us/step - loss: 0.4006 - f1_m: 0.7817 - val_loss: 0.2058 - val_f1_m: 0.9990\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 809us/step - loss: 0.3005 - f1_m: 0.9105 - val_loss: 0.1653 - val_f1_m: 0.9893\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 777us/step - loss: 0.2356 - f1_m: 0.9548 - val_loss: 0.1325 - val_f1_m: 0.9922\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 800us/step - loss: 0.1908 - f1_m: 0.9703 - val_loss: 0.1143 - val_f1_m: 0.9912\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 794us/step - loss: 0.1595 - f1_m: 0.9795 - val_loss: 0.1073 - val_f1_m: 0.9873\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 784us/step - loss: 0.1372 - f1_m: 0.9868 - val_loss: 0.0897 - val_f1_m: 0.9941\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 773us/step - loss: 0.1201 - f1_m: 0.9890 - val_loss: 0.0735 - val_f1_m: 0.9961\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 828us/step - loss: 0.1072 - f1_m: 0.9920 - val_loss: 0.0711 - val_f1_m: 0.9941\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 808us/step - loss: 0.0971 - f1_m: 0.9930 - val_loss: 0.0568 - val_f1_m: 0.9980\n",
      "0.9929999709129333\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5711 - f1_m: 0.6980 - val_loss: 0.2727 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 792us/step - loss: 0.4021 - f1_m: 0.7760 - val_loss: 0.1855 - val_f1_m: 0.9990\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 791us/step - loss: 0.3042 - f1_m: 0.9038 - val_loss: 0.1497 - val_f1_m: 0.9932\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 832us/step - loss: 0.2400 - f1_m: 0.9435 - val_loss: 0.1313 - val_f1_m: 0.9873\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 813us/step - loss: 0.1963 - f1_m: 0.9643 - val_loss: 0.0953 - val_f1_m: 0.9971\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 801us/step - loss: 0.1652 - f1_m: 0.9765 - val_loss: 0.0757 - val_f1_m: 0.9980\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 822us/step - loss: 0.1425 - f1_m: 0.9835 - val_loss: 0.0718 - val_f1_m: 0.9980\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 783us/step - loss: 0.1258 - f1_m: 0.9893 - val_loss: 0.0653 - val_f1_m: 0.9980\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 806us/step - loss: 0.1127 - f1_m: 0.9893 - val_loss: 0.0625 - val_f1_m: 0.9961\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 815us/step - loss: 0.1022 - f1_m: 0.9920 - val_loss: 0.0570 - val_f1_m: 0.9951\n",
      "0.9919999837875366\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.5740 - f1_m: 0.6992 - val_loss: 0.2891 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.4048 - f1_m: 0.7740 - val_loss: 0.2063 - val_f1_m: 0.9990\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 893us/step - loss: 0.3044 - f1_m: 0.9082 - val_loss: 0.1712 - val_f1_m: 0.9844\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 792us/step - loss: 0.2390 - f1_m: 0.9525 - val_loss: 0.1266 - val_f1_m: 0.9922\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 793us/step - loss: 0.1940 - f1_m: 0.9700 - val_loss: 0.1168 - val_f1_m: 0.9854\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 796us/step - loss: 0.1620 - f1_m: 0.9825 - val_loss: 0.1045 - val_f1_m: 0.9814\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 842us/step - loss: 0.1387 - f1_m: 0.9895 - val_loss: 0.0889 - val_f1_m: 0.9893\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 872us/step - loss: 0.1214 - f1_m: 0.9910 - val_loss: 0.0778 - val_f1_m: 0.9912\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 903us/step - loss: 0.1082 - f1_m: 0.9912 - val_loss: 0.0788 - val_f1_m: 0.9805\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 786us/step - loss: 0.0978 - f1_m: 0.9915 - val_loss: 0.0667 - val_f1_m: 0.9893\n",
      "0.9915000200271606\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.5724 - f1_m: 0.6965 - val_loss: 0.2892 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 829us/step - loss: 0.4009 - f1_m: 0.7797 - val_loss: 0.2042 - val_f1_m: 0.9980\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 879us/step - loss: 0.3013 - f1_m: 0.9120 - val_loss: 0.1697 - val_f1_m: 0.9854\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 833us/step - loss: 0.2372 - f1_m: 0.9515 - val_loss: 0.1401 - val_f1_m: 0.9863\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 807us/step - loss: 0.1936 - f1_m: 0.9663 - val_loss: 0.1291 - val_f1_m: 0.9805\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 774us/step - loss: 0.1628 - f1_m: 0.9790 - val_loss: 0.1025 - val_f1_m: 0.9902\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 802us/step - loss: 0.1407 - f1_m: 0.9843 - val_loss: 0.0921 - val_f1_m: 0.9902\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 810us/step - loss: 0.1238 - f1_m: 0.9880 - val_loss: 0.0961 - val_f1_m: 0.9805\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 798us/step - loss: 0.1112 - f1_m: 0.9887 - val_loss: 0.0824 - val_f1_m: 0.9873\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 784us/step - loss: 0.1010 - f1_m: 0.9905 - val_loss: 0.0771 - val_f1_m: 0.9844\n",
      "0.9904999732971191\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5707 - f1_m: 0.6955 - val_loss: 0.2818 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 814us/step - loss: 0.3983 - f1_m: 0.7727 - val_loss: 0.2020 - val_f1_m: 0.9990\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 801us/step - loss: 0.2981 - f1_m: 0.9130 - val_loss: 0.1555 - val_f1_m: 0.9912\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 793us/step - loss: 0.2332 - f1_m: 0.9517 - val_loss: 0.1266 - val_f1_m: 0.9912\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 782us/step - loss: 0.1890 - f1_m: 0.9707 - val_loss: 0.1108 - val_f1_m: 0.9893\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 785us/step - loss: 0.1581 - f1_m: 0.9808 - val_loss: 0.0915 - val_f1_m: 0.9941\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 795us/step - loss: 0.1362 - f1_m: 0.9843 - val_loss: 0.0845 - val_f1_m: 0.9941\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 789us/step - loss: 0.1196 - f1_m: 0.9883 - val_loss: 0.0744 - val_f1_m: 0.9961\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 772us/step - loss: 0.1071 - f1_m: 0.9895 - val_loss: 0.0700 - val_f1_m: 0.9932\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 811us/step - loss: 0.0970 - f1_m: 0.9905 - val_loss: 0.0577 - val_f1_m: 0.9980\n",
      "0.9904999732971191\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.5739 - f1_m: 0.6960 - val_loss: 0.2691 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 812us/step - loss: 0.4089 - f1_m: 0.7682 - val_loss: 0.1795 - val_f1_m: 0.9990\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 811us/step - loss: 0.3091 - f1_m: 0.8987 - val_loss: 0.1565 - val_f1_m: 0.9863\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 975us/step - loss: 0.2431 - f1_m: 0.9473 - val_loss: 0.1255 - val_f1_m: 0.9854\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 806us/step - loss: 0.1972 - f1_m: 0.9688 - val_loss: 0.1076 - val_f1_m: 0.9844\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 782us/step - loss: 0.1647 - f1_m: 0.9795 - val_loss: 0.0888 - val_f1_m: 0.9883\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 798us/step - loss: 0.1414 - f1_m: 0.9852 - val_loss: 0.0850 - val_f1_m: 0.9834\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 811us/step - loss: 0.1243 - f1_m: 0.9895 - val_loss: 0.0667 - val_f1_m: 0.9932\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 800us/step - loss: 0.1112 - f1_m: 0.9915 - val_loss: 0.0625 - val_f1_m: 0.9912\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 798us/step - loss: 0.1007 - f1_m: 0.9920 - val_loss: 0.0593 - val_f1_m: 0.9912\n",
      "0.9919999837875366\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 1s 1ms/step - loss: 0.5717 - f1_m: 0.6945 - val_loss: 0.2736 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 799us/step - loss: 0.4035 - f1_m: 0.7747 - val_loss: 0.1821 - val_f1_m: 1.0000\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 781us/step - loss: 0.3049 - f1_m: 0.8987 - val_loss: 0.1464 - val_f1_m: 0.9932\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 779us/step - loss: 0.2394 - f1_m: 0.9485 - val_loss: 0.1321 - val_f1_m: 0.9873\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 764us/step - loss: 0.1952 - f1_m: 0.9693 - val_loss: 0.1008 - val_f1_m: 0.9912\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 781us/step - loss: 0.1637 - f1_m: 0.9803 - val_loss: 0.0847 - val_f1_m: 0.9932\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 791us/step - loss: 0.1406 - f1_m: 0.9852 - val_loss: 0.0874 - val_f1_m: 0.9893\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 777us/step - loss: 0.1239 - f1_m: 0.9908 - val_loss: 0.0722 - val_f1_m: 0.9912\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 818us/step - loss: 0.1104 - f1_m: 0.9910 - val_loss: 0.0600 - val_f1_m: 0.9951\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 797us/step - loss: 0.1002 - f1_m: 0.9918 - val_loss: 0.0546 - val_f1_m: 0.9961\n",
      "0.9917500019073486\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.5729 - f1_m: 0.6977 - val_loss: 0.2830 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 872us/step - loss: 0.4066 - f1_m: 0.7727 - val_loss: 0.2117 - val_f1_m: 0.9980\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 814us/step - loss: 0.3070 - f1_m: 0.8992 - val_loss: 0.1801 - val_f1_m: 0.9814\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 788us/step - loss: 0.2417 - f1_m: 0.9470 - val_loss: 0.1513 - val_f1_m: 0.9795\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 796us/step - loss: 0.1964 - f1_m: 0.9665 - val_loss: 0.1209 - val_f1_m: 0.9893\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 853us/step - loss: 0.1643 - f1_m: 0.9783 - val_loss: 0.1168 - val_f1_m: 0.9863\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 786us/step - loss: 0.1409 - f1_m: 0.9875 - val_loss: 0.1016 - val_f1_m: 0.9893\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 833us/step - loss: 0.1234 - f1_m: 0.9895 - val_loss: 0.0854 - val_f1_m: 0.9961\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 816us/step - loss: 0.1102 - f1_m: 0.9883 - val_loss: 0.0816 - val_f1_m: 0.9932\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 786us/step - loss: 0.0996 - f1_m: 0.9912 - val_loss: 0.0731 - val_f1_m: 0.9922\n",
      "0.9912499785423279\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5753 - f1_m: 0.6955 - val_loss: 0.2780 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 782us/step - loss: 0.4083 - f1_m: 0.7642 - val_loss: 0.1974 - val_f1_m: 0.9980\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 778us/step - loss: 0.3074 - f1_m: 0.9052 - val_loss: 0.1400 - val_f1_m: 0.9932\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 779us/step - loss: 0.2404 - f1_m: 0.9475 - val_loss: 0.1138 - val_f1_m: 0.9932\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 790us/step - loss: 0.1944 - f1_m: 0.9672 - val_loss: 0.1035 - val_f1_m: 0.9893\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 804us/step - loss: 0.1620 - f1_m: 0.9840 - val_loss: 0.0765 - val_f1_m: 0.9961\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 805us/step - loss: 0.1388 - f1_m: 0.9860 - val_loss: 0.0686 - val_f1_m: 0.9941\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 796us/step - loss: 0.1212 - f1_m: 0.9898 - val_loss: 0.0670 - val_f1_m: 0.9912\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 784us/step - loss: 0.1079 - f1_m: 0.9900 - val_loss: 0.0516 - val_f1_m: 0.9961\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 778us/step - loss: 0.0974 - f1_m: 0.9918 - val_loss: 0.0534 - val_f1_m: 0.9922\n",
      "0.9917500019073486\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5717 - f1_m: 0.6962 - val_loss: 0.2783 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 786us/step - loss: 0.4035 - f1_m: 0.7745 - val_loss: 0.2001 - val_f1_m: 0.9980\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 779us/step - loss: 0.3057 - f1_m: 0.9018 - val_loss: 0.1538 - val_f1_m: 0.9912\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 758us/step - loss: 0.2410 - f1_m: 0.9520 - val_loss: 0.1393 - val_f1_m: 0.9795\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 770us/step - loss: 0.1958 - f1_m: 0.9700 - val_loss: 0.1235 - val_f1_m: 0.9785\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 767us/step - loss: 0.1640 - f1_m: 0.9803 - val_loss: 0.1071 - val_f1_m: 0.9805\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 760us/step - loss: 0.1413 - f1_m: 0.9875 - val_loss: 0.0872 - val_f1_m: 0.9893\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 761us/step - loss: 0.1239 - f1_m: 0.9898 - val_loss: 0.0701 - val_f1_m: 0.9951\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 783us/step - loss: 0.1106 - f1_m: 0.9900 - val_loss: 0.0826 - val_f1_m: 0.9805\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 775us/step - loss: 0.1003 - f1_m: 0.9905 - val_loss: 0.0619 - val_f1_m: 0.9951\n",
      "0.9904999732971191\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5695 - f1_m: 0.7005 - val_loss: 0.2720 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 802us/step - loss: 0.4055 - f1_m: 0.7662 - val_loss: 0.2102 - val_f1_m: 0.9980\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 786us/step - loss: 0.3066 - f1_m: 0.9067 - val_loss: 0.1530 - val_f1_m: 0.9961\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 784us/step - loss: 0.2407 - f1_m: 0.9470 - val_loss: 0.1404 - val_f1_m: 0.9844\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 787us/step - loss: 0.1957 - f1_m: 0.9707 - val_loss: 0.1223 - val_f1_m: 0.9844\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 771us/step - loss: 0.1639 - f1_m: 0.9812 - val_loss: 0.0967 - val_f1_m: 0.9951\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 791us/step - loss: 0.1410 - f1_m: 0.9850 - val_loss: 0.0918 - val_f1_m: 0.9902\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 772us/step - loss: 0.1241 - f1_m: 0.9908 - val_loss: 0.0874 - val_f1_m: 0.9854\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 782us/step - loss: 0.1111 - f1_m: 0.9920 - val_loss: 0.0696 - val_f1_m: 0.9961\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 774us/step - loss: 0.1011 - f1_m: 0.9887 - val_loss: 0.0592 - val_f1_m: 0.9980\n",
      "0.9887499809265137\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5665 - f1_m: 0.6997 - val_loss: 0.2712 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 771us/step - loss: 0.4012 - f1_m: 0.7757 - val_loss: 0.1900 - val_f1_m: 1.0000\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 788us/step - loss: 0.3023 - f1_m: 0.9067 - val_loss: 0.1433 - val_f1_m: 0.9990\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 783us/step - loss: 0.2376 - f1_m: 0.9507 - val_loss: 0.1290 - val_f1_m: 0.9941\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 774us/step - loss: 0.1932 - f1_m: 0.9715 - val_loss: 0.1044 - val_f1_m: 0.9941\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - 0s 779us/step - loss: 0.1619 - f1_m: 0.9765 - val_loss: 0.0873 - val_f1_m: 0.9980\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 785us/step - loss: 0.1393 - f1_m: 0.9840 - val_loss: 0.0745 - val_f1_m: 1.0000\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 779us/step - loss: 0.1220 - f1_m: 0.9883 - val_loss: 0.0642 - val_f1_m: 1.0000\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 790us/step - loss: 0.1091 - f1_m: 0.9900 - val_loss: 0.0629 - val_f1_m: 0.9961\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 771us/step - loss: 0.0990 - f1_m: 0.9918 - val_loss: 0.0590 - val_f1_m: 0.9961\n",
      "0.9917500019073486\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5756 - f1_m: 0.6960 - val_loss: 0.2828 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 784us/step - loss: 0.4077 - f1_m: 0.7720 - val_loss: 0.1966 - val_f1_m: 0.9990\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 798us/step - loss: 0.3097 - f1_m: 0.9005 - val_loss: 0.1454 - val_f1_m: 0.9941\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 785us/step - loss: 0.2447 - f1_m: 0.9492 - val_loss: 0.1220 - val_f1_m: 0.9902\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 792us/step - loss: 0.1986 - f1_m: 0.9695 - val_loss: 0.0985 - val_f1_m: 0.9922\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 797us/step - loss: 0.1667 - f1_m: 0.9793 - val_loss: 0.0777 - val_f1_m: 0.9971\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 769us/step - loss: 0.1430 - f1_m: 0.9860 - val_loss: 0.0721 - val_f1_m: 0.9961\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 786us/step - loss: 0.1251 - f1_m: 0.9890 - val_loss: 0.0650 - val_f1_m: 0.9961\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 793us/step - loss: 0.1113 - f1_m: 0.9895 - val_loss: 0.0612 - val_f1_m: 0.9941\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 781us/step - loss: 0.1004 - f1_m: 0.9920 - val_loss: 0.0573 - val_f1_m: 0.9922\n",
      "0.9919999837875366\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5711 - f1_m: 0.6942 - val_loss: 0.2658 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 800us/step - loss: 0.4046 - f1_m: 0.7660 - val_loss: 0.1995 - val_f1_m: 0.9990\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 763us/step - loss: 0.3047 - f1_m: 0.9025 - val_loss: 0.1647 - val_f1_m: 0.9854\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 759us/step - loss: 0.2396 - f1_m: 0.9485 - val_loss: 0.1297 - val_f1_m: 0.9893\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 763us/step - loss: 0.1953 - f1_m: 0.9688 - val_loss: 0.1138 - val_f1_m: 0.9873\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 762us/step - loss: 0.1639 - f1_m: 0.9812 - val_loss: 0.0976 - val_f1_m: 0.9883\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 739us/step - loss: 0.1413 - f1_m: 0.9862 - val_loss: 0.0905 - val_f1_m: 0.9883\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 771us/step - loss: 0.1245 - f1_m: 0.9915 - val_loss: 0.0799 - val_f1_m: 0.9854\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 761us/step - loss: 0.1111 - f1_m: 0.9890 - val_loss: 0.0661 - val_f1_m: 0.9951\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 762us/step - loss: 0.1007 - f1_m: 0.9925 - val_loss: 0.0746 - val_f1_m: 0.9775\n",
      "0.9925000071525574\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5697 - f1_m: 0.6980 - val_loss: 0.2806 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 772us/step - loss: 0.4033 - f1_m: 0.7650 - val_loss: 0.2052 - val_f1_m: 0.9990\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 770us/step - loss: 0.3036 - f1_m: 0.9032 - val_loss: 0.1651 - val_f1_m: 0.9922\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 778us/step - loss: 0.2381 - f1_m: 0.9455 - val_loss: 0.1447 - val_f1_m: 0.9824\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 777us/step - loss: 0.1935 - f1_m: 0.9680 - val_loss: 0.1283 - val_f1_m: 0.9834\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 778us/step - loss: 0.1620 - f1_m: 0.9793 - val_loss: 0.1047 - val_f1_m: 0.9893\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 728us/step - loss: 0.1393 - f1_m: 0.9875 - val_loss: 0.0932 - val_f1_m: 0.9912\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 803us/step - loss: 0.1225 - f1_m: 0.9895 - val_loss: 0.0884 - val_f1_m: 0.9883\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 752us/step - loss: 0.1094 - f1_m: 0.9927 - val_loss: 0.0790 - val_f1_m: 0.9893\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 786us/step - loss: 0.0993 - f1_m: 0.9918 - val_loss: 0.0756 - val_f1_m: 0.9873\n",
      "0.9917500019073486\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.5728 - f1_m: 0.6965 - val_loss: 0.2842 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 796us/step - loss: 0.4047 - f1_m: 0.7670 - val_loss: 0.2068 - val_f1_m: 0.9971\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 785us/step - loss: 0.3028 - f1_m: 0.9077 - val_loss: 0.1544 - val_f1_m: 0.9922\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 785us/step - loss: 0.2362 - f1_m: 0.9548 - val_loss: 0.1222 - val_f1_m: 0.9932\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 790us/step - loss: 0.1914 - f1_m: 0.9705 - val_loss: 0.1156 - val_f1_m: 0.9824\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 769us/step - loss: 0.1599 - f1_m: 0.9805 - val_loss: 0.0967 - val_f1_m: 0.9863\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 787us/step - loss: 0.1375 - f1_m: 0.9837 - val_loss: 0.0749 - val_f1_m: 0.9951\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 782us/step - loss: 0.1208 - f1_m: 0.9883 - val_loss: 0.0822 - val_f1_m: 0.9844\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 773us/step - loss: 0.1082 - f1_m: 0.9893 - val_loss: 0.0642 - val_f1_m: 0.9932\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 768us/step - loss: 0.0977 - f1_m: 0.9910 - val_loss: 0.0659 - val_f1_m: 0.9863\n",
      "0.9909999966621399\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.5741 - f1_m: 0.6980 - val_loss: 0.2852 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 787us/step - loss: 0.4043 - f1_m: 0.7765 - val_loss: 0.2031 - val_f1_m: 0.9971\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 781us/step - loss: 0.3042 - f1_m: 0.9087 - val_loss: 0.1561 - val_f1_m: 0.9912\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 790us/step - loss: 0.2379 - f1_m: 0.9490 - val_loss: 0.1421 - val_f1_m: 0.9766\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 777us/step - loss: 0.1932 - f1_m: 0.9710 - val_loss: 0.1056 - val_f1_m: 0.9932\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 782us/step - loss: 0.1615 - f1_m: 0.9810 - val_loss: 0.0974 - val_f1_m: 0.9902\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 771us/step - loss: 0.1388 - f1_m: 0.9845 - val_loss: 0.0889 - val_f1_m: 0.9902\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 776us/step - loss: 0.1220 - f1_m: 0.9890 - val_loss: 0.0772 - val_f1_m: 0.9951\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 784us/step - loss: 0.1090 - f1_m: 0.9900 - val_loss: 0.0753 - val_f1_m: 0.9922\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 774us/step - loss: 0.0985 - f1_m: 0.9898 - val_loss: 0.0753 - val_f1_m: 0.9873\n",
      "0.9897500276565552\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.5743 - f1_m: 0.6975 - val_loss: 0.2763 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 797us/step - loss: 0.4063 - f1_m: 0.7665 - val_loss: 0.1939 - val_f1_m: 1.0000\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 792us/step - loss: 0.3069 - f1_m: 0.9007 - val_loss: 0.1669 - val_f1_m: 0.9834\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 786us/step - loss: 0.2414 - f1_m: 0.9517 - val_loss: 0.1261 - val_f1_m: 0.9883\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 786us/step - loss: 0.1963 - f1_m: 0.9690 - val_loss: 0.1162 - val_f1_m: 0.9805\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 792us/step - loss: 0.1641 - f1_m: 0.9793 - val_loss: 0.0953 - val_f1_m: 0.9902\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 796us/step - loss: 0.1413 - f1_m: 0.9860 - val_loss: 0.0914 - val_f1_m: 0.9824\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 781us/step - loss: 0.1234 - f1_m: 0.9883 - val_loss: 0.0769 - val_f1_m: 0.9902\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 907us/step - loss: 0.1099 - f1_m: 0.9902 - val_loss: 0.0692 - val_f1_m: 0.9902\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 772us/step - loss: 0.0992 - f1_m: 0.9900 - val_loss: 0.0622 - val_f1_m: 0.9922\n",
      "0.9900000095367432\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5724 - f1_m: 0.6980 - val_loss: 0.2800 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 987us/step - loss: 0.4030 - f1_m: 0.7710 - val_loss: 0.1971 - val_f1_m: 1.0000\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 915us/step - loss: 0.3030 - f1_m: 0.9055 - val_loss: 0.1588 - val_f1_m: 0.9932\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 882us/step - loss: 0.2374 - f1_m: 0.9487 - val_loss: 0.1455 - val_f1_m: 0.9805\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 847us/step - loss: 0.1926 - f1_m: 0.9693 - val_loss: 0.1212 - val_f1_m: 0.9844\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 797us/step - loss: 0.1612 - f1_m: 0.9815 - val_loss: 0.0953 - val_f1_m: 0.9941\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 809us/step - loss: 0.1390 - f1_m: 0.9847 - val_loss: 0.0947 - val_f1_m: 0.9873\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 802us/step - loss: 0.1219 - f1_m: 0.9900 - val_loss: 0.0889 - val_f1_m: 0.9844\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 917us/step - loss: 0.1089 - f1_m: 0.9900 - val_loss: 0.0656 - val_f1_m: 0.9980\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 919us/step - loss: 0.0985 - f1_m: 0.9908 - val_loss: 0.0663 - val_f1_m: 0.9941\n",
      "0.9907500147819519\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.5769 - f1_m: 0.6950 - val_loss: 0.2843 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 908us/step - loss: 0.4061 - f1_m: 0.7797 - val_loss: 0.2055 - val_f1_m: 0.9971\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 798us/step - loss: 0.3055 - f1_m: 0.9000 - val_loss: 0.1537 - val_f1_m: 0.9932\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 797us/step - loss: 0.2403 - f1_m: 0.9487 - val_loss: 0.1205 - val_f1_m: 0.9941\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 784us/step - loss: 0.1955 - f1_m: 0.9647 - val_loss: 0.1102 - val_f1_m: 0.9883\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 787us/step - loss: 0.1643 - f1_m: 0.9793 - val_loss: 0.0897 - val_f1_m: 0.9941\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 910us/step - loss: 0.1417 - f1_m: 0.9847 - val_loss: 0.0761 - val_f1_m: 0.9941\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 947us/step - loss: 0.1249 - f1_m: 0.9847 - val_loss: 0.0667 - val_f1_m: 0.9951\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 837us/step - loss: 0.1120 - f1_m: 0.9885 - val_loss: 0.0596 - val_f1_m: 0.9951\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 775us/step - loss: 0.1017 - f1_m: 0.9890 - val_loss: 0.0555 - val_f1_m: 0.9951\n",
      "0.9890000224113464\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.5693 - f1_m: 0.6972 - val_loss: 0.2649 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 878us/step - loss: 0.4065 - f1_m: 0.7657 - val_loss: 0.1935 - val_f1_m: 0.9990\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 868us/step - loss: 0.3063 - f1_m: 0.9020 - val_loss: 0.1555 - val_f1_m: 0.9980\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 809us/step - loss: 0.2408 - f1_m: 0.9473 - val_loss: 0.1250 - val_f1_m: 0.9961\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 797us/step - loss: 0.1958 - f1_m: 0.9682 - val_loss: 0.1106 - val_f1_m: 0.9941\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 790us/step - loss: 0.1636 - f1_m: 0.9830 - val_loss: 0.0964 - val_f1_m: 0.9941\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 840us/step - loss: 0.1402 - f1_m: 0.9870 - val_loss: 0.0823 - val_f1_m: 0.9951\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 914us/step - loss: 0.1229 - f1_m: 0.9893 - val_loss: 0.0788 - val_f1_m: 0.9902\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 892us/step - loss: 0.1096 - f1_m: 0.9918 - val_loss: 0.0666 - val_f1_m: 0.9941\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 896us/step - loss: 0.0991 - f1_m: 0.9918 - val_loss: 0.0662 - val_f1_m: 0.9893\n",
      "0.9917500019073486\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 1s 3ms/step - loss: 0.5728 - f1_m: 0.6962 - val_loss: 0.2750 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 871us/step - loss: 0.4003 - f1_m: 0.7745 - val_loss: 0.1948 - val_f1_m: 0.9990\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 818us/step - loss: 0.3003 - f1_m: 0.9190 - val_loss: 0.1538 - val_f1_m: 0.9893\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 820us/step - loss: 0.2354 - f1_m: 0.9530 - val_loss: 0.1267 - val_f1_m: 0.9902\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 796us/step - loss: 0.1907 - f1_m: 0.9718 - val_loss: 0.1122 - val_f1_m: 0.9863\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 797us/step - loss: 0.1589 - f1_m: 0.9810 - val_loss: 0.0968 - val_f1_m: 0.9863\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 784us/step - loss: 0.1362 - f1_m: 0.9860 - val_loss: 0.0817 - val_f1_m: 0.9922\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 773us/step - loss: 0.1191 - f1_m: 0.9887 - val_loss: 0.0782 - val_f1_m: 0.9873\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 777us/step - loss: 0.1057 - f1_m: 0.9905 - val_loss: 0.0725 - val_f1_m: 0.9883\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 777us/step - loss: 0.0954 - f1_m: 0.9905 - val_loss: 0.0668 - val_f1_m: 0.9893\n",
      "0.9904999732971191\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5715 - f1_m: 0.6955 - val_loss: 0.2640 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 791us/step - loss: 0.4065 - f1_m: 0.7642 - val_loss: 0.1959 - val_f1_m: 0.9980\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 794us/step - loss: 0.3074 - f1_m: 0.9042 - val_loss: 0.1473 - val_f1_m: 0.9922\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 775us/step - loss: 0.2430 - f1_m: 0.9465 - val_loss: 0.1329 - val_f1_m: 0.9805\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 771us/step - loss: 0.1984 - f1_m: 0.9702 - val_loss: 0.1084 - val_f1_m: 0.9854\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 785us/step - loss: 0.1671 - f1_m: 0.9787 - val_loss: 0.0890 - val_f1_m: 0.9893\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 782us/step - loss: 0.1438 - f1_m: 0.9862 - val_loss: 0.0898 - val_f1_m: 0.9824\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 777us/step - loss: 0.1267 - f1_m: 0.9912 - val_loss: 0.0677 - val_f1_m: 0.9951\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 778us/step - loss: 0.1135 - f1_m: 0.9912 - val_loss: 0.0674 - val_f1_m: 0.9922\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 771us/step - loss: 0.1028 - f1_m: 0.9925 - val_loss: 0.0615 - val_f1_m: 0.9941\n",
      "0.9925000071525574\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5706 - f1_m: 0.6995 - val_loss: 0.2841 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 782us/step - loss: 0.3999 - f1_m: 0.7725 - val_loss: 0.2154 - val_f1_m: 0.9990\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 784us/step - loss: 0.3004 - f1_m: 0.9157 - val_loss: 0.1572 - val_f1_m: 0.9961\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - 0s 773us/step - loss: 0.2355 - f1_m: 0.9532 - val_loss: 0.1324 - val_f1_m: 0.9922\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 836us/step - loss: 0.1911 - f1_m: 0.9707 - val_loss: 0.1190 - val_f1_m: 0.9883\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 838us/step - loss: 0.1598 - f1_m: 0.9822 - val_loss: 0.0862 - val_f1_m: 0.9971\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 781us/step - loss: 0.1380 - f1_m: 0.9840 - val_loss: 0.0758 - val_f1_m: 0.9971\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 832us/step - loss: 0.1214 - f1_m: 0.9868 - val_loss: 0.0781 - val_f1_m: 0.9922\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 802us/step - loss: 0.1086 - f1_m: 0.9900 - val_loss: 0.0689 - val_f1_m: 0.9922\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 790us/step - loss: 0.0986 - f1_m: 0.9915 - val_loss: 0.0593 - val_f1_m: 0.9951\n",
      "0.9915000200271606\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.5706 - f1_m: 0.6967 - val_loss: 0.2687 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 817us/step - loss: 0.4041 - f1_m: 0.7775 - val_loss: 0.1837 - val_f1_m: 1.0000\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 804us/step - loss: 0.3044 - f1_m: 0.9015 - val_loss: 0.1505 - val_f1_m: 0.9922\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 810us/step - loss: 0.2383 - f1_m: 0.9563 - val_loss: 0.1141 - val_f1_m: 0.9951\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 805us/step - loss: 0.1932 - f1_m: 0.9710 - val_loss: 0.1067 - val_f1_m: 0.9893\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 795us/step - loss: 0.1613 - f1_m: 0.9835 - val_loss: 0.0829 - val_f1_m: 0.9961\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 785us/step - loss: 0.1380 - f1_m: 0.9880 - val_loss: 0.0698 - val_f1_m: 0.9980\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 822us/step - loss: 0.1205 - f1_m: 0.9910 - val_loss: 0.0572 - val_f1_m: 0.9980\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 853us/step - loss: 0.1069 - f1_m: 0.9918 - val_loss: 0.0630 - val_f1_m: 0.9912\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 800us/step - loss: 0.0963 - f1_m: 0.9940 - val_loss: 0.0563 - val_f1_m: 0.9922\n",
      "0.9940000176429749\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5761 - f1_m: 0.6965 - val_loss: 0.2753 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 831us/step - loss: 0.4064 - f1_m: 0.7710 - val_loss: 0.1924 - val_f1_m: 1.0000\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 821us/step - loss: 0.3056 - f1_m: 0.9075 - val_loss: 0.1606 - val_f1_m: 0.9805\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 793us/step - loss: 0.2404 - f1_m: 0.9520 - val_loss: 0.1257 - val_f1_m: 0.9824\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 815us/step - loss: 0.1962 - f1_m: 0.9695 - val_loss: 0.1067 - val_f1_m: 0.9834\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 795us/step - loss: 0.1651 - f1_m: 0.9772 - val_loss: 0.0899 - val_f1_m: 0.9893\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 767us/step - loss: 0.1424 - f1_m: 0.9820 - val_loss: 0.0954 - val_f1_m: 0.9746\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 765us/step - loss: 0.1259 - f1_m: 0.9835 - val_loss: 0.0641 - val_f1_m: 0.9980\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 769us/step - loss: 0.1125 - f1_m: 0.9862 - val_loss: 0.0661 - val_f1_m: 0.9932\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.1024 - f1_m: 0.9887 - val_loss: 0.0592 - val_f1_m: 0.9951\n",
      "0.9887499809265137\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.5743 - f1_m: 0.6972 - val_loss: 0.2863 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.4027 - f1_m: 0.7750 - val_loss: 0.1987 - val_f1_m: 1.0000\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 953us/step - loss: 0.3016 - f1_m: 0.9067 - val_loss: 0.1623 - val_f1_m: 0.9912\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 959us/step - loss: 0.2362 - f1_m: 0.9552 - val_loss: 0.1276 - val_f1_m: 0.9922\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 888us/step - loss: 0.1915 - f1_m: 0.9710 - val_loss: 0.1117 - val_f1_m: 0.9922\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 828us/step - loss: 0.1604 - f1_m: 0.9810 - val_loss: 0.1000 - val_f1_m: 0.9922\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.1377 - f1_m: 0.9890 - val_loss: 0.0920 - val_f1_m: 0.9883\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 829us/step - loss: 0.1207 - f1_m: 0.9910 - val_loss: 0.0679 - val_f1_m: 0.9971\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 889us/step - loss: 0.1077 - f1_m: 0.9920 - val_loss: 0.0756 - val_f1_m: 0.9883\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 830us/step - loss: 0.0973 - f1_m: 0.9923 - val_loss: 0.0587 - val_f1_m: 0.9961\n",
      "0.9922500252723694\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.5726 - f1_m: 0.6947 - val_loss: 0.2795 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 798us/step - loss: 0.4030 - f1_m: 0.7772 - val_loss: 0.1977 - val_f1_m: 0.9990\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 780us/step - loss: 0.3047 - f1_m: 0.9032 - val_loss: 0.1626 - val_f1_m: 0.9893\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 880us/step - loss: 0.2394 - f1_m: 0.9522 - val_loss: 0.1392 - val_f1_m: 0.9824\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.1943 - f1_m: 0.9678 - val_loss: 0.1192 - val_f1_m: 0.9824\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 847us/step - loss: 0.1627 - f1_m: 0.9827 - val_loss: 0.0891 - val_f1_m: 0.9922\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 941us/step - loss: 0.1399 - f1_m: 0.9862 - val_loss: 0.0864 - val_f1_m: 0.9902\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 923us/step - loss: 0.1232 - f1_m: 0.9902 - val_loss: 0.0816 - val_f1_m: 0.9893\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 962us/step - loss: 0.1100 - f1_m: 0.9915 - val_loss: 0.0656 - val_f1_m: 0.9941\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 888us/step - loss: 0.0999 - f1_m: 0.9900 - val_loss: 0.0611 - val_f1_m: 0.9941\n",
      "0.9900000095367432\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.5732 - f1_m: 0.6922 - val_loss: 0.2840 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 837us/step - loss: 0.4019 - f1_m: 0.7770 - val_loss: 0.1991 - val_f1_m: 1.0000\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 920us/step - loss: 0.3032 - f1_m: 0.9080 - val_loss: 0.1563 - val_f1_m: 0.9932\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 865us/step - loss: 0.2390 - f1_m: 0.9500 - val_loss: 0.1399 - val_f1_m: 0.9873\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 806us/step - loss: 0.1946 - f1_m: 0.9720 - val_loss: 0.1204 - val_f1_m: 0.9912\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 986us/step - loss: 0.1627 - f1_m: 0.9833 - val_loss: 0.0946 - val_f1_m: 0.9961\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 847us/step - loss: 0.1397 - f1_m: 0.9885 - val_loss: 0.0865 - val_f1_m: 0.9951\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 808us/step - loss: 0.1222 - f1_m: 0.9908 - val_loss: 0.0752 - val_f1_m: 0.9980\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 800us/step - loss: 0.1089 - f1_m: 0.9920 - val_loss: 0.0681 - val_f1_m: 0.9980\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 797us/step - loss: 0.0984 - f1_m: 0.9925 - val_loss: 0.0705 - val_f1_m: 0.9883\n",
      "0.9925000071525574\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.5734 - f1_m: 0.6982 - val_loss: 0.2823 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 818us/step - loss: 0.4051 - f1_m: 0.7767 - val_loss: 0.2001 - val_f1_m: 0.9971\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 819us/step - loss: 0.3070 - f1_m: 0.8967 - val_loss: 0.1544 - val_f1_m: 0.9932\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 808us/step - loss: 0.2414 - f1_m: 0.9445 - val_loss: 0.1285 - val_f1_m: 0.9932\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 827us/step - loss: 0.1960 - f1_m: 0.9705 - val_loss: 0.1025 - val_f1_m: 0.9980\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 830us/step - loss: 0.1645 - f1_m: 0.9770 - val_loss: 0.1048 - val_f1_m: 0.9873\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 846us/step - loss: 0.1414 - f1_m: 0.9847 - val_loss: 0.0981 - val_f1_m: 0.9834\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 795us/step - loss: 0.1245 - f1_m: 0.9877 - val_loss: 0.0833 - val_f1_m: 0.9941\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 788us/step - loss: 0.1116 - f1_m: 0.9898 - val_loss: 0.0720 - val_f1_m: 0.9961\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 788us/step - loss: 0.1009 - f1_m: 0.9883 - val_loss: 0.0769 - val_f1_m: 0.9814\n",
      "0.9882500171661377\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5736 - f1_m: 0.6965 - val_loss: 0.2856 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 828us/step - loss: 0.4037 - f1_m: 0.7687 - val_loss: 0.2027 - val_f1_m: 1.0000\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 838us/step - loss: 0.3037 - f1_m: 0.9045 - val_loss: 0.1600 - val_f1_m: 0.9912\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 823us/step - loss: 0.2389 - f1_m: 0.9505 - val_loss: 0.1172 - val_f1_m: 0.9990\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 874us/step - loss: 0.1947 - f1_m: 0.9670 - val_loss: 0.1029 - val_f1_m: 0.9941\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 843us/step - loss: 0.1632 - f1_m: 0.9818 - val_loss: 0.0853 - val_f1_m: 0.9971\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 826us/step - loss: 0.1404 - f1_m: 0.9862 - val_loss: 0.0796 - val_f1_m: 0.9951\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 799us/step - loss: 0.1236 - f1_m: 0.9905 - val_loss: 0.0711 - val_f1_m: 0.9961\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 810us/step - loss: 0.1107 - f1_m: 0.9877 - val_loss: 0.0617 - val_f1_m: 0.9971\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 777us/step - loss: 0.1002 - f1_m: 0.9912 - val_loss: 0.0644 - val_f1_m: 0.9932\n",
      "0.9912499785423279\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.5711 - f1_m: 0.6962 - val_loss: 0.2690 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 801us/step - loss: 0.4025 - f1_m: 0.7765 - val_loss: 0.2037 - val_f1_m: 0.9990\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 821us/step - loss: 0.3029 - f1_m: 0.9035 - val_loss: 0.1528 - val_f1_m: 0.9980\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 820us/step - loss: 0.2381 - f1_m: 0.9477 - val_loss: 0.1243 - val_f1_m: 0.9980\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 803us/step - loss: 0.1937 - f1_m: 0.9670 - val_loss: 0.1167 - val_f1_m: 0.9902\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 809us/step - loss: 0.1626 - f1_m: 0.9760 - val_loss: 0.0941 - val_f1_m: 0.9980\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 799us/step - loss: 0.1401 - f1_m: 0.9840 - val_loss: 0.0819 - val_f1_m: 0.9980\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 811us/step - loss: 0.1232 - f1_m: 0.9865 - val_loss: 0.0769 - val_f1_m: 0.9971\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 780us/step - loss: 0.1102 - f1_m: 0.9893 - val_loss: 0.0737 - val_f1_m: 0.9932\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 811us/step - loss: 0.0996 - f1_m: 0.9908 - val_loss: 0.0678 - val_f1_m: 0.9912\n",
      "0.9907500147819519\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.5718 - f1_m: 0.6960 - val_loss: 0.2751 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 868us/step - loss: 0.4017 - f1_m: 0.7775 - val_loss: 0.2089 - val_f1_m: 0.9941\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 796us/step - loss: 0.3025 - f1_m: 0.9080 - val_loss: 0.1469 - val_f1_m: 0.9941\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 779us/step - loss: 0.2391 - f1_m: 0.9470 - val_loss: 0.1378 - val_f1_m: 0.9834\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 792us/step - loss: 0.1956 - f1_m: 0.9670 - val_loss: 0.1160 - val_f1_m: 0.9834\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 893us/step - loss: 0.1650 - f1_m: 0.9810 - val_loss: 0.0998 - val_f1_m: 0.9873\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 852us/step - loss: 0.1427 - f1_m: 0.9855 - val_loss: 0.0893 - val_f1_m: 0.9883\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 844us/step - loss: 0.1260 - f1_m: 0.9868 - val_loss: 0.0772 - val_f1_m: 0.9922\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 788us/step - loss: 0.1128 - f1_m: 0.9915 - val_loss: 0.0717 - val_f1_m: 0.9922\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 794us/step - loss: 0.1027 - f1_m: 0.9902 - val_loss: 0.0659 - val_f1_m: 0.9941\n",
      "0.9902499914169312\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5715 - f1_m: 0.6975 - val_loss: 0.2787 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 788us/step - loss: 0.4014 - f1_m: 0.7775 - val_loss: 0.2118 - val_f1_m: 0.9990\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 800us/step - loss: 0.3026 - f1_m: 0.9120 - val_loss: 0.1538 - val_f1_m: 0.9951\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 923us/step - loss: 0.2394 - f1_m: 0.9505 - val_loss: 0.1130 - val_f1_m: 0.9961\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 876us/step - loss: 0.1957 - f1_m: 0.9688 - val_loss: 0.1079 - val_f1_m: 0.9902\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 845us/step - loss: 0.1646 - f1_m: 0.9800 - val_loss: 0.0975 - val_f1_m: 0.9863\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 826us/step - loss: 0.1421 - f1_m: 0.9850 - val_loss: 0.0807 - val_f1_m: 0.9912\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 811us/step - loss: 0.1250 - f1_m: 0.9900 - val_loss: 0.0682 - val_f1_m: 0.9922\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 805us/step - loss: 0.1120 - f1_m: 0.9905 - val_loss: 0.0558 - val_f1_m: 0.9980\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 812us/step - loss: 0.1016 - f1_m: 0.9900 - val_loss: 0.0442 - val_f1_m: 0.9990\n",
      "0.9900000095367432\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.5714 - f1_m: 0.6990 - val_loss: 0.2770 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 798us/step - loss: 0.4018 - f1_m: 0.7757 - val_loss: 0.1833 - val_f1_m: 1.0000\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 799us/step - loss: 0.3032 - f1_m: 0.9050 - val_loss: 0.1484 - val_f1_m: 0.9922\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 817us/step - loss: 0.2386 - f1_m: 0.9503 - val_loss: 0.1220 - val_f1_m: 0.9902\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 812us/step - loss: 0.1944 - f1_m: 0.9685 - val_loss: 0.1006 - val_f1_m: 0.9932\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 830us/step - loss: 0.1631 - f1_m: 0.9765 - val_loss: 0.1076 - val_f1_m: 0.9805\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 823us/step - loss: 0.1413 - f1_m: 0.9855 - val_loss: 0.0730 - val_f1_m: 0.9980\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 792us/step - loss: 0.1242 - f1_m: 0.9880 - val_loss: 0.0713 - val_f1_m: 0.9951\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 814us/step - loss: 0.1112 - f1_m: 0.9902 - val_loss: 0.0590 - val_f1_m: 0.9990\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 806us/step - loss: 0.1013 - f1_m: 0.9895 - val_loss: 0.0593 - val_f1_m: 0.9951\n",
      "0.9894999861717224\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5734 - f1_m: 0.6980 - val_loss: 0.2728 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - 0s 854us/step - loss: 0.4028 - f1_m: 0.7775 - val_loss: 0.2009 - val_f1_m: 1.0000\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 862us/step - loss: 0.3013 - f1_m: 0.9075 - val_loss: 0.1632 - val_f1_m: 0.9873\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 797us/step - loss: 0.2357 - f1_m: 0.9535 - val_loss: 0.1309 - val_f1_m: 0.9883\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 852us/step - loss: 0.1910 - f1_m: 0.9735 - val_loss: 0.1056 - val_f1_m: 0.9912\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 796us/step - loss: 0.1598 - f1_m: 0.9803 - val_loss: 0.0929 - val_f1_m: 0.9912\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 801us/step - loss: 0.1375 - f1_m: 0.9850 - val_loss: 0.0842 - val_f1_m: 0.9902\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 799us/step - loss: 0.1206 - f1_m: 0.9893 - val_loss: 0.0779 - val_f1_m: 0.9883\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 781us/step - loss: 0.1077 - f1_m: 0.9893 - val_loss: 0.0682 - val_f1_m: 0.9941\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 816us/step - loss: 0.0975 - f1_m: 0.9902 - val_loss: 0.0713 - val_f1_m: 0.9854\n",
      "0.9902499914169312\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.5707 - f1_m: 0.6980 - val_loss: 0.2686 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 794us/step - loss: 0.4050 - f1_m: 0.7645 - val_loss: 0.1881 - val_f1_m: 1.0000\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 789us/step - loss: 0.3054 - f1_m: 0.9027 - val_loss: 0.1634 - val_f1_m: 0.9727\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 794us/step - loss: 0.2403 - f1_m: 0.9498 - val_loss: 0.1286 - val_f1_m: 0.9795\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 829us/step - loss: 0.1954 - f1_m: 0.9677 - val_loss: 0.0990 - val_f1_m: 0.9902\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 785us/step - loss: 0.1639 - f1_m: 0.9790 - val_loss: 0.0939 - val_f1_m: 0.9854\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 784us/step - loss: 0.1406 - f1_m: 0.9885 - val_loss: 0.0708 - val_f1_m: 0.9971\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 813us/step - loss: 0.1234 - f1_m: 0.9880 - val_loss: 0.0783 - val_f1_m: 0.9824\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 819us/step - loss: 0.1104 - f1_m: 0.9887 - val_loss: 0.0662 - val_f1_m: 0.9902\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 779us/step - loss: 0.0997 - f1_m: 0.9900 - val_loss: 0.0603 - val_f1_m: 0.9902\n",
      "0.9900000095367432\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 1s 2ms/step - loss: 0.5745 - f1_m: 0.6962 - val_loss: 0.2832 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 817us/step - loss: 0.4070 - f1_m: 0.7682 - val_loss: 0.1972 - val_f1_m: 1.0000\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 810us/step - loss: 0.3088 - f1_m: 0.9020 - val_loss: 0.1546 - val_f1_m: 0.9971\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 813us/step - loss: 0.2442 - f1_m: 0.9423 - val_loss: 0.1332 - val_f1_m: 0.9932\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 844us/step - loss: 0.1991 - f1_m: 0.9640 - val_loss: 0.1067 - val_f1_m: 0.9971\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 802us/step - loss: 0.1669 - f1_m: 0.9737 - val_loss: 0.1017 - val_f1_m: 0.9941\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 774us/step - loss: 0.1435 - f1_m: 0.9858 - val_loss: 0.0823 - val_f1_m: 0.9980\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 792us/step - loss: 0.1259 - f1_m: 0.9893 - val_loss: 0.0709 - val_f1_m: 1.0000\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 784us/step - loss: 0.1125 - f1_m: 0.9898 - val_loss: 0.0682 - val_f1_m: 0.9971\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 822us/step - loss: 0.1016 - f1_m: 0.9915 - val_loss: 0.0659 - val_f1_m: 0.9951\n",
      "0.9915000200271606\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.5707 - f1_m: 0.6977 - val_loss: 0.2801 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 799us/step - loss: 0.4041 - f1_m: 0.7697 - val_loss: 0.1959 - val_f1_m: 1.0000\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 791us/step - loss: 0.3047 - f1_m: 0.9057 - val_loss: 0.1540 - val_f1_m: 0.9932\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 938us/step - loss: 0.2399 - f1_m: 0.9437 - val_loss: 0.1358 - val_f1_m: 0.9883\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 811us/step - loss: 0.1956 - f1_m: 0.9685 - val_loss: 0.1090 - val_f1_m: 0.9932\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 835us/step - loss: 0.1649 - f1_m: 0.9808 - val_loss: 0.0999 - val_f1_m: 0.9912\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 807us/step - loss: 0.1422 - f1_m: 0.9870 - val_loss: 0.1003 - val_f1_m: 0.9844\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 797us/step - loss: 0.1252 - f1_m: 0.9905 - val_loss: 0.0871 - val_f1_m: 0.9893\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 819us/step - loss: 0.1126 - f1_m: 0.9887 - val_loss: 0.0779 - val_f1_m: 0.9941\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 996us/step - loss: 0.1016 - f1_m: 0.9908 - val_loss: 0.0660 - val_f1_m: 0.9971\n",
      "0.9907500147819519\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.5696 - f1_m: 0.6962 - val_loss: 0.2662 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 817us/step - loss: 0.4056 - f1_m: 0.7697 - val_loss: 0.2057 - val_f1_m: 1.0000\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 962us/step - loss: 0.3077 - f1_m: 0.8997 - val_loss: 0.1584 - val_f1_m: 0.9961\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 790us/step - loss: 0.2430 - f1_m: 0.9488 - val_loss: 0.1396 - val_f1_m: 0.9883\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 852us/step - loss: 0.1986 - f1_m: 0.9668 - val_loss: 0.1252 - val_f1_m: 0.9844\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 810us/step - loss: 0.1669 - f1_m: 0.9797 - val_loss: 0.1031 - val_f1_m: 0.9902\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 811us/step - loss: 0.1441 - f1_m: 0.9870 - val_loss: 0.0872 - val_f1_m: 0.9941\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 779us/step - loss: 0.1268 - f1_m: 0.9885 - val_loss: 0.0739 - val_f1_m: 0.9980\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 773us/step - loss: 0.1132 - f1_m: 0.9898 - val_loss: 0.0743 - val_f1_m: 0.9873\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 792us/step - loss: 0.1027 - f1_m: 0.9902 - val_loss: 0.0660 - val_f1_m: 0.9932\n",
      "0.9902499914169312\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.5731 - f1_m: 0.6970 - val_loss: 0.2728 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 874us/step - loss: 0.4034 - f1_m: 0.7700 - val_loss: 0.1949 - val_f1_m: 0.9980\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 804us/step - loss: 0.3028 - f1_m: 0.9087 - val_loss: 0.1521 - val_f1_m: 0.9941\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 781us/step - loss: 0.2374 - f1_m: 0.9572 - val_loss: 0.1213 - val_f1_m: 0.9941\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 817us/step - loss: 0.1929 - f1_m: 0.9722 - val_loss: 0.1071 - val_f1_m: 0.9912\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 805us/step - loss: 0.1619 - f1_m: 0.9808 - val_loss: 0.0879 - val_f1_m: 0.9932\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 806us/step - loss: 0.1389 - f1_m: 0.9868 - val_loss: 0.0773 - val_f1_m: 0.9951\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 799us/step - loss: 0.1220 - f1_m: 0.9895 - val_loss: 0.0798 - val_f1_m: 0.9854\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 815us/step - loss: 0.1090 - f1_m: 0.9905 - val_loss: 0.0635 - val_f1_m: 0.9951\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 784us/step - loss: 0.0987 - f1_m: 0.9920 - val_loss: 0.0557 - val_f1_m: 0.9951\n",
      "0.9919999837875366\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.5716 - f1_m: 0.6955 - val_loss: 0.2842 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 796us/step - loss: 0.3996 - f1_m: 0.7795 - val_loss: 0.2039 - val_f1_m: 0.9980\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 822us/step - loss: 0.2982 - f1_m: 0.9122 - val_loss: 0.1763 - val_f1_m: 0.9795\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 840us/step - loss: 0.2334 - f1_m: 0.9548 - val_loss: 0.1424 - val_f1_m: 0.9824\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 806us/step - loss: 0.1890 - f1_m: 0.9695 - val_loss: 0.1208 - val_f1_m: 0.9863\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 786us/step - loss: 0.1580 - f1_m: 0.9797 - val_loss: 0.1034 - val_f1_m: 0.9893\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 828us/step - loss: 0.1359 - f1_m: 0.9877 - val_loss: 0.0858 - val_f1_m: 0.9912\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 834us/step - loss: 0.1196 - f1_m: 0.9898 - val_loss: 0.0861 - val_f1_m: 0.9873\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 817us/step - loss: 0.1066 - f1_m: 0.9898 - val_loss: 0.0662 - val_f1_m: 0.9951\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 796us/step - loss: 0.0971 - f1_m: 0.9895 - val_loss: 0.0671 - val_f1_m: 0.9932\n",
      "0.9894999861717224\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.5698 - f1_m: 0.6957 - val_loss: 0.2725 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 801us/step - loss: 0.4004 - f1_m: 0.7712 - val_loss: 0.2102 - val_f1_m: 0.9980\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 811us/step - loss: 0.2996 - f1_m: 0.9090 - val_loss: 0.1614 - val_f1_m: 0.9873\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 782us/step - loss: 0.2350 - f1_m: 0.9550 - val_loss: 0.1343 - val_f1_m: 0.9873\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 787us/step - loss: 0.1907 - f1_m: 0.9710 - val_loss: 0.1039 - val_f1_m: 0.9951\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 788us/step - loss: 0.1600 - f1_m: 0.9815 - val_loss: 0.1111 - val_f1_m: 0.9766\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 781us/step - loss: 0.1381 - f1_m: 0.9865 - val_loss: 0.1005 - val_f1_m: 0.9746\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 782us/step - loss: 0.1213 - f1_m: 0.9910 - val_loss: 0.0844 - val_f1_m: 0.9854\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 785us/step - loss: 0.1087 - f1_m: 0.9912 - val_loss: 0.0817 - val_f1_m: 0.9785\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 797us/step - loss: 0.0985 - f1_m: 0.9900 - val_loss: 0.0694 - val_f1_m: 0.9873\n",
      "0.9900000095367432\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5709 - f1_m: 0.6995 - val_loss: 0.2802 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 807us/step - loss: 0.4030 - f1_m: 0.7742 - val_loss: 0.2022 - val_f1_m: 1.0000\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 807us/step - loss: 0.3056 - f1_m: 0.9087 - val_loss: 0.1525 - val_f1_m: 0.9941\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 786us/step - loss: 0.2421 - f1_m: 0.9472 - val_loss: 0.1222 - val_f1_m: 0.9961\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 788us/step - loss: 0.1982 - f1_m: 0.9632 - val_loss: 0.1078 - val_f1_m: 0.9951\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 801us/step - loss: 0.1666 - f1_m: 0.9795 - val_loss: 0.1038 - val_f1_m: 0.9902\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 805us/step - loss: 0.1437 - f1_m: 0.9852 - val_loss: 0.0913 - val_f1_m: 0.9922\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 793us/step - loss: 0.1265 - f1_m: 0.9893 - val_loss: 0.0666 - val_f1_m: 0.9971\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 777us/step - loss: 0.1141 - f1_m: 0.9870 - val_loss: 0.0700 - val_f1_m: 0.9932\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 783us/step - loss: 0.1031 - f1_m: 0.9912 - val_loss: 0.0698 - val_f1_m: 0.9902\n",
      "0.9912499785423279\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5716 - f1_m: 0.6967 - val_loss: 0.2834 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 799us/step - loss: 0.3998 - f1_m: 0.7782 - val_loss: 0.2112 - val_f1_m: 0.9980\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 782us/step - loss: 0.2994 - f1_m: 0.9103 - val_loss: 0.1743 - val_f1_m: 0.9834\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 793us/step - loss: 0.2357 - f1_m: 0.9567 - val_loss: 0.1349 - val_f1_m: 0.9912\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 783us/step - loss: 0.1930 - f1_m: 0.9695 - val_loss: 0.1186 - val_f1_m: 0.9893\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 799us/step - loss: 0.1627 - f1_m: 0.9820 - val_loss: 0.1039 - val_f1_m: 0.9902\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 784us/step - loss: 0.1406 - f1_m: 0.9855 - val_loss: 0.0959 - val_f1_m: 0.9873\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 797us/step - loss: 0.1244 - f1_m: 0.9875 - val_loss: 0.0786 - val_f1_m: 0.9932\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 802us/step - loss: 0.1117 - f1_m: 0.9905 - val_loss: 0.0735 - val_f1_m: 0.9912\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 806us/step - loss: 0.1015 - f1_m: 0.9902 - val_loss: 0.0646 - val_f1_m: 0.9941\n",
      "0.9902499914169312\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5724 - f1_m: 0.6987 - val_loss: 0.2815 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 805us/step - loss: 0.4035 - f1_m: 0.7710 - val_loss: 0.2178 - val_f1_m: 0.9961\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 791us/step - loss: 0.3052 - f1_m: 0.9028 - val_loss: 0.1841 - val_f1_m: 0.9795\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 776us/step - loss: 0.2414 - f1_m: 0.9463 - val_loss: 0.1455 - val_f1_m: 0.9834\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 768us/step - loss: 0.1971 - f1_m: 0.9705 - val_loss: 0.1211 - val_f1_m: 0.9893\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 767us/step - loss: 0.1659 - f1_m: 0.9810 - val_loss: 0.1003 - val_f1_m: 0.9941\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 784us/step - loss: 0.1436 - f1_m: 0.9865 - val_loss: 0.1087 - val_f1_m: 0.9785\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 772us/step - loss: 0.1266 - f1_m: 0.9887 - val_loss: 0.0878 - val_f1_m: 0.9893\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 766us/step - loss: 0.1135 - f1_m: 0.9893 - val_loss: 0.0827 - val_f1_m: 0.9854\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 767us/step - loss: 0.1030 - f1_m: 0.9910 - val_loss: 0.0724 - val_f1_m: 0.9893\n",
      "0.9909999966621399\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5707 - f1_m: 0.6965 - val_loss: 0.2801 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 797us/step - loss: 0.4024 - f1_m: 0.7762 - val_loss: 0.2040 - val_f1_m: 0.9990\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 797us/step - loss: 0.3038 - f1_m: 0.9095 - val_loss: 0.1618 - val_f1_m: 0.9854\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 784us/step - loss: 0.2392 - f1_m: 0.9510 - val_loss: 0.1331 - val_f1_m: 0.9844\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 829us/step - loss: 0.1944 - f1_m: 0.9695 - val_loss: 0.1079 - val_f1_m: 0.9912\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 778us/step - loss: 0.1625 - f1_m: 0.9768 - val_loss: 0.0955 - val_f1_m: 0.9912\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 791us/step - loss: 0.1395 - f1_m: 0.9852 - val_loss: 0.0825 - val_f1_m: 0.9941\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 823us/step - loss: 0.1221 - f1_m: 0.9885 - val_loss: 0.0700 - val_f1_m: 0.9980\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 787us/step - loss: 0.1088 - f1_m: 0.9915 - val_loss: 0.0681 - val_f1_m: 0.9951\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - 0s 784us/step - loss: 0.0984 - f1_m: 0.9902 - val_loss: 0.0556 - val_f1_m: 0.9971\n",
      "0.9902499914169312\n",
      "if any\n",
      "[100]\n",
      "[0]\n",
      "[array([[-0.42580584, -0.07013362,  1.0006561 , -0.5905953 , -0.01767386,\n",
      "        -0.00771929, -0.5342802 , -0.47137925, -0.55544597, -0.54929185],\n",
      "       [ 0.64229506, -0.06516605, -0.7918179 ,  0.60629785, -0.06953891,\n",
      "         0.03103748,  0.71023005,  0.62538314,  0.8085791 ,  0.66716975],\n",
      "       [ 0.03397203, -0.10502755,  0.44382372,  0.0811562 , -0.04991733,\n",
      "        -0.03237156,  0.04417628,  0.07920366,  0.05415411,  0.08158582],\n",
      "       [ 0.09811223, -0.06410748,  0.44889215,  0.07268428, -0.00441315,\n",
      "        -0.08265713,  0.08124705,  0.05614668,  0.10723123,  0.06454743]],\n",
      "      dtype=float32), array([ 0.06053121, -0.0582438 ,  0.5846767 ,  0.16705962, -0.02366949,\n",
      "       -0.02923338,  0.11037496,  0.08999928,  0.08666178,  0.12237508],\n",
      "      dtype=float32), array([[-0.89834356,  1.345767  ],\n",
      "       [ 0.57428825,  0.00250948],\n",
      "       [ 0.887047  , -1.0630339 ],\n",
      "       [-1.2410891 ,  0.738496  ],\n",
      "       [ 0.09611318, -0.2876962 ],\n",
      "       [ 0.48401168,  0.18491079],\n",
      "       [-1.5033237 ,  0.38280663],\n",
      "       [-1.0757271 ,  0.92506474],\n",
      "       [-0.9181504 ,  0.62382185],\n",
      "       [-1.2215967 ,  0.62675554]], dtype=float32), array([ 0.11376112, -0.11376112], dtype=float32)]\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "yhn tk\n",
      "100\n",
      "Done\n",
      "   1/4375 [..............................] - ETA: 6:50 - loss: 0.3449 - accuracy: 0.8750"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ayushkv/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:123: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "/Users/ayushkv/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:124: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "/Users/ayushkv/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:130: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4375/4375 [==============================] - 2s 507us/step - loss: 0.3046 - accuracy: 0.9453\n",
      "1875/1875 [==============================] - 1s 509us/step - loss: 0.3060 - accuracy: 0.9452\n",
      "313/313 [==============================] - 0s 404us/step\n",
      "10000\n",
      "drift has been detecte models must be retrained\n",
      "finished\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5499 - f1_m: 0.7172 - val_loss: 0.2889 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 794us/step - loss: 0.3659 - f1_m: 0.8680 - val_loss: 0.1936 - val_f1_m: 0.9961\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 810us/step - loss: 0.2671 - f1_m: 0.9460 - val_loss: 0.1532 - val_f1_m: 0.9883\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 788us/step - loss: 0.2086 - f1_m: 0.9670 - val_loss: 0.1126 - val_f1_m: 0.9922\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 801us/step - loss: 0.1710 - f1_m: 0.9785 - val_loss: 0.0954 - val_f1_m: 0.9932\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 792us/step - loss: 0.1455 - f1_m: 0.9843 - val_loss: 0.0859 - val_f1_m: 0.9932\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 785us/step - loss: 0.1269 - f1_m: 0.9877 - val_loss: 0.0716 - val_f1_m: 0.9971\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 800us/step - loss: 0.1132 - f1_m: 0.9898 - val_loss: 0.0726 - val_f1_m: 0.9863\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 790us/step - loss: 0.1030 - f1_m: 0.9900 - val_loss: 0.0642 - val_f1_m: 0.9873\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 770us/step - loss: 0.0944 - f1_m: 0.9877 - val_loss: 0.0625 - val_f1_m: 0.9854\n",
      "0.9877499938011169\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5489 - f1_m: 0.7202 - val_loss: 0.2926 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 800us/step - loss: 0.3613 - f1_m: 0.8715 - val_loss: 0.2187 - val_f1_m: 0.9766\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 788us/step - loss: 0.2614 - f1_m: 0.9507 - val_loss: 0.1668 - val_f1_m: 0.9785\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 781us/step - loss: 0.2029 - f1_m: 0.9650 - val_loss: 0.1298 - val_f1_m: 0.9824\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 793us/step - loss: 0.1659 - f1_m: 0.9780 - val_loss: 0.1099 - val_f1_m: 0.9854\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 775us/step - loss: 0.1401 - f1_m: 0.9847 - val_loss: 0.0893 - val_f1_m: 0.9912\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 777us/step - loss: 0.1218 - f1_m: 0.9902 - val_loss: 0.0963 - val_f1_m: 0.9766\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 793us/step - loss: 0.1085 - f1_m: 0.9880 - val_loss: 0.0694 - val_f1_m: 0.9912\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.0977 - f1_m: 0.9915 - val_loss: 0.0654 - val_f1_m: 0.9912\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.0891 - f1_m: 0.9900 - val_loss: 0.0677 - val_f1_m: 0.9873\n",
      "0.9900000095367432\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.5483 - f1_m: 0.7167 - val_loss: 0.2905 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 821us/step - loss: 0.3659 - f1_m: 0.8670 - val_loss: 0.2007 - val_f1_m: 0.9932\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 802us/step - loss: 0.2682 - f1_m: 0.9390 - val_loss: 0.1636 - val_f1_m: 0.9727\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 782us/step - loss: 0.2102 - f1_m: 0.9635 - val_loss: 0.1168 - val_f1_m: 0.9941\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 786us/step - loss: 0.1724 - f1_m: 0.9750 - val_loss: 0.1067 - val_f1_m: 0.9844\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 777us/step - loss: 0.1465 - f1_m: 0.9835 - val_loss: 0.0805 - val_f1_m: 0.9961\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 789us/step - loss: 0.1280 - f1_m: 0.9875 - val_loss: 0.0746 - val_f1_m: 0.9941\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 773us/step - loss: 0.1139 - f1_m: 0.9890 - val_loss: 0.0722 - val_f1_m: 0.9863\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 774us/step - loss: 0.1031 - f1_m: 0.9895 - val_loss: 0.0587 - val_f1_m: 0.9961\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 769us/step - loss: 0.0943 - f1_m: 0.9908 - val_loss: 0.0609 - val_f1_m: 0.9844\n",
      "0.9907500147819519\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5515 - f1_m: 0.7187 - val_loss: 0.2790 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 809us/step - loss: 0.3656 - f1_m: 0.8660 - val_loss: 0.1816 - val_f1_m: 0.9941\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 794us/step - loss: 0.2655 - f1_m: 0.9482 - val_loss: 0.1500 - val_f1_m: 0.9736\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 794us/step - loss: 0.2069 - f1_m: 0.9715 - val_loss: 0.1070 - val_f1_m: 0.9922\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 789us/step - loss: 0.1688 - f1_m: 0.9822 - val_loss: 0.0968 - val_f1_m: 0.9873\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 800us/step - loss: 0.1429 - f1_m: 0.9883 - val_loss: 0.0808 - val_f1_m: 0.9932\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 801us/step - loss: 0.1242 - f1_m: 0.9895 - val_loss: 0.0695 - val_f1_m: 0.9941\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 782us/step - loss: 0.1097 - f1_m: 0.9920 - val_loss: 0.0680 - val_f1_m: 0.9902\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 781us/step - loss: 0.0990 - f1_m: 0.9927 - val_loss: 0.0569 - val_f1_m: 0.9922\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 782us/step - loss: 0.0897 - f1_m: 0.9927 - val_loss: 0.0651 - val_f1_m: 0.9844\n",
      "0.9927499890327454\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5493 - f1_m: 0.7215 - val_loss: 0.2986 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 804us/step - loss: 0.3618 - f1_m: 0.8772 - val_loss: 0.1987 - val_f1_m: 0.9980\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 784us/step - loss: 0.2633 - f1_m: 0.9467 - val_loss: 0.1491 - val_f1_m: 0.9941\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 782us/step - loss: 0.2063 - f1_m: 0.9640 - val_loss: 0.1211 - val_f1_m: 0.9941\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 786us/step - loss: 0.1693 - f1_m: 0.9755 - val_loss: 0.1037 - val_f1_m: 0.9951\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 775us/step - loss: 0.1441 - f1_m: 0.9858 - val_loss: 0.0967 - val_f1_m: 0.9912\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 791us/step - loss: 0.1260 - f1_m: 0.9895 - val_loss: 0.0811 - val_f1_m: 0.9951\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 775us/step - loss: 0.1121 - f1_m: 0.9883 - val_loss: 0.0757 - val_f1_m: 0.9932\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 827us/step - loss: 0.1015 - f1_m: 0.9885 - val_loss: 0.0602 - val_f1_m: 0.9971\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 784us/step - loss: 0.0933 - f1_m: 0.9902 - val_loss: 0.0600 - val_f1_m: 0.9941\n",
      "0.9902499914169312\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.5479 - f1_m: 0.7217 - val_loss: 0.2797 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 805us/step - loss: 0.3612 - f1_m: 0.8702 - val_loss: 0.1993 - val_f1_m: 0.9941\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 816us/step - loss: 0.2609 - f1_m: 0.9528 - val_loss: 0.1469 - val_f1_m: 0.9922\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 772us/step - loss: 0.2022 - f1_m: 0.9685 - val_loss: 0.1070 - val_f1_m: 0.9980\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 789us/step - loss: 0.1648 - f1_m: 0.9795 - val_loss: 0.0939 - val_f1_m: 0.9961\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 790us/step - loss: 0.1397 - f1_m: 0.9865 - val_loss: 0.0802 - val_f1_m: 0.9980\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 774us/step - loss: 0.1216 - f1_m: 0.9898 - val_loss: 0.0851 - val_f1_m: 0.9834\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 774us/step - loss: 0.1082 - f1_m: 0.9895 - val_loss: 0.0673 - val_f1_m: 0.9980\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 821us/step - loss: 0.0975 - f1_m: 0.9915 - val_loss: 0.0627 - val_f1_m: 0.9961\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 773us/step - loss: 0.0891 - f1_m: 0.9898 - val_loss: 0.0667 - val_f1_m: 0.9824\n",
      "0.9897500276565552\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 1s 1ms/step - loss: 0.5524 - f1_m: 0.7217 - val_loss: 0.2831 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 791us/step - loss: 0.3639 - f1_m: 0.8657 - val_loss: 0.2015 - val_f1_m: 0.9785\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 796us/step - loss: 0.2642 - f1_m: 0.9503 - val_loss: 0.1444 - val_f1_m: 0.9873\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 785us/step - loss: 0.2060 - f1_m: 0.9665 - val_loss: 0.1136 - val_f1_m: 0.9902\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 783us/step - loss: 0.1684 - f1_m: 0.9785 - val_loss: 0.0978 - val_f1_m: 0.9922\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 776us/step - loss: 0.1430 - f1_m: 0.9872 - val_loss: 0.0808 - val_f1_m: 0.9990\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 798us/step - loss: 0.1248 - f1_m: 0.9872 - val_loss: 0.0756 - val_f1_m: 0.9951\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 770us/step - loss: 0.1110 - f1_m: 0.9890 - val_loss: 0.0605 - val_f1_m: 0.9990\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 793us/step - loss: 0.1003 - f1_m: 0.9887 - val_loss: 0.0591 - val_f1_m: 0.9971\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 808us/step - loss: 0.0921 - f1_m: 0.9890 - val_loss: 0.0507 - val_f1_m: 0.9971\n",
      "0.9890000224113464\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5492 - f1_m: 0.7190 - val_loss: 0.2857 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 796us/step - loss: 0.3614 - f1_m: 0.8735 - val_loss: 0.2100 - val_f1_m: 0.9834\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 779us/step - loss: 0.2608 - f1_m: 0.9515 - val_loss: 0.1509 - val_f1_m: 0.9863\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 788us/step - loss: 0.2018 - f1_m: 0.9720 - val_loss: 0.1264 - val_f1_m: 0.9844\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 777us/step - loss: 0.1644 - f1_m: 0.9790 - val_loss: 0.0952 - val_f1_m: 0.9951\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 779us/step - loss: 0.1390 - f1_m: 0.9865 - val_loss: 0.0829 - val_f1_m: 0.9980\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 780us/step - loss: 0.1207 - f1_m: 0.9895 - val_loss: 0.0717 - val_f1_m: 0.9971\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 777us/step - loss: 0.1069 - f1_m: 0.9895 - val_loss: 0.0709 - val_f1_m: 0.9922\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 768us/step - loss: 0.0961 - f1_m: 0.9930 - val_loss: 0.0620 - val_f1_m: 0.9932\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 785us/step - loss: 0.0875 - f1_m: 0.9930 - val_loss: 0.0572 - val_f1_m: 0.9932\n",
      "0.9929999709129333\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5482 - f1_m: 0.7182 - val_loss: 0.2691 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 822us/step - loss: 0.3625 - f1_m: 0.8677 - val_loss: 0.1800 - val_f1_m: 0.9980\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 807us/step - loss: 0.2623 - f1_m: 0.9460 - val_loss: 0.1468 - val_f1_m: 0.9873\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 783us/step - loss: 0.2038 - f1_m: 0.9665 - val_loss: 0.1170 - val_f1_m: 0.9893\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 789us/step - loss: 0.1666 - f1_m: 0.9783 - val_loss: 0.0961 - val_f1_m: 0.9922\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 796us/step - loss: 0.1410 - f1_m: 0.9835 - val_loss: 0.0805 - val_f1_m: 0.9951\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 779us/step - loss: 0.1229 - f1_m: 0.9870 - val_loss: 0.0739 - val_f1_m: 0.9941\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 780us/step - loss: 0.1094 - f1_m: 0.9887 - val_loss: 0.0706 - val_f1_m: 0.9922\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 781us/step - loss: 0.0988 - f1_m: 0.9905 - val_loss: 0.0583 - val_f1_m: 0.9951\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 775us/step - loss: 0.0905 - f1_m: 0.9887 - val_loss: 0.0529 - val_f1_m: 0.9961\n",
      "0.9887499809265137\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5474 - f1_m: 0.7222 - val_loss: 0.2898 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 799us/step - loss: 0.3548 - f1_m: 0.8755 - val_loss: 0.2131 - val_f1_m: 0.9658\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 776us/step - loss: 0.2545 - f1_m: 0.9555 - val_loss: 0.1570 - val_f1_m: 0.9688\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 766us/step - loss: 0.1967 - f1_m: 0.9722 - val_loss: 0.1287 - val_f1_m: 0.9746\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 789us/step - loss: 0.1605 - f1_m: 0.9805 - val_loss: 0.1063 - val_f1_m: 0.9805\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 794us/step - loss: 0.1360 - f1_m: 0.9870 - val_loss: 0.0963 - val_f1_m: 0.9775\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 789us/step - loss: 0.1181 - f1_m: 0.9898 - val_loss: 0.0734 - val_f1_m: 0.9980\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 782us/step - loss: 0.1049 - f1_m: 0.9895 - val_loss: 0.0671 - val_f1_m: 0.9971\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 783us/step - loss: 0.0947 - f1_m: 0.9912 - val_loss: 0.0613 - val_f1_m: 0.9961\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 793us/step - loss: 0.0863 - f1_m: 0.9908 - val_loss: 0.0604 - val_f1_m: 0.9932\n",
      "0.9907500147819519\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5494 - f1_m: 0.7232 - val_loss: 0.2811 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 798us/step - loss: 0.3623 - f1_m: 0.8740 - val_loss: 0.2024 - val_f1_m: 0.9775\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 789us/step - loss: 0.2628 - f1_m: 0.9490 - val_loss: 0.1414 - val_f1_m: 0.9834\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 780us/step - loss: 0.2047 - f1_m: 0.9628 - val_loss: 0.1203 - val_f1_m: 0.9756\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 766us/step - loss: 0.1676 - f1_m: 0.9770 - val_loss: 0.0944 - val_f1_m: 0.9922\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 765us/step - loss: 0.1422 - f1_m: 0.9835 - val_loss: 0.0864 - val_f1_m: 0.9902\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 777us/step - loss: 0.1239 - f1_m: 0.9890 - val_loss: 0.0789 - val_f1_m: 0.9863\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 771us/step - loss: 0.1102 - f1_m: 0.9912 - val_loss: 0.0601 - val_f1_m: 0.9951\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 771us/step - loss: 0.0994 - f1_m: 0.9910 - val_loss: 0.0603 - val_f1_m: 0.9893\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 774us/step - loss: 0.0910 - f1_m: 0.9910 - val_loss: 0.0576 - val_f1_m: 0.9893\n",
      "0.9909999966621399\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5563 - f1_m: 0.7217 - val_loss: 0.2865 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 791us/step - loss: 0.3679 - f1_m: 0.8615 - val_loss: 0.1888 - val_f1_m: 0.9961\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 785us/step - loss: 0.2669 - f1_m: 0.9423 - val_loss: 0.1640 - val_f1_m: 0.9805\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 768us/step - loss: 0.2074 - f1_m: 0.9695 - val_loss: 0.1150 - val_f1_m: 0.9932\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 771us/step - loss: 0.1691 - f1_m: 0.9790 - val_loss: 0.1006 - val_f1_m: 0.9922\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 787us/step - loss: 0.1432 - f1_m: 0.9872 - val_loss: 0.0875 - val_f1_m: 0.9941\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - 0s 787us/step - loss: 0.1248 - f1_m: 0.9880 - val_loss: 0.0770 - val_f1_m: 0.9961\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 796us/step - loss: 0.1107 - f1_m: 0.9890 - val_loss: 0.0724 - val_f1_m: 0.9951\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 793us/step - loss: 0.1000 - f1_m: 0.9905 - val_loss: 0.0642 - val_f1_m: 0.9961\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 777us/step - loss: 0.0915 - f1_m: 0.9895 - val_loss: 0.0601 - val_f1_m: 0.9951\n",
      "0.9894999861717224\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5490 - f1_m: 0.7212 - val_loss: 0.2855 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 802us/step - loss: 0.3621 - f1_m: 0.8685 - val_loss: 0.2080 - val_f1_m: 0.9883\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 805us/step - loss: 0.2621 - f1_m: 0.9475 - val_loss: 0.1609 - val_f1_m: 0.9814\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 796us/step - loss: 0.2039 - f1_m: 0.9657 - val_loss: 0.1307 - val_f1_m: 0.9854\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 784us/step - loss: 0.1671 - f1_m: 0.9778 - val_loss: 0.1063 - val_f1_m: 0.9922\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 798us/step - loss: 0.1420 - f1_m: 0.9818 - val_loss: 0.1018 - val_f1_m: 0.9844\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 813us/step - loss: 0.1239 - f1_m: 0.9885 - val_loss: 0.0887 - val_f1_m: 0.9844\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 805us/step - loss: 0.1102 - f1_m: 0.9898 - val_loss: 0.0716 - val_f1_m: 0.9941\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 792us/step - loss: 0.1000 - f1_m: 0.9902 - val_loss: 0.0781 - val_f1_m: 0.9805\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 787us/step - loss: 0.0918 - f1_m: 0.9902 - val_loss: 0.0717 - val_f1_m: 0.9844\n",
      "0.9902499914169312\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.5493 - f1_m: 0.7215 - val_loss: 0.3017 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 861us/step - loss: 0.3635 - f1_m: 0.8750 - val_loss: 0.2123 - val_f1_m: 0.9893\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 960us/step - loss: 0.2648 - f1_m: 0.9440 - val_loss: 0.1795 - val_f1_m: 0.9688\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 814us/step - loss: 0.2066 - f1_m: 0.9642 - val_loss: 0.1277 - val_f1_m: 0.9932\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 801us/step - loss: 0.1690 - f1_m: 0.9772 - val_loss: 0.1137 - val_f1_m: 0.9902\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 780us/step - loss: 0.1429 - f1_m: 0.9827 - val_loss: 0.0965 - val_f1_m: 0.9971\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 779us/step - loss: 0.1246 - f1_m: 0.9868 - val_loss: 0.0957 - val_f1_m: 0.9902\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 787us/step - loss: 0.1105 - f1_m: 0.9893 - val_loss: 0.0861 - val_f1_m: 0.9912\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 773us/step - loss: 0.0995 - f1_m: 0.9905 - val_loss: 0.0704 - val_f1_m: 0.9971\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 777us/step - loss: 0.0912 - f1_m: 0.9883 - val_loss: 0.0611 - val_f1_m: 0.9990\n",
      "0.9882500171661377\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5479 - f1_m: 0.7185 - val_loss: 0.2880 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 797us/step - loss: 0.3624 - f1_m: 0.8655 - val_loss: 0.2232 - val_f1_m: 0.9727\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 792us/step - loss: 0.2636 - f1_m: 0.9512 - val_loss: 0.1691 - val_f1_m: 0.9727\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 779us/step - loss: 0.2057 - f1_m: 0.9688 - val_loss: 0.1293 - val_f1_m: 0.9814\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 783us/step - loss: 0.1688 - f1_m: 0.9768 - val_loss: 0.1061 - val_f1_m: 0.9873\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 773us/step - loss: 0.1435 - f1_m: 0.9860 - val_loss: 0.0867 - val_f1_m: 0.9941\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 782us/step - loss: 0.1256 - f1_m: 0.9885 - val_loss: 0.0809 - val_f1_m: 0.9922\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 773us/step - loss: 0.1118 - f1_m: 0.9893 - val_loss: 0.0736 - val_f1_m: 0.9922\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 780us/step - loss: 0.1009 - f1_m: 0.9920 - val_loss: 0.0723 - val_f1_m: 0.9883\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 790us/step - loss: 0.0925 - f1_m: 0.9910 - val_loss: 0.0618 - val_f1_m: 0.9932\n",
      "0.9909999966621399\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5495 - f1_m: 0.7187 - val_loss: 0.3017 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 786us/step - loss: 0.3600 - f1_m: 0.8797 - val_loss: 0.2053 - val_f1_m: 0.9922\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 780us/step - loss: 0.2592 - f1_m: 0.9553 - val_loss: 0.1566 - val_f1_m: 0.9805\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 790us/step - loss: 0.2004 - f1_m: 0.9705 - val_loss: 0.1382 - val_f1_m: 0.9707\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 770us/step - loss: 0.1630 - f1_m: 0.9793 - val_loss: 0.1015 - val_f1_m: 0.9902\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 771us/step - loss: 0.1377 - f1_m: 0.9865 - val_loss: 0.0854 - val_f1_m: 0.9932\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 788us/step - loss: 0.1194 - f1_m: 0.9883 - val_loss: 0.0736 - val_f1_m: 0.9980\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 795us/step - loss: 0.1057 - f1_m: 0.9910 - val_loss: 0.0683 - val_f1_m: 0.9932\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 790us/step - loss: 0.0951 - f1_m: 0.9910 - val_loss: 0.0627 - val_f1_m: 0.9922\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 783us/step - loss: 0.0865 - f1_m: 0.9915 - val_loss: 0.0492 - val_f1_m: 1.0000\n",
      "0.9915000200271606\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.5507 - f1_m: 0.7215 - val_loss: 0.2883 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 790us/step - loss: 0.3658 - f1_m: 0.8672 - val_loss: 0.2027 - val_f1_m: 0.9883\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 774us/step - loss: 0.2672 - f1_m: 0.9452 - val_loss: 0.1547 - val_f1_m: 0.9785\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 762us/step - loss: 0.2091 - f1_m: 0.9635 - val_loss: 0.1200 - val_f1_m: 0.9912\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 785us/step - loss: 0.1715 - f1_m: 0.9747 - val_loss: 0.0899 - val_f1_m: 0.9961\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 771us/step - loss: 0.1461 - f1_m: 0.9845 - val_loss: 0.0788 - val_f1_m: 0.9980\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 778us/step - loss: 0.1279 - f1_m: 0.9885 - val_loss: 0.0673 - val_f1_m: 0.9980\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 729us/step - loss: 0.1141 - f1_m: 0.9900 - val_loss: 0.0628 - val_f1_m: 0.9961\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 786us/step - loss: 0.1044 - f1_m: 0.9870 - val_loss: 0.0570 - val_f1_m: 0.9951\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 755us/step - loss: 0.0952 - f1_m: 0.9895 - val_loss: 0.0533 - val_f1_m: 0.9951\n",
      "0.9894999861717224\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5503 - f1_m: 0.7200 - val_loss: 0.2889 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 783us/step - loss: 0.3648 - f1_m: 0.8677 - val_loss: 0.2054 - val_f1_m: 0.9922\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 778us/step - loss: 0.2659 - f1_m: 0.9445 - val_loss: 0.1501 - val_f1_m: 0.9932\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 774us/step - loss: 0.2074 - f1_m: 0.9665 - val_loss: 0.1331 - val_f1_m: 0.9873\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 770us/step - loss: 0.1702 - f1_m: 0.9830 - val_loss: 0.1060 - val_f1_m: 0.9922\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 778us/step - loss: 0.1448 - f1_m: 0.9860 - val_loss: 0.0961 - val_f1_m: 0.9883\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 781us/step - loss: 0.1264 - f1_m: 0.9898 - val_loss: 0.0790 - val_f1_m: 0.9941\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 782us/step - loss: 0.1122 - f1_m: 0.9905 - val_loss: 0.0843 - val_f1_m: 0.9756\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 800us/step - loss: 0.1016 - f1_m: 0.9877 - val_loss: 0.0598 - val_f1_m: 0.9961\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 778us/step - loss: 0.0926 - f1_m: 0.9893 - val_loss: 0.0568 - val_f1_m: 0.9932\n",
      "0.9892500042915344\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5489 - f1_m: 0.7195 - val_loss: 0.2774 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 802us/step - loss: 0.3640 - f1_m: 0.8717 - val_loss: 0.1834 - val_f1_m: 0.9971\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 793us/step - loss: 0.2639 - f1_m: 0.9455 - val_loss: 0.1405 - val_f1_m: 0.9902\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 783us/step - loss: 0.2046 - f1_m: 0.9657 - val_loss: 0.1185 - val_f1_m: 0.9824\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 779us/step - loss: 0.1669 - f1_m: 0.9780 - val_loss: 0.0979 - val_f1_m: 0.9902\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 794us/step - loss: 0.1414 - f1_m: 0.9840 - val_loss: 0.0844 - val_f1_m: 0.9941\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 780us/step - loss: 0.1232 - f1_m: 0.9893 - val_loss: 0.0696 - val_f1_m: 0.9922\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 803us/step - loss: 0.1095 - f1_m: 0.9898 - val_loss: 0.0618 - val_f1_m: 0.9932\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 776us/step - loss: 0.0990 - f1_m: 0.9933 - val_loss: 0.0597 - val_f1_m: 0.9902\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 772us/step - loss: 0.0905 - f1_m: 0.9908 - val_loss: 0.0552 - val_f1_m: 0.9902\n",
      "0.9907500147819519\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5491 - f1_m: 0.7225 - val_loss: 0.2959 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 783us/step - loss: 0.3589 - f1_m: 0.8800 - val_loss: 0.2019 - val_f1_m: 0.9893\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 779us/step - loss: 0.2603 - f1_m: 0.9500 - val_loss: 0.1572 - val_f1_m: 0.9824\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 778us/step - loss: 0.2025 - f1_m: 0.9647 - val_loss: 0.1213 - val_f1_m: 0.9883\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 794us/step - loss: 0.1657 - f1_m: 0.9762 - val_loss: 0.1118 - val_f1_m: 0.9834\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 807us/step - loss: 0.1404 - f1_m: 0.9860 - val_loss: 0.0803 - val_f1_m: 0.9941\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 783us/step - loss: 0.1226 - f1_m: 0.9852 - val_loss: 0.0650 - val_f1_m: 0.9980\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 786us/step - loss: 0.1089 - f1_m: 0.9898 - val_loss: 0.0711 - val_f1_m: 0.9912\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 790us/step - loss: 0.0985 - f1_m: 0.9898 - val_loss: 0.0650 - val_f1_m: 0.9912\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 791us/step - loss: 0.0900 - f1_m: 0.9893 - val_loss: 0.0566 - val_f1_m: 0.9932\n",
      "0.9892500042915344\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5483 - f1_m: 0.7190 - val_loss: 0.2931 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 806us/step - loss: 0.3657 - f1_m: 0.8662 - val_loss: 0.2021 - val_f1_m: 0.9873\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 779us/step - loss: 0.2673 - f1_m: 0.9405 - val_loss: 0.1593 - val_f1_m: 0.9785\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 784us/step - loss: 0.2092 - f1_m: 0.9638 - val_loss: 0.1370 - val_f1_m: 0.9717\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 768us/step - loss: 0.1718 - f1_m: 0.9760 - val_loss: 0.1201 - val_f1_m: 0.9727\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 730us/step - loss: 0.1464 - f1_m: 0.9833 - val_loss: 0.1120 - val_f1_m: 0.9668\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 748us/step - loss: 0.1281 - f1_m: 0.9862 - val_loss: 0.0881 - val_f1_m: 0.9844\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 783us/step - loss: 0.1141 - f1_m: 0.9870 - val_loss: 0.0787 - val_f1_m: 0.9863\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 788us/step - loss: 0.1034 - f1_m: 0.9885 - val_loss: 0.0654 - val_f1_m: 0.9932\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 780us/step - loss: 0.0946 - f1_m: 0.9883 - val_loss: 0.0657 - val_f1_m: 0.9883\n",
      "0.9882500171661377\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 1s 3ms/step - loss: 0.5537 - f1_m: 0.7172 - val_loss: 0.2858 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 808us/step - loss: 0.3688 - f1_m: 0.8657 - val_loss: 0.1979 - val_f1_m: 0.9893\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 783us/step - loss: 0.2671 - f1_m: 0.9502 - val_loss: 0.1516 - val_f1_m: 0.9844\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 776us/step - loss: 0.2071 - f1_m: 0.9705 - val_loss: 0.1208 - val_f1_m: 0.9893\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 787us/step - loss: 0.1690 - f1_m: 0.9787 - val_loss: 0.1018 - val_f1_m: 0.9902\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 793us/step - loss: 0.1425 - f1_m: 0.9868 - val_loss: 0.0965 - val_f1_m: 0.9834\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 781us/step - loss: 0.1239 - f1_m: 0.9893 - val_loss: 0.0776 - val_f1_m: 0.9941\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 847us/step - loss: 0.1098 - f1_m: 0.9865 - val_loss: 0.0694 - val_f1_m: 0.9941\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 778us/step - loss: 0.0990 - f1_m: 0.9895 - val_loss: 0.0683 - val_f1_m: 0.9922\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 780us/step - loss: 0.0901 - f1_m: 0.9902 - val_loss: 0.0715 - val_f1_m: 0.9834\n",
      "0.9902499914169312\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5472 - f1_m: 0.7210 - val_loss: 0.2986 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 786us/step - loss: 0.3581 - f1_m: 0.8755 - val_loss: 0.2083 - val_f1_m: 0.9795\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 790us/step - loss: 0.2589 - f1_m: 0.9498 - val_loss: 0.1503 - val_f1_m: 0.9873\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 772us/step - loss: 0.2017 - f1_m: 0.9670 - val_loss: 0.1359 - val_f1_m: 0.9746\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 779us/step - loss: 0.1657 - f1_m: 0.9787 - val_loss: 0.1063 - val_f1_m: 0.9883\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 766us/step - loss: 0.1410 - f1_m: 0.9840 - val_loss: 0.0847 - val_f1_m: 0.9951\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 770us/step - loss: 0.1228 - f1_m: 0.9908 - val_loss: 0.0862 - val_f1_m: 0.9863\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 774us/step - loss: 0.1094 - f1_m: 0.9895 - val_loss: 0.0712 - val_f1_m: 0.9922\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 780us/step - loss: 0.0988 - f1_m: 0.9905 - val_loss: 0.0666 - val_f1_m: 0.9922\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 789us/step - loss: 0.0904 - f1_m: 0.9912 - val_loss: 0.0683 - val_f1_m: 0.9863\n",
      "0.9912499785423279\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.5485 - f1_m: 0.7242 - val_loss: 0.3048 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 810us/step - loss: 0.3611 - f1_m: 0.8862 - val_loss: 0.1989 - val_f1_m: 0.9922\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 782us/step - loss: 0.2614 - f1_m: 0.9490 - val_loss: 0.1572 - val_f1_m: 0.9844\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 794us/step - loss: 0.2028 - f1_m: 0.9663 - val_loss: 0.1199 - val_f1_m: 0.9912\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - 0s 784us/step - loss: 0.1654 - f1_m: 0.9753 - val_loss: 0.1024 - val_f1_m: 0.9922\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 783us/step - loss: 0.1400 - f1_m: 0.9835 - val_loss: 0.0828 - val_f1_m: 0.9961\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 805us/step - loss: 0.1221 - f1_m: 0.9893 - val_loss: 0.0753 - val_f1_m: 0.9951\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 767us/step - loss: 0.1088 - f1_m: 0.9877 - val_loss: 0.0682 - val_f1_m: 0.9941\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 773us/step - loss: 0.0986 - f1_m: 0.9895 - val_loss: 0.0583 - val_f1_m: 0.9951\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 770us/step - loss: 0.0904 - f1_m: 0.9923 - val_loss: 0.0658 - val_f1_m: 0.9854\n",
      "0.9922500252723694\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5532 - f1_m: 0.7170 - val_loss: 0.3038 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 797us/step - loss: 0.3661 - f1_m: 0.8690 - val_loss: 0.2060 - val_f1_m: 0.9922\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 784us/step - loss: 0.2635 - f1_m: 0.9520 - val_loss: 0.1592 - val_f1_m: 0.9844\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 783us/step - loss: 0.2040 - f1_m: 0.9688 - val_loss: 0.1245 - val_f1_m: 0.9902\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 800us/step - loss: 0.1660 - f1_m: 0.9795 - val_loss: 0.0932 - val_f1_m: 0.9961\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 783us/step - loss: 0.1402 - f1_m: 0.9860 - val_loss: 0.0829 - val_f1_m: 0.9941\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 796us/step - loss: 0.1215 - f1_m: 0.9885 - val_loss: 0.0658 - val_f1_m: 0.9971\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 780us/step - loss: 0.1079 - f1_m: 0.9898 - val_loss: 0.0617 - val_f1_m: 0.9951\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 783us/step - loss: 0.0971 - f1_m: 0.9900 - val_loss: 0.0494 - val_f1_m: 0.9971\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 788us/step - loss: 0.0886 - f1_m: 0.9923 - val_loss: 0.0565 - val_f1_m: 0.9912\n",
      "0.9922500252723694\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5484 - f1_m: 0.7222 - val_loss: 0.2876 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 799us/step - loss: 0.3608 - f1_m: 0.8705 - val_loss: 0.2045 - val_f1_m: 0.9844\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 792us/step - loss: 0.2609 - f1_m: 0.9507 - val_loss: 0.1600 - val_f1_m: 0.9756\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 779us/step - loss: 0.2028 - f1_m: 0.9643 - val_loss: 0.1151 - val_f1_m: 0.9902\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 770us/step - loss: 0.1662 - f1_m: 0.9772 - val_loss: 0.1086 - val_f1_m: 0.9863\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 777us/step - loss: 0.1407 - f1_m: 0.9818 - val_loss: 0.0910 - val_f1_m: 0.9902\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 789us/step - loss: 0.1230 - f1_m: 0.9858 - val_loss: 0.0844 - val_f1_m: 0.9863\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 770us/step - loss: 0.1097 - f1_m: 0.9852 - val_loss: 0.0644 - val_f1_m: 0.9980\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 775us/step - loss: 0.0994 - f1_m: 0.9858 - val_loss: 0.0701 - val_f1_m: 0.9893\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 784us/step - loss: 0.0912 - f1_m: 0.9877 - val_loss: 0.0640 - val_f1_m: 0.9902\n",
      "0.9877499938011169\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5517 - f1_m: 0.7190 - val_loss: 0.2822 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 810us/step - loss: 0.3684 - f1_m: 0.8565 - val_loss: 0.2109 - val_f1_m: 0.9883\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 801us/step - loss: 0.2680 - f1_m: 0.9452 - val_loss: 0.1708 - val_f1_m: 0.9648\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 781us/step - loss: 0.2092 - f1_m: 0.9680 - val_loss: 0.1241 - val_f1_m: 0.9854\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 809us/step - loss: 0.1710 - f1_m: 0.9778 - val_loss: 0.1114 - val_f1_m: 0.9834\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 799us/step - loss: 0.1449 - f1_m: 0.9820 - val_loss: 0.0935 - val_f1_m: 0.9893\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 779us/step - loss: 0.1262 - f1_m: 0.9872 - val_loss: 0.0783 - val_f1_m: 0.9902\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 817us/step - loss: 0.1118 - f1_m: 0.9890 - val_loss: 0.0794 - val_f1_m: 0.9814\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 892us/step - loss: 0.1010 - f1_m: 0.9912 - val_loss: 0.0643 - val_f1_m: 0.9883\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 838us/step - loss: 0.0922 - f1_m: 0.9895 - val_loss: 0.0664 - val_f1_m: 0.9824\n",
      "0.9894999861717224\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5509 - f1_m: 0.7172 - val_loss: 0.2795 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 785us/step - loss: 0.3641 - f1_m: 0.8647 - val_loss: 0.2062 - val_f1_m: 0.9854\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 766us/step - loss: 0.2638 - f1_m: 0.9477 - val_loss: 0.1494 - val_f1_m: 0.9854\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 744us/step - loss: 0.2050 - f1_m: 0.9660 - val_loss: 0.1123 - val_f1_m: 0.9922\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 774us/step - loss: 0.1681 - f1_m: 0.9750 - val_loss: 0.1020 - val_f1_m: 0.9922\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 780us/step - loss: 0.1427 - f1_m: 0.9812 - val_loss: 0.0830 - val_f1_m: 0.9941\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 776us/step - loss: 0.1244 - f1_m: 0.9883 - val_loss: 0.0801 - val_f1_m: 0.9863\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 766us/step - loss: 0.1109 - f1_m: 0.9895 - val_loss: 0.0698 - val_f1_m: 0.9922\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 774us/step - loss: 0.1004 - f1_m: 0.9893 - val_loss: 0.0618 - val_f1_m: 0.9932\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 779us/step - loss: 0.0921 - f1_m: 0.9877 - val_loss: 0.0523 - val_f1_m: 0.9971\n",
      "0.9877499938011169\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5523 - f1_m: 0.7167 - val_loss: 0.2873 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 813us/step - loss: 0.3644 - f1_m: 0.8735 - val_loss: 0.1949 - val_f1_m: 0.9980\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 796us/step - loss: 0.2623 - f1_m: 0.9517 - val_loss: 0.1431 - val_f1_m: 0.9961\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 774us/step - loss: 0.2026 - f1_m: 0.9685 - val_loss: 0.1133 - val_f1_m: 0.9971\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 773us/step - loss: 0.1653 - f1_m: 0.9768 - val_loss: 0.1020 - val_f1_m: 0.9941\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 770us/step - loss: 0.1402 - f1_m: 0.9858 - val_loss: 0.0923 - val_f1_m: 0.9912\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 788us/step - loss: 0.1223 - f1_m: 0.9877 - val_loss: 0.0710 - val_f1_m: 0.9990\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 776us/step - loss: 0.1089 - f1_m: 0.9900 - val_loss: 0.0720 - val_f1_m: 0.9912\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 771us/step - loss: 0.0987 - f1_m: 0.9905 - val_loss: 0.0654 - val_f1_m: 0.9922\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 778us/step - loss: 0.0903 - f1_m: 0.9877 - val_loss: 0.0505 - val_f1_m: 0.9990\n",
      "0.9877499938011169\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.5520 - f1_m: 0.7217 - val_loss: 0.2976 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 879us/step - loss: 0.3641 - f1_m: 0.8732 - val_loss: 0.1976 - val_f1_m: 0.9912\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 851us/step - loss: 0.2641 - f1_m: 0.9463 - val_loss: 0.1611 - val_f1_m: 0.9775\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 787us/step - loss: 0.2053 - f1_m: 0.9685 - val_loss: 0.1259 - val_f1_m: 0.9873\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 819us/step - loss: 0.1677 - f1_m: 0.9755 - val_loss: 0.1160 - val_f1_m: 0.9795\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 883us/step - loss: 0.1429 - f1_m: 0.9833 - val_loss: 0.0906 - val_f1_m: 0.9941\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 815us/step - loss: 0.1244 - f1_m: 0.9872 - val_loss: 0.0766 - val_f1_m: 0.9961\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 839us/step - loss: 0.1106 - f1_m: 0.9890 - val_loss: 0.0642 - val_f1_m: 0.9980\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 957us/step - loss: 0.0997 - f1_m: 0.9902 - val_loss: 0.0648 - val_f1_m: 0.9961\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 900us/step - loss: 0.0913 - f1_m: 0.9890 - val_loss: 0.0632 - val_f1_m: 0.9922\n",
      "0.9890000224113464\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.5478 - f1_m: 0.7260 - val_loss: 0.2729 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 822us/step - loss: 0.3615 - f1_m: 0.8685 - val_loss: 0.1937 - val_f1_m: 0.9941\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 819us/step - loss: 0.2628 - f1_m: 0.9485 - val_loss: 0.1391 - val_f1_m: 0.9922\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 955us/step - loss: 0.2053 - f1_m: 0.9643 - val_loss: 0.1149 - val_f1_m: 0.9912\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 788us/step - loss: 0.1686 - f1_m: 0.9768 - val_loss: 0.0895 - val_f1_m: 0.9941\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 801us/step - loss: 0.1430 - f1_m: 0.9835 - val_loss: 0.0845 - val_f1_m: 0.9922\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 788us/step - loss: 0.1244 - f1_m: 0.9887 - val_loss: 0.0721 - val_f1_m: 0.9932\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 780us/step - loss: 0.1108 - f1_m: 0.9905 - val_loss: 0.0644 - val_f1_m: 0.9951\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 808us/step - loss: 0.0998 - f1_m: 0.9910 - val_loss: 0.0523 - val_f1_m: 0.9961\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 790us/step - loss: 0.0914 - f1_m: 0.9915 - val_loss: 0.0536 - val_f1_m: 0.9941\n",
      "0.9915000200271606\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5487 - f1_m: 0.7222 - val_loss: 0.2793 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 792us/step - loss: 0.3601 - f1_m: 0.8732 - val_loss: 0.1980 - val_f1_m: 0.9912\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 793us/step - loss: 0.2605 - f1_m: 0.9482 - val_loss: 0.1538 - val_f1_m: 0.9834\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 789us/step - loss: 0.2027 - f1_m: 0.9678 - val_loss: 0.1223 - val_f1_m: 0.9893\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 773us/step - loss: 0.1659 - f1_m: 0.9800 - val_loss: 0.0989 - val_f1_m: 0.9922\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 774us/step - loss: 0.1409 - f1_m: 0.9862 - val_loss: 0.0932 - val_f1_m: 0.9844\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 773us/step - loss: 0.1229 - f1_m: 0.9905 - val_loss: 0.0744 - val_f1_m: 0.9941\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 761us/step - loss: 0.1097 - f1_m: 0.9902 - val_loss: 0.0741 - val_f1_m: 0.9873\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 777us/step - loss: 0.0991 - f1_m: 0.9900 - val_loss: 0.0598 - val_f1_m: 0.9951\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 804us/step - loss: 0.0911 - f1_m: 0.9885 - val_loss: 0.0583 - val_f1_m: 0.9932\n",
      "0.9884999990463257\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5504 - f1_m: 0.7190 - val_loss: 0.2843 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 789us/step - loss: 0.3613 - f1_m: 0.8680 - val_loss: 0.2017 - val_f1_m: 0.9844\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 772us/step - loss: 0.2615 - f1_m: 0.9460 - val_loss: 0.1553 - val_f1_m: 0.9795\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 778us/step - loss: 0.2032 - f1_m: 0.9675 - val_loss: 0.1190 - val_f1_m: 0.9844\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 768us/step - loss: 0.1664 - f1_m: 0.9772 - val_loss: 0.1013 - val_f1_m: 0.9873\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 767us/step - loss: 0.1410 - f1_m: 0.9830 - val_loss: 0.0847 - val_f1_m: 0.9902\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 770us/step - loss: 0.1221 - f1_m: 0.9865 - val_loss: 0.0838 - val_f1_m: 0.9854\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 775us/step - loss: 0.1088 - f1_m: 0.9900 - val_loss: 0.0695 - val_f1_m: 0.9893\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 770us/step - loss: 0.0981 - f1_m: 0.9925 - val_loss: 0.0655 - val_f1_m: 0.9883\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 763us/step - loss: 0.0897 - f1_m: 0.9918 - val_loss: 0.0611 - val_f1_m: 0.9883\n",
      "0.9917500019073486\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5495 - f1_m: 0.7202 - val_loss: 0.2835 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 790us/step - loss: 0.3620 - f1_m: 0.8682 - val_loss: 0.2088 - val_f1_m: 0.9834\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 784us/step - loss: 0.2612 - f1_m: 0.9503 - val_loss: 0.1595 - val_f1_m: 0.9785\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 766us/step - loss: 0.2026 - f1_m: 0.9672 - val_loss: 0.1371 - val_f1_m: 0.9746\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 772us/step - loss: 0.1664 - f1_m: 0.9760 - val_loss: 0.1086 - val_f1_m: 0.9854\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 779us/step - loss: 0.1413 - f1_m: 0.9815 - val_loss: 0.0879 - val_f1_m: 0.9922\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 784us/step - loss: 0.1237 - f1_m: 0.9858 - val_loss: 0.0837 - val_f1_m: 0.9893\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 779us/step - loss: 0.1102 - f1_m: 0.9893 - val_loss: 0.0730 - val_f1_m: 0.9893\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 791us/step - loss: 0.0999 - f1_m: 0.9893 - val_loss: 0.0742 - val_f1_m: 0.9824\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 767us/step - loss: 0.0918 - f1_m: 0.9893 - val_loss: 0.0556 - val_f1_m: 0.9961\n",
      "0.9892500042915344\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.5512 - f1_m: 0.7255 - val_loss: 0.2926 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 805us/step - loss: 0.3589 - f1_m: 0.8790 - val_loss: 0.2072 - val_f1_m: 0.9893\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 783us/step - loss: 0.2590 - f1_m: 0.9538 - val_loss: 0.1591 - val_f1_m: 0.9824\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 788us/step - loss: 0.2013 - f1_m: 0.9697 - val_loss: 0.1258 - val_f1_m: 0.9844\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 776us/step - loss: 0.1650 - f1_m: 0.9795 - val_loss: 0.1017 - val_f1_m: 0.9912\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 776us/step - loss: 0.1403 - f1_m: 0.9852 - val_loss: 0.0757 - val_f1_m: 1.0000\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 786us/step - loss: 0.1223 - f1_m: 0.9877 - val_loss: 0.0801 - val_f1_m: 0.9893\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 814us/step - loss: 0.1089 - f1_m: 0.9902 - val_loss: 0.0800 - val_f1_m: 0.9805\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 782us/step - loss: 0.0987 - f1_m: 0.9902 - val_loss: 0.0666 - val_f1_m: 0.9893\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 769us/step - loss: 0.0897 - f1_m: 0.9902 - val_loss: 0.0492 - val_f1_m: 0.9961\n",
      "0.9902499914169312\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5471 - f1_m: 0.7240 - val_loss: 0.3046 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 806us/step - loss: 0.3559 - f1_m: 0.8877 - val_loss: 0.2142 - val_f1_m: 0.9814\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - 0s 798us/step - loss: 0.2576 - f1_m: 0.9530 - val_loss: 0.1632 - val_f1_m: 0.9736\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 787us/step - loss: 0.2017 - f1_m: 0.9672 - val_loss: 0.1240 - val_f1_m: 0.9863\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 784us/step - loss: 0.1664 - f1_m: 0.9750 - val_loss: 0.1043 - val_f1_m: 0.9854\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 808us/step - loss: 0.1417 - f1_m: 0.9815 - val_loss: 0.0795 - val_f1_m: 0.9941\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 831us/step - loss: 0.1241 - f1_m: 0.9877 - val_loss: 0.0796 - val_f1_m: 0.9873\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 826us/step - loss: 0.1108 - f1_m: 0.9905 - val_loss: 0.0685 - val_f1_m: 0.9922\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 788us/step - loss: 0.1004 - f1_m: 0.9910 - val_loss: 0.0624 - val_f1_m: 0.9922\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 775us/step - loss: 0.0920 - f1_m: 0.9912 - val_loss: 0.0514 - val_f1_m: 0.9971\n",
      "0.9912499785423279\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.5459 - f1_m: 0.7235 - val_loss: 0.2800 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 892us/step - loss: 0.3603 - f1_m: 0.8677 - val_loss: 0.1961 - val_f1_m: 0.9902\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 879us/step - loss: 0.2605 - f1_m: 0.9520 - val_loss: 0.1634 - val_f1_m: 0.9658\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 874us/step - loss: 0.2027 - f1_m: 0.9680 - val_loss: 0.1167 - val_f1_m: 0.9883\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 858us/step - loss: 0.1663 - f1_m: 0.9750 - val_loss: 0.1090 - val_f1_m: 0.9854\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 772us/step - loss: 0.1414 - f1_m: 0.9830 - val_loss: 0.0993 - val_f1_m: 0.9854\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 786us/step - loss: 0.1238 - f1_m: 0.9880 - val_loss: 0.0788 - val_f1_m: 0.9932\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 781us/step - loss: 0.1102 - f1_m: 0.9893 - val_loss: 0.0644 - val_f1_m: 0.9971\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 795us/step - loss: 0.1000 - f1_m: 0.9900 - val_loss: 0.0645 - val_f1_m: 0.9951\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 787us/step - loss: 0.0915 - f1_m: 0.9908 - val_loss: 0.0662 - val_f1_m: 0.9893\n",
      "0.9907500147819519\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 1s 2ms/step - loss: 0.5509 - f1_m: 0.7200 - val_loss: 0.2880 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 878us/step - loss: 0.3653 - f1_m: 0.8595 - val_loss: 0.2037 - val_f1_m: 0.9902\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 865us/step - loss: 0.2658 - f1_m: 0.9503 - val_loss: 0.1594 - val_f1_m: 0.9785\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 819us/step - loss: 0.2066 - f1_m: 0.9688 - val_loss: 0.1188 - val_f1_m: 0.9883\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 894us/step - loss: 0.1690 - f1_m: 0.9793 - val_loss: 0.1051 - val_f1_m: 0.9824\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 883us/step - loss: 0.1434 - f1_m: 0.9883 - val_loss: 0.0853 - val_f1_m: 0.9922\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 867us/step - loss: 0.1249 - f1_m: 0.9898 - val_loss: 0.0761 - val_f1_m: 0.9912\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 820us/step - loss: 0.1114 - f1_m: 0.9900 - val_loss: 0.0739 - val_f1_m: 0.9854\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 814us/step - loss: 0.1004 - f1_m: 0.9900 - val_loss: 0.0588 - val_f1_m: 0.9932\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 782us/step - loss: 0.0919 - f1_m: 0.9908 - val_loss: 0.0600 - val_f1_m: 0.9902\n",
      "0.9907500147819519\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.5478 - f1_m: 0.7177 - val_loss: 0.2883 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 790us/step - loss: 0.3618 - f1_m: 0.8777 - val_loss: 0.2084 - val_f1_m: 0.9883\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 789us/step - loss: 0.2613 - f1_m: 0.9505 - val_loss: 0.1667 - val_f1_m: 0.9736\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 792us/step - loss: 0.2024 - f1_m: 0.9693 - val_loss: 0.1484 - val_f1_m: 0.9668\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 760us/step - loss: 0.1654 - f1_m: 0.9768 - val_loss: 0.1112 - val_f1_m: 0.9795\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 795us/step - loss: 0.1394 - f1_m: 0.9860 - val_loss: 0.0886 - val_f1_m: 0.9941\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 770us/step - loss: 0.1209 - f1_m: 0.9883 - val_loss: 0.0893 - val_f1_m: 0.9863\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 784us/step - loss: 0.1073 - f1_m: 0.9915 - val_loss: 0.0742 - val_f1_m: 0.9941\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 765us/step - loss: 0.0963 - f1_m: 0.9920 - val_loss: 0.0678 - val_f1_m: 0.9951\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 770us/step - loss: 0.0876 - f1_m: 0.9925 - val_loss: 0.0635 - val_f1_m: 0.9941\n",
      "0.9925000071525574\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5444 - f1_m: 0.7257 - val_loss: 0.2940 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 782us/step - loss: 0.3538 - f1_m: 0.8875 - val_loss: 0.2124 - val_f1_m: 0.9805\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 777us/step - loss: 0.2552 - f1_m: 0.9540 - val_loss: 0.1655 - val_f1_m: 0.9736\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 786us/step - loss: 0.1980 - f1_m: 0.9707 - val_loss: 0.1223 - val_f1_m: 0.9873\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 774us/step - loss: 0.1619 - f1_m: 0.9815 - val_loss: 0.0993 - val_f1_m: 0.9961\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 767us/step - loss: 0.1378 - f1_m: 0.9810 - val_loss: 0.0884 - val_f1_m: 0.9961\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 764us/step - loss: 0.1199 - f1_m: 0.9893 - val_loss: 0.0706 - val_f1_m: 1.0000\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 764us/step - loss: 0.1069 - f1_m: 0.9893 - val_loss: 0.0784 - val_f1_m: 0.9941\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 775us/step - loss: 0.0966 - f1_m: 0.9908 - val_loss: 0.0669 - val_f1_m: 0.9961\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 777us/step - loss: 0.0885 - f1_m: 0.9902 - val_loss: 0.0640 - val_f1_m: 0.9941\n",
      "0.9902499914169312\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5528 - f1_m: 0.7212 - val_loss: 0.3045 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 785us/step - loss: 0.3627 - f1_m: 0.8707 - val_loss: 0.2186 - val_f1_m: 0.9854\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 780us/step - loss: 0.2625 - f1_m: 0.9498 - val_loss: 0.1588 - val_f1_m: 0.9883\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 780us/step - loss: 0.2046 - f1_m: 0.9700 - val_loss: 0.1270 - val_f1_m: 0.9893\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 770us/step - loss: 0.1674 - f1_m: 0.9795 - val_loss: 0.1036 - val_f1_m: 0.9951\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 768us/step - loss: 0.1420 - f1_m: 0.9850 - val_loss: 0.0882 - val_f1_m: 0.9971\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 769us/step - loss: 0.1237 - f1_m: 0.9905 - val_loss: 0.0863 - val_f1_m: 0.9893\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 778us/step - loss: 0.1100 - f1_m: 0.9933 - val_loss: 0.0745 - val_f1_m: 0.9932\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 771us/step - loss: 0.0993 - f1_m: 0.9920 - val_loss: 0.0697 - val_f1_m: 0.9932\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 770us/step - loss: 0.0909 - f1_m: 0.9930 - val_loss: 0.0681 - val_f1_m: 0.9922\n",
      "0.9929999709129333\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5457 - f1_m: 0.7235 - val_loss: 0.3034 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 795us/step - loss: 0.3547 - f1_m: 0.8862 - val_loss: 0.2139 - val_f1_m: 0.9883\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 783us/step - loss: 0.2563 - f1_m: 0.9500 - val_loss: 0.1872 - val_f1_m: 0.9629\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 779us/step - loss: 0.1991 - f1_m: 0.9693 - val_loss: 0.1245 - val_f1_m: 0.9912\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 772us/step - loss: 0.1632 - f1_m: 0.9810 - val_loss: 0.1216 - val_f1_m: 0.9824\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 778us/step - loss: 0.1383 - f1_m: 0.9850 - val_loss: 0.1065 - val_f1_m: 0.9873\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 785us/step - loss: 0.1202 - f1_m: 0.9887 - val_loss: 0.1053 - val_f1_m: 0.9707\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 782us/step - loss: 0.1070 - f1_m: 0.9883 - val_loss: 0.0850 - val_f1_m: 0.9883\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 762us/step - loss: 0.0966 - f1_m: 0.9868 - val_loss: 0.0820 - val_f1_m: 0.9834\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 790us/step - loss: 0.0886 - f1_m: 0.9877 - val_loss: 0.0658 - val_f1_m: 0.9951\n",
      "0.9877499938011169\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5487 - f1_m: 0.7175 - val_loss: 0.2967 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 798us/step - loss: 0.3632 - f1_m: 0.8760 - val_loss: 0.1978 - val_f1_m: 0.9941\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 786us/step - loss: 0.2641 - f1_m: 0.9450 - val_loss: 0.1426 - val_f1_m: 0.9902\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 793us/step - loss: 0.2061 - f1_m: 0.9643 - val_loss: 0.1288 - val_f1_m: 0.9824\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 788us/step - loss: 0.1695 - f1_m: 0.9743 - val_loss: 0.0986 - val_f1_m: 0.9922\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 786us/step - loss: 0.1445 - f1_m: 0.9808 - val_loss: 0.0897 - val_f1_m: 0.9922\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 783us/step - loss: 0.1266 - f1_m: 0.9893 - val_loss: 0.0798 - val_f1_m: 0.9932\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 783us/step - loss: 0.1128 - f1_m: 0.9887 - val_loss: 0.0788 - val_f1_m: 0.9824\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 779us/step - loss: 0.1021 - f1_m: 0.9900 - val_loss: 0.0720 - val_f1_m: 0.9824\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 776us/step - loss: 0.0934 - f1_m: 0.9883 - val_loss: 0.0664 - val_f1_m: 0.9805\n",
      "0.9882500171661377\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5544 - f1_m: 0.7165 - val_loss: 0.2840 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 796us/step - loss: 0.3683 - f1_m: 0.8642 - val_loss: 0.1967 - val_f1_m: 0.9980\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 788us/step - loss: 0.2667 - f1_m: 0.9507 - val_loss: 0.1450 - val_f1_m: 0.9932\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 785us/step - loss: 0.2073 - f1_m: 0.9668 - val_loss: 0.1120 - val_f1_m: 0.9951\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 783us/step - loss: 0.1694 - f1_m: 0.9803 - val_loss: 0.0912 - val_f1_m: 0.9971\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 766us/step - loss: 0.1439 - f1_m: 0.9865 - val_loss: 0.0915 - val_f1_m: 0.9854\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 783us/step - loss: 0.1254 - f1_m: 0.9905 - val_loss: 0.0784 - val_f1_m: 0.9902\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 780us/step - loss: 0.1114 - f1_m: 0.9918 - val_loss: 0.0642 - val_f1_m: 0.9971\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 775us/step - loss: 0.1007 - f1_m: 0.9877 - val_loss: 0.0580 - val_f1_m: 0.9971\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 776us/step - loss: 0.0921 - f1_m: 0.9883 - val_loss: 0.0564 - val_f1_m: 0.9922\n",
      "0.9882500171661377\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5507 - f1_m: 0.7175 - val_loss: 0.2853 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 797us/step - loss: 0.3642 - f1_m: 0.8662 - val_loss: 0.2137 - val_f1_m: 0.9785\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 781us/step - loss: 0.2638 - f1_m: 0.9482 - val_loss: 0.1634 - val_f1_m: 0.9746\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 774us/step - loss: 0.2053 - f1_m: 0.9670 - val_loss: 0.1202 - val_f1_m: 0.9854\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 800us/step - loss: 0.1689 - f1_m: 0.9753 - val_loss: 0.1108 - val_f1_m: 0.9824\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 786us/step - loss: 0.1434 - f1_m: 0.9822 - val_loss: 0.0941 - val_f1_m: 0.9854\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 787us/step - loss: 0.1252 - f1_m: 0.9885 - val_loss: 0.0828 - val_f1_m: 0.9863\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 772us/step - loss: 0.1117 - f1_m: 0.9885 - val_loss: 0.0650 - val_f1_m: 0.9961\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 788us/step - loss: 0.1012 - f1_m: 0.9887 - val_loss: 0.0630 - val_f1_m: 0.9941\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 768us/step - loss: 0.0928 - f1_m: 0.9883 - val_loss: 0.0619 - val_f1_m: 0.9912\n",
      "0.9882500171661377\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5467 - f1_m: 0.7187 - val_loss: 0.2811 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 782us/step - loss: 0.3648 - f1_m: 0.8657 - val_loss: 0.2099 - val_f1_m: 0.9854\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 789us/step - loss: 0.2665 - f1_m: 0.9467 - val_loss: 0.1551 - val_f1_m: 0.9854\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 774us/step - loss: 0.2076 - f1_m: 0.9650 - val_loss: 0.1316 - val_f1_m: 0.9844\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 786us/step - loss: 0.1698 - f1_m: 0.9758 - val_loss: 0.1111 - val_f1_m: 0.9883\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 775us/step - loss: 0.1437 - f1_m: 0.9843 - val_loss: 0.1120 - val_f1_m: 0.9746\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 788us/step - loss: 0.1253 - f1_m: 0.9880 - val_loss: 0.0971 - val_f1_m: 0.9805\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 788us/step - loss: 0.1111 - f1_m: 0.9915 - val_loss: 0.0827 - val_f1_m: 0.9844\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 789us/step - loss: 0.1003 - f1_m: 0.9900 - val_loss: 0.0798 - val_f1_m: 0.9775\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 788us/step - loss: 0.0917 - f1_m: 0.9918 - val_loss: 0.0744 - val_f1_m: 0.9785\n",
      "0.9917500019073486\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.5521 - f1_m: 0.7210 - val_loss: 0.2970 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 821us/step - loss: 0.3634 - f1_m: 0.8735 - val_loss: 0.2166 - val_f1_m: 0.9912\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 809us/step - loss: 0.2624 - f1_m: 0.9507 - val_loss: 0.1559 - val_f1_m: 0.9922\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 817us/step - loss: 0.2036 - f1_m: 0.9700 - val_loss: 0.1291 - val_f1_m: 0.9922\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 799us/step - loss: 0.1664 - f1_m: 0.9780 - val_loss: 0.1108 - val_f1_m: 0.9932\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 763us/step - loss: 0.1413 - f1_m: 0.9815 - val_loss: 0.0911 - val_f1_m: 0.9941\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 763us/step - loss: 0.1232 - f1_m: 0.9860 - val_loss: 0.0807 - val_f1_m: 0.9951\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 770us/step - loss: 0.1099 - f1_m: 0.9862 - val_loss: 0.0628 - val_f1_m: 0.9990\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 772us/step - loss: 0.0998 - f1_m: 0.9868 - val_loss: 0.0628 - val_f1_m: 0.9961\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 768us/step - loss: 0.0916 - f1_m: 0.9877 - val_loss: 0.0628 - val_f1_m: 0.9922\n",
      "0.9877499938011169\n",
      "if any\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5470 - f1_m: 0.7197 - val_loss: 0.2814 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 800us/step - loss: 0.3587 - f1_m: 0.8685 - val_loss: 0.2222 - val_f1_m: 0.9678\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 799us/step - loss: 0.2584 - f1_m: 0.9555 - val_loss: 0.1667 - val_f1_m: 0.9668\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 786us/step - loss: 0.2004 - f1_m: 0.9697 - val_loss: 0.1410 - val_f1_m: 0.9658\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 782us/step - loss: 0.1640 - f1_m: 0.9778 - val_loss: 0.1104 - val_f1_m: 0.9824\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 770us/step - loss: 0.1392 - f1_m: 0.9843 - val_loss: 0.1002 - val_f1_m: 0.9844\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 776us/step - loss: 0.1214 - f1_m: 0.9865 - val_loss: 0.0840 - val_f1_m: 0.9912\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 790us/step - loss: 0.1085 - f1_m: 0.9883 - val_loss: 0.0742 - val_f1_m: 0.9932\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 777us/step - loss: 0.0981 - f1_m: 0.9875 - val_loss: 0.0651 - val_f1_m: 0.9961\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 791us/step - loss: 0.0902 - f1_m: 0.9898 - val_loss: 0.0680 - val_f1_m: 0.9902\n",
      "0.9897500276565552\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5527 - f1_m: 0.7215 - val_loss: 0.3038 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 804us/step - loss: 0.3640 - f1_m: 0.8800 - val_loss: 0.2068 - val_f1_m: 0.9932\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 793us/step - loss: 0.2628 - f1_m: 0.9512 - val_loss: 0.1551 - val_f1_m: 0.9854\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 773us/step - loss: 0.2039 - f1_m: 0.9707 - val_loss: 0.1207 - val_f1_m: 0.9893\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 772us/step - loss: 0.1667 - f1_m: 0.9790 - val_loss: 0.1037 - val_f1_m: 0.9893\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 780us/step - loss: 0.1414 - f1_m: 0.9840 - val_loss: 0.0905 - val_f1_m: 0.9873\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 789us/step - loss: 0.1230 - f1_m: 0.9870 - val_loss: 0.0813 - val_f1_m: 0.9854\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 790us/step - loss: 0.1094 - f1_m: 0.9877 - val_loss: 0.0743 - val_f1_m: 0.9863\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 784us/step - loss: 0.0988 - f1_m: 0.9885 - val_loss: 0.0589 - val_f1_m: 0.9961\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 788us/step - loss: 0.0904 - f1_m: 0.9893 - val_loss: 0.0542 - val_f1_m: 0.9961\n",
      "0.9892500042915344\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.5500 - f1_m: 0.7192 - val_loss: 0.2911 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 831us/step - loss: 0.3642 - f1_m: 0.8665 - val_loss: 0.2043 - val_f1_m: 0.9912\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 809us/step - loss: 0.2646 - f1_m: 0.9467 - val_loss: 0.1719 - val_f1_m: 0.9697\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 814us/step - loss: 0.2059 - f1_m: 0.9670 - val_loss: 0.1366 - val_f1_m: 0.9805\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 804us/step - loss: 0.1688 - f1_m: 0.9772 - val_loss: 0.1116 - val_f1_m: 0.9863\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 782us/step - loss: 0.1435 - f1_m: 0.9833 - val_loss: 0.0887 - val_f1_m: 0.9941\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 835us/step - loss: 0.1251 - f1_m: 0.9870 - val_loss: 0.0833 - val_f1_m: 0.9922\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 772us/step - loss: 0.1117 - f1_m: 0.9902 - val_loss: 0.0726 - val_f1_m: 0.9951\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 784us/step - loss: 0.1011 - f1_m: 0.9908 - val_loss: 0.0709 - val_f1_m: 0.9902\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 780us/step - loss: 0.0925 - f1_m: 0.9933 - val_loss: 0.0644 - val_f1_m: 0.9902\n",
      "0.9932500123977661\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.5519 - f1_m: 0.7177 - val_loss: 0.3016 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 943us/step - loss: 0.3669 - f1_m: 0.8750 - val_loss: 0.2030 - val_f1_m: 0.9932\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 941us/step - loss: 0.2670 - f1_m: 0.9435 - val_loss: 0.1635 - val_f1_m: 0.9844\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 873us/step - loss: 0.2075 - f1_m: 0.9657 - val_loss: 0.1315 - val_f1_m: 0.9873\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 838us/step - loss: 0.1693 - f1_m: 0.9760 - val_loss: 0.1130 - val_f1_m: 0.9893\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 872us/step - loss: 0.1434 - f1_m: 0.9820 - val_loss: 0.1025 - val_f1_m: 0.9854\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 793us/step - loss: 0.1244 - f1_m: 0.9850 - val_loss: 0.0879 - val_f1_m: 0.9854\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 783us/step - loss: 0.1104 - f1_m: 0.9890 - val_loss: 0.0764 - val_f1_m: 0.9902\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 798us/step - loss: 0.0999 - f1_m: 0.9895 - val_loss: 0.0687 - val_f1_m: 0.9912\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 783us/step - loss: 0.0911 - f1_m: 0.9912 - val_loss: 0.0648 - val_f1_m: 0.9893\n",
      "0.9912499785423279\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.5446 - f1_m: 0.7190 - val_loss: 0.2784 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 825us/step - loss: 0.3595 - f1_m: 0.8705 - val_loss: 0.2100 - val_f1_m: 0.9863\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 783us/step - loss: 0.2598 - f1_m: 0.9490 - val_loss: 0.1545 - val_f1_m: 0.9883\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 776us/step - loss: 0.2013 - f1_m: 0.9645 - val_loss: 0.1324 - val_f1_m: 0.9854\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 787us/step - loss: 0.1641 - f1_m: 0.9778 - val_loss: 0.1134 - val_f1_m: 0.9883\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 775us/step - loss: 0.1387 - f1_m: 0.9862 - val_loss: 0.0881 - val_f1_m: 0.9951\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 796us/step - loss: 0.1206 - f1_m: 0.9898 - val_loss: 0.0829 - val_f1_m: 0.9941\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 775us/step - loss: 0.1071 - f1_m: 0.9908 - val_loss: 0.0783 - val_f1_m: 0.9902\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 770us/step - loss: 0.0968 - f1_m: 0.9908 - val_loss: 0.0760 - val_f1_m: 0.9834\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 774us/step - loss: 0.0890 - f1_m: 0.9923 - val_loss: 0.0630 - val_f1_m: 0.9912\n",
      "0.9922500252723694\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5487 - f1_m: 0.7187 - val_loss: 0.2813 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 805us/step - loss: 0.3606 - f1_m: 0.8732 - val_loss: 0.2032 - val_f1_m: 0.9873\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 788us/step - loss: 0.2595 - f1_m: 0.9525 - val_loss: 0.1343 - val_f1_m: 0.9951\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 785us/step - loss: 0.2010 - f1_m: 0.9660 - val_loss: 0.1163 - val_f1_m: 0.9912\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 809us/step - loss: 0.1637 - f1_m: 0.9783 - val_loss: 0.1007 - val_f1_m: 0.9902\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 836us/step - loss: 0.1383 - f1_m: 0.9868 - val_loss: 0.0965 - val_f1_m: 0.9844\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 802us/step - loss: 0.1203 - f1_m: 0.9908 - val_loss: 0.0721 - val_f1_m: 0.9980\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 780us/step - loss: 0.1068 - f1_m: 0.9910 - val_loss: 0.0639 - val_f1_m: 0.9980\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 793us/step - loss: 0.0962 - f1_m: 0.9908 - val_loss: 0.0536 - val_f1_m: 0.9990\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 773us/step - loss: 0.0877 - f1_m: 0.9905 - val_loss: 0.0583 - val_f1_m: 0.9961\n",
      "0.9904999732971191\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 1s 2ms/step - loss: 0.5488 - f1_m: 0.7207 - val_loss: 0.2959 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.3605 - f1_m: 0.8727 - val_loss: 0.2134 - val_f1_m: 0.9863\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.2607 - f1_m: 0.9505 - val_loss: 0.1599 - val_f1_m: 0.9844\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 820us/step - loss: 0.2025 - f1_m: 0.9693 - val_loss: 0.1266 - val_f1_m: 0.9873\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 775us/step - loss: 0.1649 - f1_m: 0.9803 - val_loss: 0.1253 - val_f1_m: 0.9727\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 793us/step - loss: 0.1397 - f1_m: 0.9872 - val_loss: 0.0882 - val_f1_m: 0.9961\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 754us/step - loss: 0.1213 - f1_m: 0.9898 - val_loss: 0.0832 - val_f1_m: 0.9912\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 781us/step - loss: 0.1074 - f1_m: 0.9910 - val_loss: 0.0729 - val_f1_m: 0.9912\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 818us/step - loss: 0.0967 - f1_m: 0.9920 - val_loss: 0.0638 - val_f1_m: 0.9951\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 807us/step - loss: 0.0882 - f1_m: 0.9915 - val_loss: 0.0547 - val_f1_m: 0.9961\n",
      "0.9915000200271606\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5496 - f1_m: 0.7235 - val_loss: 0.2975 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 781us/step - loss: 0.3644 - f1_m: 0.8707 - val_loss: 0.2043 - val_f1_m: 0.9893\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 785us/step - loss: 0.2648 - f1_m: 0.9463 - val_loss: 0.1718 - val_f1_m: 0.9775\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 766us/step - loss: 0.2070 - f1_m: 0.9682 - val_loss: 0.1387 - val_f1_m: 0.9766\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 757us/step - loss: 0.1698 - f1_m: 0.9775 - val_loss: 0.1072 - val_f1_m: 0.9863\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 756us/step - loss: 0.1441 - f1_m: 0.9830 - val_loss: 0.0971 - val_f1_m: 0.9863\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 769us/step - loss: 0.1255 - f1_m: 0.9858 - val_loss: 0.0806 - val_f1_m: 0.9932\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 773us/step - loss: 0.1116 - f1_m: 0.9883 - val_loss: 0.0811 - val_f1_m: 0.9834\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 788us/step - loss: 0.1006 - f1_m: 0.9883 - val_loss: 0.0795 - val_f1_m: 0.9775\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 787us/step - loss: 0.0923 - f1_m: 0.9885 - val_loss: 0.0639 - val_f1_m: 0.9893\n",
      "0.9884999990463257\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5484 - f1_m: 0.7187 - val_loss: 0.2778 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 776us/step - loss: 0.3589 - f1_m: 0.8687 - val_loss: 0.1893 - val_f1_m: 0.9961\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 771us/step - loss: 0.2596 - f1_m: 0.9527 - val_loss: 0.1444 - val_f1_m: 0.9854\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 773us/step - loss: 0.2015 - f1_m: 0.9695 - val_loss: 0.1306 - val_f1_m: 0.9736\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 766us/step - loss: 0.1646 - f1_m: 0.9787 - val_loss: 0.0928 - val_f1_m: 0.9893\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 749us/step - loss: 0.1393 - f1_m: 0.9860 - val_loss: 0.0823 - val_f1_m: 0.9883\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 768us/step - loss: 0.1210 - f1_m: 0.9890 - val_loss: 0.0724 - val_f1_m: 0.9902\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 751us/step - loss: 0.1074 - f1_m: 0.9900 - val_loss: 0.0636 - val_f1_m: 0.9932\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 773us/step - loss: 0.0971 - f1_m: 0.9898 - val_loss: 0.0591 - val_f1_m: 0.9932\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 740us/step - loss: 0.0883 - f1_m: 0.9893 - val_loss: 0.0550 - val_f1_m: 0.9922\n",
      "0.9892500042915344\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5509 - f1_m: 0.7240 - val_loss: 0.3041 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 783us/step - loss: 0.3645 - f1_m: 0.8690 - val_loss: 0.2183 - val_f1_m: 0.9873\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 777us/step - loss: 0.2640 - f1_m: 0.9480 - val_loss: 0.1604 - val_f1_m: 0.9863\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 773us/step - loss: 0.2046 - f1_m: 0.9712 - val_loss: 0.1198 - val_f1_m: 0.9951\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 773us/step - loss: 0.1673 - f1_m: 0.9803 - val_loss: 0.1163 - val_f1_m: 0.9844\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 769us/step - loss: 0.1418 - f1_m: 0.9858 - val_loss: 0.0932 - val_f1_m: 0.9922\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 795us/step - loss: 0.1234 - f1_m: 0.9883 - val_loss: 0.0854 - val_f1_m: 0.9873\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 758us/step - loss: 0.1097 - f1_m: 0.9880 - val_loss: 0.0691 - val_f1_m: 0.9961\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 765us/step - loss: 0.0994 - f1_m: 0.9908 - val_loss: 0.0730 - val_f1_m: 0.9814\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 757us/step - loss: 0.0909 - f1_m: 0.9890 - val_loss: 0.0736 - val_f1_m: 0.9746\n",
      "0.9890000224113464\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.5467 - f1_m: 0.7182 - val_loss: 0.2691 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 784us/step - loss: 0.3660 - f1_m: 0.8590 - val_loss: 0.2060 - val_f1_m: 0.9844\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 773us/step - loss: 0.2669 - f1_m: 0.9487 - val_loss: 0.1504 - val_f1_m: 0.9854\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 756us/step - loss: 0.2083 - f1_m: 0.9672 - val_loss: 0.1212 - val_f1_m: 0.9883\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 762us/step - loss: 0.1706 - f1_m: 0.9775 - val_loss: 0.1093 - val_f1_m: 0.9824\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 758us/step - loss: 0.1453 - f1_m: 0.9812 - val_loss: 0.1016 - val_f1_m: 0.9785\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 766us/step - loss: 0.1266 - f1_m: 0.9880 - val_loss: 0.0723 - val_f1_m: 0.9990\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 965us/step - loss: 0.1133 - f1_m: 0.9880 - val_loss: 0.0732 - val_f1_m: 0.9932\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 789us/step - loss: 0.1021 - f1_m: 0.9883 - val_loss: 0.0684 - val_f1_m: 0.9922\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 781us/step - loss: 0.0936 - f1_m: 0.9885 - val_loss: 0.0617 - val_f1_m: 0.9932\n",
      "0.9884999990463257\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.5502 - f1_m: 0.7162 - val_loss: 0.2834 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 798us/step - loss: 0.3627 - f1_m: 0.8672 - val_loss: 0.2121 - val_f1_m: 0.9854\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 797us/step - loss: 0.2619 - f1_m: 0.9505 - val_loss: 0.1542 - val_f1_m: 0.9883\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 778us/step - loss: 0.2037 - f1_m: 0.9663 - val_loss: 0.1360 - val_f1_m: 0.9746\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 785us/step - loss: 0.1670 - f1_m: 0.9787 - val_loss: 0.1048 - val_f1_m: 0.9932\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 784us/step - loss: 0.1419 - f1_m: 0.9850 - val_loss: 0.0882 - val_f1_m: 0.9971\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 785us/step - loss: 0.1241 - f1_m: 0.9868 - val_loss: 0.0858 - val_f1_m: 0.9902\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 786us/step - loss: 0.1105 - f1_m: 0.9880 - val_loss: 0.0804 - val_f1_m: 0.9854\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - 0s 783us/step - loss: 0.1002 - f1_m: 0.9872 - val_loss: 0.0687 - val_f1_m: 0.9932\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 789us/step - loss: 0.0916 - f1_m: 0.9885 - val_loss: 0.0607 - val_f1_m: 0.9951\n",
      "0.9884999990463257\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5476 - f1_m: 0.7197 - val_loss: 0.2705 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 841us/step - loss: 0.3611 - f1_m: 0.8690 - val_loss: 0.1955 - val_f1_m: 0.9922\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 797us/step - loss: 0.2619 - f1_m: 0.9435 - val_loss: 0.1560 - val_f1_m: 0.9746\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 805us/step - loss: 0.2058 - f1_m: 0.9663 - val_loss: 0.1051 - val_f1_m: 0.9941\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 826us/step - loss: 0.1701 - f1_m: 0.9750 - val_loss: 0.0898 - val_f1_m: 0.9951\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 808us/step - loss: 0.1447 - f1_m: 0.9840 - val_loss: 0.0844 - val_f1_m: 0.9912\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 987us/step - loss: 0.1268 - f1_m: 0.9862 - val_loss: 0.0792 - val_f1_m: 0.9873\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 965us/step - loss: 0.1134 - f1_m: 0.9893 - val_loss: 0.0582 - val_f1_m: 0.9951\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 938us/step - loss: 0.1026 - f1_m: 0.9895 - val_loss: 0.0528 - val_f1_m: 0.9951\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 861us/step - loss: 0.0941 - f1_m: 0.9905 - val_loss: 0.0516 - val_f1_m: 0.9941\n",
      "0.9904999732971191\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.5478 - f1_m: 0.7217 - val_loss: 0.2833 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 872us/step - loss: 0.3585 - f1_m: 0.8737 - val_loss: 0.2104 - val_f1_m: 0.9814\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 910us/step - loss: 0.2595 - f1_m: 0.9517 - val_loss: 0.1539 - val_f1_m: 0.9834\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 871us/step - loss: 0.2019 - f1_m: 0.9643 - val_loss: 0.1303 - val_f1_m: 0.9805\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 809us/step - loss: 0.1656 - f1_m: 0.9755 - val_loss: 0.1074 - val_f1_m: 0.9863\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 781us/step - loss: 0.1408 - f1_m: 0.9833 - val_loss: 0.0886 - val_f1_m: 0.9912\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 791us/step - loss: 0.1231 - f1_m: 0.9880 - val_loss: 0.0805 - val_f1_m: 0.9902\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 859us/step - loss: 0.1095 - f1_m: 0.9898 - val_loss: 0.0717 - val_f1_m: 0.9922\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 837us/step - loss: 0.0992 - f1_m: 0.9895 - val_loss: 0.0728 - val_f1_m: 0.9834\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 773us/step - loss: 0.0907 - f1_m: 0.9902 - val_loss: 0.0599 - val_f1_m: 0.9912\n",
      "0.9902499914169312\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.5528 - f1_m: 0.7177 - val_loss: 0.2972 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 847us/step - loss: 0.3605 - f1_m: 0.8705 - val_loss: 0.2159 - val_f1_m: 0.9844\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 904us/step - loss: 0.2589 - f1_m: 0.9530 - val_loss: 0.1548 - val_f1_m: 0.9873\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 822us/step - loss: 0.2001 - f1_m: 0.9693 - val_loss: 0.1358 - val_f1_m: 0.9824\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 819us/step - loss: 0.1630 - f1_m: 0.9795 - val_loss: 0.1104 - val_f1_m: 0.9902\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 791us/step - loss: 0.1379 - f1_m: 0.9855 - val_loss: 0.0992 - val_f1_m: 0.9873\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 776us/step - loss: 0.1200 - f1_m: 0.9890 - val_loss: 0.0893 - val_f1_m: 0.9863\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 783us/step - loss: 0.1070 - f1_m: 0.9875 - val_loss: 0.0703 - val_f1_m: 0.9971\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 793us/step - loss: 0.0964 - f1_m: 0.9877 - val_loss: 0.0665 - val_f1_m: 0.9951\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 778us/step - loss: 0.0886 - f1_m: 0.9860 - val_loss: 0.0562 - val_f1_m: 0.9971\n",
      "0.9860000014305115\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5462 - f1_m: 0.7207 - val_loss: 0.2805 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 817us/step - loss: 0.3588 - f1_m: 0.8757 - val_loss: 0.2012 - val_f1_m: 0.9902\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 789us/step - loss: 0.2586 - f1_m: 0.9490 - val_loss: 0.1490 - val_f1_m: 0.9893\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 781us/step - loss: 0.2002 - f1_m: 0.9675 - val_loss: 0.1179 - val_f1_m: 0.9932\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 781us/step - loss: 0.1631 - f1_m: 0.9785 - val_loss: 0.1012 - val_f1_m: 0.9951\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 771us/step - loss: 0.1380 - f1_m: 0.9860 - val_loss: 0.0814 - val_f1_m: 0.9971\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 775us/step - loss: 0.1204 - f1_m: 0.9900 - val_loss: 0.0713 - val_f1_m: 0.9990\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 773us/step - loss: 0.1069 - f1_m: 0.9910 - val_loss: 0.0657 - val_f1_m: 0.9922\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 766us/step - loss: 0.0968 - f1_m: 0.9883 - val_loss: 0.0574 - val_f1_m: 0.9932\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 773us/step - loss: 0.0880 - f1_m: 0.9915 - val_loss: 0.0552 - val_f1_m: 0.9922\n",
      "0.9915000200271606\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.5518 - f1_m: 0.7182 - val_loss: 0.3092 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 830us/step - loss: 0.3620 - f1_m: 0.8755 - val_loss: 0.2169 - val_f1_m: 0.9863\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 829us/step - loss: 0.2608 - f1_m: 0.9475 - val_loss: 0.1601 - val_f1_m: 0.9873\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 805us/step - loss: 0.2028 - f1_m: 0.9622 - val_loss: 0.1355 - val_f1_m: 0.9854\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 838us/step - loss: 0.1664 - f1_m: 0.9743 - val_loss: 0.1178 - val_f1_m: 0.9854\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 825us/step - loss: 0.1414 - f1_m: 0.9803 - val_loss: 0.0930 - val_f1_m: 0.9922\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 829us/step - loss: 0.1238 - f1_m: 0.9843 - val_loss: 0.0798 - val_f1_m: 0.9932\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 859us/step - loss: 0.1103 - f1_m: 0.9862 - val_loss: 0.0800 - val_f1_m: 0.9883\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 831us/step - loss: 0.0997 - f1_m: 0.9862 - val_loss: 0.0665 - val_f1_m: 0.9941\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 820us/step - loss: 0.0916 - f1_m: 0.9872 - val_loss: 0.0644 - val_f1_m: 0.9902\n",
      "0.9872499704360962\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.5474 - f1_m: 0.7247 - val_loss: 0.2725 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 801us/step - loss: 0.3621 - f1_m: 0.8720 - val_loss: 0.1863 - val_f1_m: 1.0000\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 784us/step - loss: 0.2627 - f1_m: 0.9485 - val_loss: 0.1427 - val_f1_m: 0.9971\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 776us/step - loss: 0.2044 - f1_m: 0.9682 - val_loss: 0.1143 - val_f1_m: 0.9990\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 760us/step - loss: 0.1667 - f1_m: 0.9783 - val_loss: 0.0893 - val_f1_m: 0.9990\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 777us/step - loss: 0.1412 - f1_m: 0.9805 - val_loss: 0.0805 - val_f1_m: 0.9990\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 780us/step - loss: 0.1223 - f1_m: 0.9898 - val_loss: 0.0630 - val_f1_m: 1.0000\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 772us/step - loss: 0.1086 - f1_m: 0.9880 - val_loss: 0.0677 - val_f1_m: 0.9961\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 779us/step - loss: 0.0979 - f1_m: 0.9883 - val_loss: 0.0615 - val_f1_m: 0.9941\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 778us/step - loss: 0.0893 - f1_m: 0.9902 - val_loss: 0.0569 - val_f1_m: 0.9941\n",
      "0.9902499914169312\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5507 - f1_m: 0.7222 - val_loss: 0.2809 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 789us/step - loss: 0.3607 - f1_m: 0.8733 - val_loss: 0.1876 - val_f1_m: 0.9980\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 784us/step - loss: 0.2604 - f1_m: 0.9505 - val_loss: 0.1447 - val_f1_m: 0.9893\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 780us/step - loss: 0.2027 - f1_m: 0.9685 - val_loss: 0.1252 - val_f1_m: 0.9805\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 770us/step - loss: 0.1663 - f1_m: 0.9797 - val_loss: 0.1009 - val_f1_m: 0.9902\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 773us/step - loss: 0.1413 - f1_m: 0.9843 - val_loss: 0.0797 - val_f1_m: 0.9951\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 770us/step - loss: 0.1237 - f1_m: 0.9843 - val_loss: 0.0778 - val_f1_m: 0.9893\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 786us/step - loss: 0.1099 - f1_m: 0.9875 - val_loss: 0.0703 - val_f1_m: 0.9902\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 789us/step - loss: 0.0997 - f1_m: 0.9887 - val_loss: 0.0548 - val_f1_m: 0.9971\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 781us/step - loss: 0.0915 - f1_m: 0.9883 - val_loss: 0.0626 - val_f1_m: 0.9873\n",
      "0.9882500171661377\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5490 - f1_m: 0.7195 - val_loss: 0.2848 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 786us/step - loss: 0.3622 - f1_m: 0.8702 - val_loss: 0.2093 - val_f1_m: 0.9834\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 784us/step - loss: 0.2639 - f1_m: 0.9465 - val_loss: 0.1620 - val_f1_m: 0.9746\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 750us/step - loss: 0.2060 - f1_m: 0.9675 - val_loss: 0.1306 - val_f1_m: 0.9775\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 905us/step - loss: 0.1686 - f1_m: 0.9747 - val_loss: 0.1073 - val_f1_m: 0.9824\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 805us/step - loss: 0.1434 - f1_m: 0.9822 - val_loss: 0.0999 - val_f1_m: 0.9766\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 773us/step - loss: 0.1252 - f1_m: 0.9852 - val_loss: 0.0734 - val_f1_m: 0.9971\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 768us/step - loss: 0.1123 - f1_m: 0.9843 - val_loss: 0.0702 - val_f1_m: 0.9951\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 780us/step - loss: 0.1019 - f1_m: 0.9875 - val_loss: 0.0751 - val_f1_m: 0.9805\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 775us/step - loss: 0.0935 - f1_m: 0.9887 - val_loss: 0.0614 - val_f1_m: 0.9941\n",
      "0.9887499809265137\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5516 - f1_m: 0.7210 - val_loss: 0.2970 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 784us/step - loss: 0.3646 - f1_m: 0.8747 - val_loss: 0.1987 - val_f1_m: 0.9912\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 785us/step - loss: 0.2654 - f1_m: 0.9470 - val_loss: 0.1515 - val_f1_m: 0.9902\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 793us/step - loss: 0.2068 - f1_m: 0.9610 - val_loss: 0.1306 - val_f1_m: 0.9873\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 772us/step - loss: 0.1694 - f1_m: 0.9720 - val_loss: 0.1118 - val_f1_m: 0.9893\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 778us/step - loss: 0.1437 - f1_m: 0.9825 - val_loss: 0.0902 - val_f1_m: 0.9932\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 787us/step - loss: 0.1253 - f1_m: 0.9872 - val_loss: 0.0851 - val_f1_m: 0.9893\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 767us/step - loss: 0.1112 - f1_m: 0.9868 - val_loss: 0.0624 - val_f1_m: 1.0000\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 778us/step - loss: 0.1010 - f1_m: 0.9883 - val_loss: 0.0668 - val_f1_m: 0.9932\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 763us/step - loss: 0.0921 - f1_m: 0.9890 - val_loss: 0.0670 - val_f1_m: 0.9902\n",
      "0.9890000224113464\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5488 - f1_m: 0.7192 - val_loss: 0.2858 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 788us/step - loss: 0.3626 - f1_m: 0.8780 - val_loss: 0.1885 - val_f1_m: 0.9990\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 804us/step - loss: 0.2620 - f1_m: 0.9435 - val_loss: 0.1741 - val_f1_m: 0.9746\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 782us/step - loss: 0.2043 - f1_m: 0.9670 - val_loss: 0.1211 - val_f1_m: 0.9922\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 781us/step - loss: 0.1675 - f1_m: 0.9787 - val_loss: 0.1069 - val_f1_m: 0.9902\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 796us/step - loss: 0.1425 - f1_m: 0.9890 - val_loss: 0.0901 - val_f1_m: 0.9932\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 789us/step - loss: 0.1245 - f1_m: 0.9910 - val_loss: 0.0787 - val_f1_m: 0.9922\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 784us/step - loss: 0.1112 - f1_m: 0.9883 - val_loss: 0.0667 - val_f1_m: 0.9951\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 803us/step - loss: 0.1008 - f1_m: 0.9910 - val_loss: 0.0665 - val_f1_m: 0.9893\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 789us/step - loss: 0.0921 - f1_m: 0.9920 - val_loss: 0.0566 - val_f1_m: 0.9951\n",
      "0.9919999837875366\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5517 - f1_m: 0.7167 - val_loss: 0.2969 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 791us/step - loss: 0.3649 - f1_m: 0.8675 - val_loss: 0.2091 - val_f1_m: 0.9814\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 789us/step - loss: 0.2638 - f1_m: 0.9522 - val_loss: 0.1511 - val_f1_m: 0.9824\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 777us/step - loss: 0.2044 - f1_m: 0.9688 - val_loss: 0.1151 - val_f1_m: 0.9883\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 775us/step - loss: 0.1668 - f1_m: 0.9775 - val_loss: 0.0985 - val_f1_m: 0.9883\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 771us/step - loss: 0.1411 - f1_m: 0.9870 - val_loss: 0.0970 - val_f1_m: 0.9805\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 785us/step - loss: 0.1230 - f1_m: 0.9883 - val_loss: 0.0778 - val_f1_m: 0.9873\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 773us/step - loss: 0.1093 - f1_m: 0.9902 - val_loss: 0.0700 - val_f1_m: 0.9873\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 777us/step - loss: 0.0985 - f1_m: 0.9883 - val_loss: 0.0581 - val_f1_m: 0.9902\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 775us/step - loss: 0.0904 - f1_m: 0.9887 - val_loss: 0.0629 - val_f1_m: 0.9814\n",
      "0.9887499809265137\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5457 - f1_m: 0.7237 - val_loss: 0.2814 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 795us/step - loss: 0.3589 - f1_m: 0.8657 - val_loss: 0.1944 - val_f1_m: 0.9932\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 790us/step - loss: 0.2594 - f1_m: 0.9515 - val_loss: 0.1456 - val_f1_m: 0.9824\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 786us/step - loss: 0.2018 - f1_m: 0.9660 - val_loss: 0.1190 - val_f1_m: 0.9834\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 772us/step - loss: 0.1646 - f1_m: 0.9780 - val_loss: 0.1016 - val_f1_m: 0.9854\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 781us/step - loss: 0.1391 - f1_m: 0.9837 - val_loss: 0.1068 - val_f1_m: 0.9688\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - 0s 781us/step - loss: 0.1212 - f1_m: 0.9877 - val_loss: 0.0919 - val_f1_m: 0.9736\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 797us/step - loss: 0.1075 - f1_m: 0.9890 - val_loss: 0.0716 - val_f1_m: 0.9873\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 778us/step - loss: 0.0970 - f1_m: 0.9908 - val_loss: 0.0707 - val_f1_m: 0.9795\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 769us/step - loss: 0.0888 - f1_m: 0.9895 - val_loss: 0.0562 - val_f1_m: 0.9941\n",
      "0.9894999861717224\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5510 - f1_m: 0.7215 - val_loss: 0.2891 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 783us/step - loss: 0.3627 - f1_m: 0.8652 - val_loss: 0.2142 - val_f1_m: 0.9883\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 795us/step - loss: 0.2634 - f1_m: 0.9500 - val_loss: 0.1477 - val_f1_m: 0.9932\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 766us/step - loss: 0.2056 - f1_m: 0.9663 - val_loss: 0.1193 - val_f1_m: 0.9912\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 781us/step - loss: 0.1689 - f1_m: 0.9760 - val_loss: 0.1098 - val_f1_m: 0.9873\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 772us/step - loss: 0.1437 - f1_m: 0.9835 - val_loss: 0.0942 - val_f1_m: 0.9902\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 774us/step - loss: 0.1254 - f1_m: 0.9858 - val_loss: 0.0757 - val_f1_m: 0.9961\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 788us/step - loss: 0.1119 - f1_m: 0.9877 - val_loss: 0.0710 - val_f1_m: 0.9951\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 765us/step - loss: 0.1013 - f1_m: 0.9887 - val_loss: 0.0740 - val_f1_m: 0.9805\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 790us/step - loss: 0.0931 - f1_m: 0.9887 - val_loss: 0.0617 - val_f1_m: 0.9932\n",
      "0.9887499809265137\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5513 - f1_m: 0.7180 - val_loss: 0.2751 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 805us/step - loss: 0.3669 - f1_m: 0.8645 - val_loss: 0.1985 - val_f1_m: 0.9883\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 791us/step - loss: 0.2669 - f1_m: 0.9457 - val_loss: 0.1488 - val_f1_m: 0.9814\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 770us/step - loss: 0.2077 - f1_m: 0.9650 - val_loss: 0.1230 - val_f1_m: 0.9844\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 774us/step - loss: 0.1698 - f1_m: 0.9762 - val_loss: 0.1189 - val_f1_m: 0.9727\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 773us/step - loss: 0.1447 - f1_m: 0.9847 - val_loss: 0.0838 - val_f1_m: 0.9971\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 775us/step - loss: 0.1265 - f1_m: 0.9850 - val_loss: 0.0857 - val_f1_m: 0.9893\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 803us/step - loss: 0.1127 - f1_m: 0.9885 - val_loss: 0.0684 - val_f1_m: 0.9961\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 776us/step - loss: 0.1020 - f1_m: 0.9902 - val_loss: 0.0615 - val_f1_m: 0.9961\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 774us/step - loss: 0.0937 - f1_m: 0.9893 - val_loss: 0.0562 - val_f1_m: 0.9980\n",
      "0.9892500042915344\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5526 - f1_m: 0.7185 - val_loss: 0.2877 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 791us/step - loss: 0.3699 - f1_m: 0.8667 - val_loss: 0.1927 - val_f1_m: 0.9951\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 784us/step - loss: 0.2710 - f1_m: 0.9413 - val_loss: 0.1490 - val_f1_m: 0.9873\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 785us/step - loss: 0.2120 - f1_m: 0.9595 - val_loss: 0.1191 - val_f1_m: 0.9902\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 780us/step - loss: 0.1735 - f1_m: 0.9753 - val_loss: 0.1123 - val_f1_m: 0.9775\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 778us/step - loss: 0.1477 - f1_m: 0.9827 - val_loss: 0.0868 - val_f1_m: 0.9902\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 787us/step - loss: 0.1284 - f1_m: 0.9865 - val_loss: 0.0836 - val_f1_m: 0.9844\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 786us/step - loss: 0.1144 - f1_m: 0.9880 - val_loss: 0.0621 - val_f1_m: 0.9941\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 799us/step - loss: 0.1034 - f1_m: 0.9895 - val_loss: 0.0555 - val_f1_m: 0.9951\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 796us/step - loss: 0.0944 - f1_m: 0.9898 - val_loss: 0.0521 - val_f1_m: 0.9912\n",
      "0.9897500276565552\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.5448 - f1_m: 0.7225 - val_loss: 0.2836 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 818us/step - loss: 0.3539 - f1_m: 0.8770 - val_loss: 0.2245 - val_f1_m: 0.9795\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 790us/step - loss: 0.2544 - f1_m: 0.9578 - val_loss: 0.1652 - val_f1_m: 0.9824\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 811us/step - loss: 0.1979 - f1_m: 0.9707 - val_loss: 0.1192 - val_f1_m: 0.9951\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 771us/step - loss: 0.1625 - f1_m: 0.9750 - val_loss: 0.1075 - val_f1_m: 0.9902\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 792us/step - loss: 0.1386 - f1_m: 0.9787 - val_loss: 0.0932 - val_f1_m: 0.9912\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 782us/step - loss: 0.1217 - f1_m: 0.9837 - val_loss: 0.0810 - val_f1_m: 0.9941\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 792us/step - loss: 0.1086 - f1_m: 0.9847 - val_loss: 0.0718 - val_f1_m: 0.9951\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 792us/step - loss: 0.0986 - f1_m: 0.9862 - val_loss: 0.0638 - val_f1_m: 0.9951\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 796us/step - loss: 0.0909 - f1_m: 0.9850 - val_loss: 0.0624 - val_f1_m: 0.9922\n",
      "0.9850000143051147\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5486 - f1_m: 0.7165 - val_loss: 0.2828 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 796us/step - loss: 0.3635 - f1_m: 0.8672 - val_loss: 0.2173 - val_f1_m: 0.9814\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 793us/step - loss: 0.2641 - f1_m: 0.9507 - val_loss: 0.1566 - val_f1_m: 0.9834\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 781us/step - loss: 0.2054 - f1_m: 0.9680 - val_loss: 0.1395 - val_f1_m: 0.9766\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 782us/step - loss: 0.1681 - f1_m: 0.9780 - val_loss: 0.1042 - val_f1_m: 0.9873\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 790us/step - loss: 0.1428 - f1_m: 0.9845 - val_loss: 0.0902 - val_f1_m: 0.9912\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 770us/step - loss: 0.1246 - f1_m: 0.9902 - val_loss: 0.0815 - val_f1_m: 0.9893\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 783us/step - loss: 0.1110 - f1_m: 0.9915 - val_loss: 0.0739 - val_f1_m: 0.9863\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 780us/step - loss: 0.1004 - f1_m: 0.9908 - val_loss: 0.0684 - val_f1_m: 0.9834\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 772us/step - loss: 0.0921 - f1_m: 0.9910 - val_loss: 0.0613 - val_f1_m: 0.9883\n",
      "0.9909999966621399\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5470 - f1_m: 0.7207 - val_loss: 0.2824 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 817us/step - loss: 0.3602 - f1_m: 0.8735 - val_loss: 0.2004 - val_f1_m: 0.9932\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 821us/step - loss: 0.2600 - f1_m: 0.9427 - val_loss: 0.1639 - val_f1_m: 0.9824\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 796us/step - loss: 0.2026 - f1_m: 0.9672 - val_loss: 0.1200 - val_f1_m: 0.9932\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 808us/step - loss: 0.1661 - f1_m: 0.9743 - val_loss: 0.1078 - val_f1_m: 0.9912\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 788us/step - loss: 0.1414 - f1_m: 0.9805 - val_loss: 0.1028 - val_f1_m: 0.9854\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 785us/step - loss: 0.1238 - f1_m: 0.9865 - val_loss: 0.0978 - val_f1_m: 0.9736\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 786us/step - loss: 0.1104 - f1_m: 0.9868 - val_loss: 0.0729 - val_f1_m: 0.9932\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 801us/step - loss: 0.1001 - f1_m: 0.9880 - val_loss: 0.0739 - val_f1_m: 0.9902\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 867us/step - loss: 0.0915 - f1_m: 0.9898 - val_loss: 0.0600 - val_f1_m: 0.9932\n",
      "0.9897500276565552\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.5471 - f1_m: 0.7245 - val_loss: 0.2813 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 806us/step - loss: 0.3592 - f1_m: 0.8777 - val_loss: 0.1841 - val_f1_m: 0.9961\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 795us/step - loss: 0.2601 - f1_m: 0.9492 - val_loss: 0.1345 - val_f1_m: 0.9941\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 780us/step - loss: 0.2026 - f1_m: 0.9710 - val_loss: 0.1258 - val_f1_m: 0.9775\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 779us/step - loss: 0.1662 - f1_m: 0.9795 - val_loss: 0.0977 - val_f1_m: 0.9912\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 778us/step - loss: 0.1413 - f1_m: 0.9850 - val_loss: 0.0833 - val_f1_m: 0.9941\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 774us/step - loss: 0.1233 - f1_m: 0.9908 - val_loss: 0.0727 - val_f1_m: 0.9932\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 777us/step - loss: 0.1099 - f1_m: 0.9918 - val_loss: 0.0626 - val_f1_m: 0.9951\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 768us/step - loss: 0.0993 - f1_m: 0.9918 - val_loss: 0.0668 - val_f1_m: 0.9805\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 774us/step - loss: 0.0912 - f1_m: 0.9927 - val_loss: 0.0541 - val_f1_m: 0.9912\n",
      "0.9927499890327454\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.5480 - f1_m: 0.7210 - val_loss: 0.2907 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 906us/step - loss: 0.3562 - f1_m: 0.8720 - val_loss: 0.2263 - val_f1_m: 0.9697\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 801us/step - loss: 0.2560 - f1_m: 0.9545 - val_loss: 0.1722 - val_f1_m: 0.9688\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 772us/step - loss: 0.1988 - f1_m: 0.9697 - val_loss: 0.1300 - val_f1_m: 0.9844\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 786us/step - loss: 0.1622 - f1_m: 0.9805 - val_loss: 0.1097 - val_f1_m: 0.9893\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 818us/step - loss: 0.1374 - f1_m: 0.9872 - val_loss: 0.1026 - val_f1_m: 0.9814\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 947us/step - loss: 0.1193 - f1_m: 0.9902 - val_loss: 0.0938 - val_f1_m: 0.9795\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 934us/step - loss: 0.1057 - f1_m: 0.9915 - val_loss: 0.0810 - val_f1_m: 0.9873\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 868us/step - loss: 0.0950 - f1_m: 0.9920 - val_loss: 0.0685 - val_f1_m: 0.9922\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 810us/step - loss: 0.0866 - f1_m: 0.9930 - val_loss: 0.0680 - val_f1_m: 0.9893\n",
      "0.9929999709129333\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5464 - f1_m: 0.7227 - val_loss: 0.2790 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 820us/step - loss: 0.3620 - f1_m: 0.8705 - val_loss: 0.1941 - val_f1_m: 0.9893\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 789us/step - loss: 0.2633 - f1_m: 0.9517 - val_loss: 0.1421 - val_f1_m: 0.9844\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 787us/step - loss: 0.2058 - f1_m: 0.9672 - val_loss: 0.1122 - val_f1_m: 0.9863\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 775us/step - loss: 0.1688 - f1_m: 0.9787 - val_loss: 0.0977 - val_f1_m: 0.9854\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 789us/step - loss: 0.1435 - f1_m: 0.9865 - val_loss: 0.0727 - val_f1_m: 0.9980\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 787us/step - loss: 0.1252 - f1_m: 0.9887 - val_loss: 0.0649 - val_f1_m: 0.9990\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 790us/step - loss: 0.1118 - f1_m: 0.9885 - val_loss: 0.0612 - val_f1_m: 0.9941\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 786us/step - loss: 0.1011 - f1_m: 0.9905 - val_loss: 0.0589 - val_f1_m: 0.9902\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 798us/step - loss: 0.0930 - f1_m: 0.9887 - val_loss: 0.0595 - val_f1_m: 0.9824\n",
      "0.9887499809265137\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5493 - f1_m: 0.7217 - val_loss: 0.3109 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 798us/step - loss: 0.3614 - f1_m: 0.8807 - val_loss: 0.2169 - val_f1_m: 0.9785\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 807us/step - loss: 0.2623 - f1_m: 0.9517 - val_loss: 0.1698 - val_f1_m: 0.9707\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 777us/step - loss: 0.2041 - f1_m: 0.9685 - val_loss: 0.1383 - val_f1_m: 0.9736\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 782us/step - loss: 0.1669 - f1_m: 0.9803 - val_loss: 0.0996 - val_f1_m: 1.0000\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 779us/step - loss: 0.1421 - f1_m: 0.9840 - val_loss: 0.0976 - val_f1_m: 0.9893\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 776us/step - loss: 0.1238 - f1_m: 0.9885 - val_loss: 0.0887 - val_f1_m: 0.9912\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 787us/step - loss: 0.1101 - f1_m: 0.9895 - val_loss: 0.0797 - val_f1_m: 0.9941\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 779us/step - loss: 0.0997 - f1_m: 0.9902 - val_loss: 0.0773 - val_f1_m: 0.9873\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 777us/step - loss: 0.0909 - f1_m: 0.9898 - val_loss: 0.0631 - val_f1_m: 0.9961\n",
      "0.9897500276565552\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.5480 - f1_m: 0.7155 - val_loss: 0.2782 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 792us/step - loss: 0.3638 - f1_m: 0.8672 - val_loss: 0.2092 - val_f1_m: 0.9814\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 794us/step - loss: 0.2636 - f1_m: 0.9495 - val_loss: 0.1536 - val_f1_m: 0.9824\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 776us/step - loss: 0.2042 - f1_m: 0.9665 - val_loss: 0.1207 - val_f1_m: 0.9844\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 785us/step - loss: 0.1659 - f1_m: 0.9790 - val_loss: 0.0943 - val_f1_m: 0.9941\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 782us/step - loss: 0.1403 - f1_m: 0.9843 - val_loss: 0.0881 - val_f1_m: 0.9912\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 783us/step - loss: 0.1215 - f1_m: 0.9893 - val_loss: 0.0796 - val_f1_m: 0.9893\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 812us/step - loss: 0.1079 - f1_m: 0.9902 - val_loss: 0.0737 - val_f1_m: 0.9883\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 797us/step - loss: 0.0973 - f1_m: 0.9920 - val_loss: 0.0587 - val_f1_m: 0.9961\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 773us/step - loss: 0.0888 - f1_m: 0.9920 - val_loss: 0.0633 - val_f1_m: 0.9863\n",
      "0.9919999837875366\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5504 - f1_m: 0.7187 - val_loss: 0.2866 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 772us/step - loss: 0.3624 - f1_m: 0.8665 - val_loss: 0.2118 - val_f1_m: 0.9775\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 800us/step - loss: 0.2635 - f1_m: 0.9435 - val_loss: 0.1749 - val_f1_m: 0.9658\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 762us/step - loss: 0.2067 - f1_m: 0.9630 - val_loss: 0.1249 - val_f1_m: 0.9854\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - 0s 767us/step - loss: 0.1699 - f1_m: 0.9768 - val_loss: 0.1088 - val_f1_m: 0.9844\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 789us/step - loss: 0.1452 - f1_m: 0.9830 - val_loss: 0.0984 - val_f1_m: 0.9844\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 771us/step - loss: 0.1263 - f1_m: 0.9910 - val_loss: 0.0889 - val_f1_m: 0.9854\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 775us/step - loss: 0.1126 - f1_m: 0.9927 - val_loss: 0.0735 - val_f1_m: 0.9941\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 775us/step - loss: 0.1015 - f1_m: 0.9927 - val_loss: 0.0680 - val_f1_m: 0.9922\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 777us/step - loss: 0.0927 - f1_m: 0.9918 - val_loss: 0.0651 - val_f1_m: 0.9902\n",
      "0.9917500019073486\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5490 - f1_m: 0.7205 - val_loss: 0.3061 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 798us/step - loss: 0.3604 - f1_m: 0.8782 - val_loss: 0.2147 - val_f1_m: 0.9697\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 789us/step - loss: 0.2593 - f1_m: 0.9495 - val_loss: 0.1531 - val_f1_m: 0.9766\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 776us/step - loss: 0.2009 - f1_m: 0.9682 - val_loss: 0.1279 - val_f1_m: 0.9775\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 777us/step - loss: 0.1643 - f1_m: 0.9795 - val_loss: 0.1089 - val_f1_m: 0.9795\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 780us/step - loss: 0.1397 - f1_m: 0.9868 - val_loss: 0.0926 - val_f1_m: 0.9844\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 796us/step - loss: 0.1220 - f1_m: 0.9895 - val_loss: 0.0818 - val_f1_m: 0.9863\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 802us/step - loss: 0.1086 - f1_m: 0.9898 - val_loss: 0.0740 - val_f1_m: 0.9863\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 800us/step - loss: 0.0983 - f1_m: 0.9927 - val_loss: 0.0700 - val_f1_m: 0.9814\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 790us/step - loss: 0.0902 - f1_m: 0.9912 - val_loss: 0.0584 - val_f1_m: 0.9951\n",
      "0.9912499785423279\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 1s 3ms/step - loss: 0.5467 - f1_m: 0.7192 - val_loss: 0.2918 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 812us/step - loss: 0.3604 - f1_m: 0.8720 - val_loss: 0.2075 - val_f1_m: 0.9922\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 809us/step - loss: 0.2610 - f1_m: 0.9515 - val_loss: 0.1650 - val_f1_m: 0.9795\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 782us/step - loss: 0.2024 - f1_m: 0.9690 - val_loss: 0.1278 - val_f1_m: 0.9854\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 798us/step - loss: 0.1654 - f1_m: 0.9760 - val_loss: 0.1096 - val_f1_m: 0.9854\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 786us/step - loss: 0.1402 - f1_m: 0.9860 - val_loss: 0.0908 - val_f1_m: 0.9932\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 794us/step - loss: 0.1222 - f1_m: 0.9868 - val_loss: 0.0856 - val_f1_m: 0.9893\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 799us/step - loss: 0.1084 - f1_m: 0.9860 - val_loss: 0.0819 - val_f1_m: 0.9785\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 797us/step - loss: 0.0980 - f1_m: 0.9890 - val_loss: 0.0779 - val_f1_m: 0.9756\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 798us/step - loss: 0.0899 - f1_m: 0.9872 - val_loss: 0.0648 - val_f1_m: 0.9873\n",
      "0.9872499704360962\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5512 - f1_m: 0.7172 - val_loss: 0.2871 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 809us/step - loss: 0.3685 - f1_m: 0.8537 - val_loss: 0.2070 - val_f1_m: 0.9883\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 812us/step - loss: 0.2682 - f1_m: 0.9417 - val_loss: 0.1711 - val_f1_m: 0.9678\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 805us/step - loss: 0.2092 - f1_m: 0.9703 - val_loss: 0.1239 - val_f1_m: 0.9824\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 802us/step - loss: 0.1716 - f1_m: 0.9780 - val_loss: 0.1079 - val_f1_m: 0.9795\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 812us/step - loss: 0.1465 - f1_m: 0.9837 - val_loss: 0.0891 - val_f1_m: 0.9893\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 797us/step - loss: 0.1280 - f1_m: 0.9877 - val_loss: 0.0686 - val_f1_m: 0.9990\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 813us/step - loss: 0.1141 - f1_m: 0.9883 - val_loss: 0.0663 - val_f1_m: 0.9941\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 800us/step - loss: 0.1034 - f1_m: 0.9902 - val_loss: 0.0646 - val_f1_m: 0.9873\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 787us/step - loss: 0.0947 - f1_m: 0.9912 - val_loss: 0.0588 - val_f1_m: 0.9883\n",
      "0.9912499785423279\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.5464 - f1_m: 0.7290 - val_loss: 0.2843 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 810us/step - loss: 0.3550 - f1_m: 0.8815 - val_loss: 0.2006 - val_f1_m: 0.9951\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 814us/step - loss: 0.2573 - f1_m: 0.9465 - val_loss: 0.1471 - val_f1_m: 0.9922\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 801us/step - loss: 0.2011 - f1_m: 0.9670 - val_loss: 0.1271 - val_f1_m: 0.9854\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 803us/step - loss: 0.1652 - f1_m: 0.9795 - val_loss: 0.0906 - val_f1_m: 0.9980\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 799us/step - loss: 0.1404 - f1_m: 0.9847 - val_loss: 0.0937 - val_f1_m: 0.9873\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 796us/step - loss: 0.1229 - f1_m: 0.9862 - val_loss: 0.0690 - val_f1_m: 0.9980\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 811us/step - loss: 0.1091 - f1_m: 0.9908 - val_loss: 0.0690 - val_f1_m: 0.9902\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 807us/step - loss: 0.0990 - f1_m: 0.9905 - val_loss: 0.0625 - val_f1_m: 0.9902\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 807us/step - loss: 0.0903 - f1_m: 0.9887 - val_loss: 0.0478 - val_f1_m: 0.9961\n",
      "0.9887499809265137\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.5475 - f1_m: 0.7247 - val_loss: 0.2827 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 820us/step - loss: 0.3556 - f1_m: 0.8845 - val_loss: 0.1819 - val_f1_m: 0.9912\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 802us/step - loss: 0.2559 - f1_m: 0.9507 - val_loss: 0.1443 - val_f1_m: 0.9834\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 799us/step - loss: 0.1989 - f1_m: 0.9700 - val_loss: 0.1086 - val_f1_m: 0.9863\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 845us/step - loss: 0.1629 - f1_m: 0.9793 - val_loss: 0.0936 - val_f1_m: 0.9854\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 810us/step - loss: 0.1382 - f1_m: 0.9852 - val_loss: 0.0752 - val_f1_m: 0.9922\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 806us/step - loss: 0.1206 - f1_m: 0.9893 - val_loss: 0.0658 - val_f1_m: 0.9932\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 812us/step - loss: 0.1077 - f1_m: 0.9887 - val_loss: 0.0660 - val_f1_m: 0.9893\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 835us/step - loss: 0.0973 - f1_m: 0.9900 - val_loss: 0.0594 - val_f1_m: 0.9893\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 810us/step - loss: 0.0893 - f1_m: 0.9910 - val_loss: 0.0473 - val_f1_m: 0.9961\n",
      "0.9909999966621399\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.5484 - f1_m: 0.7202 - val_loss: 0.2765 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 835us/step - loss: 0.3624 - f1_m: 0.8667 - val_loss: 0.2081 - val_f1_m: 0.9902\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 817us/step - loss: 0.2609 - f1_m: 0.9520 - val_loss: 0.1679 - val_f1_m: 0.9775\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 803us/step - loss: 0.2023 - f1_m: 0.9705 - val_loss: 0.1356 - val_f1_m: 0.9834\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 832us/step - loss: 0.1653 - f1_m: 0.9770 - val_loss: 0.1144 - val_f1_m: 0.9883\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 816us/step - loss: 0.1403 - f1_m: 0.9847 - val_loss: 0.1027 - val_f1_m: 0.9883\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 802us/step - loss: 0.1224 - f1_m: 0.9880 - val_loss: 0.0784 - val_f1_m: 0.9961\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 812us/step - loss: 0.1090 - f1_m: 0.9883 - val_loss: 0.0732 - val_f1_m: 0.9971\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 820us/step - loss: 0.0988 - f1_m: 0.9927 - val_loss: 0.0770 - val_f1_m: 0.9883\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 812us/step - loss: 0.0905 - f1_m: 0.9883 - val_loss: 0.0722 - val_f1_m: 0.9873\n",
      "0.9882500171661377\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.5518 - f1_m: 0.7237 - val_loss: 0.3050 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 822us/step - loss: 0.3594 - f1_m: 0.8947 - val_loss: 0.1931 - val_f1_m: 0.9922\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 822us/step - loss: 0.2586 - f1_m: 0.9510 - val_loss: 0.1599 - val_f1_m: 0.9785\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 836us/step - loss: 0.2010 - f1_m: 0.9672 - val_loss: 0.1215 - val_f1_m: 0.9844\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 838us/step - loss: 0.1645 - f1_m: 0.9770 - val_loss: 0.1006 - val_f1_m: 0.9912\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 836us/step - loss: 0.1394 - f1_m: 0.9827 - val_loss: 0.0825 - val_f1_m: 0.9951\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 813us/step - loss: 0.1218 - f1_m: 0.9875 - val_loss: 0.0682 - val_f1_m: 0.9980\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 798us/step - loss: 0.1081 - f1_m: 0.9905 - val_loss: 0.0735 - val_f1_m: 0.9893\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 822us/step - loss: 0.0971 - f1_m: 0.9912 - val_loss: 0.0786 - val_f1_m: 0.9678\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 827us/step - loss: 0.0891 - f1_m: 0.9900 - val_loss: 0.0586 - val_f1_m: 0.9912\n",
      "0.9900000095367432\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.5472 - f1_m: 0.7262 - val_loss: 0.2970 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 807us/step - loss: 0.3579 - f1_m: 0.8747 - val_loss: 0.2017 - val_f1_m: 0.9932\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 819us/step - loss: 0.2602 - f1_m: 0.9477 - val_loss: 0.1606 - val_f1_m: 0.9854\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 798us/step - loss: 0.2031 - f1_m: 0.9675 - val_loss: 0.1251 - val_f1_m: 0.9893\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 785us/step - loss: 0.1672 - f1_m: 0.9745 - val_loss: 0.1002 - val_f1_m: 0.9902\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 786us/step - loss: 0.1428 - f1_m: 0.9783 - val_loss: 0.0836 - val_f1_m: 0.9951\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 766us/step - loss: 0.1249 - f1_m: 0.9843 - val_loss: 0.0748 - val_f1_m: 0.9971\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 767us/step - loss: 0.1115 - f1_m: 0.9865 - val_loss: 0.0668 - val_f1_m: 0.9980\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 782us/step - loss: 0.1010 - f1_m: 0.9865 - val_loss: 0.0587 - val_f1_m: 0.9971\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 771us/step - loss: 0.0928 - f1_m: 0.9875 - val_loss: 0.0613 - val_f1_m: 0.9941\n",
      "0.987500011920929\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5446 - f1_m: 0.7202 - val_loss: 0.2680 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 789us/step - loss: 0.3616 - f1_m: 0.8692 - val_loss: 0.1933 - val_f1_m: 0.9961\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 790us/step - loss: 0.2619 - f1_m: 0.9455 - val_loss: 0.1594 - val_f1_m: 0.9814\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 785us/step - loss: 0.2030 - f1_m: 0.9680 - val_loss: 0.1141 - val_f1_m: 0.9912\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 781us/step - loss: 0.1656 - f1_m: 0.9743 - val_loss: 0.1088 - val_f1_m: 0.9834\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 764us/step - loss: 0.1407 - f1_m: 0.9827 - val_loss: 0.0910 - val_f1_m: 0.9883\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 781us/step - loss: 0.1222 - f1_m: 0.9860 - val_loss: 0.0743 - val_f1_m: 0.9912\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 776us/step - loss: 0.1087 - f1_m: 0.9890 - val_loss: 0.0766 - val_f1_m: 0.9854\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 775us/step - loss: 0.0981 - f1_m: 0.9893 - val_loss: 0.0572 - val_f1_m: 0.9961\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 766us/step - loss: 0.0899 - f1_m: 0.9895 - val_loss: 0.0565 - val_f1_m: 0.9941\n",
      "0.9894999861717224\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5541 - f1_m: 0.7200 - val_loss: 0.3017 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 796us/step - loss: 0.3688 - f1_m: 0.8725 - val_loss: 0.2000 - val_f1_m: 0.9961\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 784us/step - loss: 0.2698 - f1_m: 0.9425 - val_loss: 0.1641 - val_f1_m: 0.9814\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 775us/step - loss: 0.2106 - f1_m: 0.9653 - val_loss: 0.1251 - val_f1_m: 0.9873\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 759us/step - loss: 0.1723 - f1_m: 0.9750 - val_loss: 0.0993 - val_f1_m: 0.9971\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 768us/step - loss: 0.1460 - f1_m: 0.9843 - val_loss: 0.0915 - val_f1_m: 0.9951\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 773us/step - loss: 0.1270 - f1_m: 0.9880 - val_loss: 0.0805 - val_f1_m: 0.9961\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 763us/step - loss: 0.1128 - f1_m: 0.9868 - val_loss: 0.0847 - val_f1_m: 0.9854\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 770us/step - loss: 0.1017 - f1_m: 0.9890 - val_loss: 0.0679 - val_f1_m: 0.9941\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 791us/step - loss: 0.0930 - f1_m: 0.9887 - val_loss: 0.0653 - val_f1_m: 0.9922\n",
      "0.9887499809265137\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.5483 - f1_m: 0.7200 - val_loss: 0.2856 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 791us/step - loss: 0.3598 - f1_m: 0.8767 - val_loss: 0.1950 - val_f1_m: 0.9951\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 781us/step - loss: 0.2594 - f1_m: 0.9500 - val_loss: 0.1483 - val_f1_m: 0.9922\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 776us/step - loss: 0.2014 - f1_m: 0.9685 - val_loss: 0.1266 - val_f1_m: 0.9863\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 782us/step - loss: 0.1644 - f1_m: 0.9778 - val_loss: 0.1050 - val_f1_m: 0.9902\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 780us/step - loss: 0.1392 - f1_m: 0.9852 - val_loss: 0.0916 - val_f1_m: 0.9941\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 770us/step - loss: 0.1210 - f1_m: 0.9893 - val_loss: 0.0815 - val_f1_m: 0.9902\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 771us/step - loss: 0.1075 - f1_m: 0.9883 - val_loss: 0.0676 - val_f1_m: 0.9932\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 775us/step - loss: 0.0969 - f1_m: 0.9893 - val_loss: 0.0581 - val_f1_m: 0.9971\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 784us/step - loss: 0.0884 - f1_m: 0.9890 - val_loss: 0.0528 - val_f1_m: 0.9971\n",
      "0.9890000224113464\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5547 - f1_m: 0.7222 - val_loss: 0.3068 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 784us/step - loss: 0.3637 - f1_m: 0.8730 - val_loss: 0.2177 - val_f1_m: 0.9805\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - 0s 791us/step - loss: 0.2628 - f1_m: 0.9523 - val_loss: 0.1742 - val_f1_m: 0.9658\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 787us/step - loss: 0.2041 - f1_m: 0.9697 - val_loss: 0.1316 - val_f1_m: 0.9824\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 768us/step - loss: 0.1667 - f1_m: 0.9770 - val_loss: 0.1098 - val_f1_m: 0.9863\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 790us/step - loss: 0.1415 - f1_m: 0.9837 - val_loss: 0.0979 - val_f1_m: 0.9854\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 771us/step - loss: 0.1230 - f1_m: 0.9865 - val_loss: 0.0887 - val_f1_m: 0.9844\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 781us/step - loss: 0.1095 - f1_m: 0.9885 - val_loss: 0.0721 - val_f1_m: 0.9932\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 793us/step - loss: 0.0987 - f1_m: 0.9902 - val_loss: 0.0710 - val_f1_m: 0.9893\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 777us/step - loss: 0.0901 - f1_m: 0.9883 - val_loss: 0.0681 - val_f1_m: 0.9863\n",
      "0.9882500171661377\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5514 - f1_m: 0.7255 - val_loss: 0.2911 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 790us/step - loss: 0.3643 - f1_m: 0.8815 - val_loss: 0.1802 - val_f1_m: 0.9971\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 795us/step - loss: 0.2650 - f1_m: 0.9435 - val_loss: 0.1430 - val_f1_m: 0.9863\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 760us/step - loss: 0.2067 - f1_m: 0.9607 - val_loss: 0.1245 - val_f1_m: 0.9805\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 781us/step - loss: 0.1693 - f1_m: 0.9762 - val_loss: 0.0968 - val_f1_m: 0.9883\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 779us/step - loss: 0.1435 - f1_m: 0.9868 - val_loss: 0.0796 - val_f1_m: 0.9961\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 781us/step - loss: 0.1252 - f1_m: 0.9883 - val_loss: 0.0789 - val_f1_m: 0.9883\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 773us/step - loss: 0.1113 - f1_m: 0.9912 - val_loss: 0.0594 - val_f1_m: 0.9980\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 773us/step - loss: 0.1006 - f1_m: 0.9918 - val_loss: 0.0625 - val_f1_m: 0.9893\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 749us/step - loss: 0.0919 - f1_m: 0.9910 - val_loss: 0.0596 - val_f1_m: 0.9883\n",
      "0.9909999966621399\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5516 - f1_m: 0.7180 - val_loss: 0.3066 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 791us/step - loss: 0.3651 - f1_m: 0.8667 - val_loss: 0.2138 - val_f1_m: 0.9922\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 792us/step - loss: 0.2655 - f1_m: 0.9442 - val_loss: 0.1685 - val_f1_m: 0.9775\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 790us/step - loss: 0.2070 - f1_m: 0.9657 - val_loss: 0.1294 - val_f1_m: 0.9863\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 796us/step - loss: 0.1698 - f1_m: 0.9750 - val_loss: 0.1029 - val_f1_m: 0.9961\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 781us/step - loss: 0.1448 - f1_m: 0.9818 - val_loss: 0.0947 - val_f1_m: 0.9912\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 795us/step - loss: 0.1263 - f1_m: 0.9865 - val_loss: 0.0940 - val_f1_m: 0.9805\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 781us/step - loss: 0.1127 - f1_m: 0.9880 - val_loss: 0.0787 - val_f1_m: 0.9922\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 776us/step - loss: 0.1024 - f1_m: 0.9880 - val_loss: 0.0724 - val_f1_m: 0.9883\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 790us/step - loss: 0.0936 - f1_m: 0.9887 - val_loss: 0.0719 - val_f1_m: 0.9834\n",
      "0.9887499809265137\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.5509 - f1_m: 0.7185 - val_loss: 0.2843 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 807us/step - loss: 0.3633 - f1_m: 0.8747 - val_loss: 0.2065 - val_f1_m: 0.9824\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 785us/step - loss: 0.2634 - f1_m: 0.9465 - val_loss: 0.1731 - val_f1_m: 0.9648\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 769us/step - loss: 0.2045 - f1_m: 0.9675 - val_loss: 0.1152 - val_f1_m: 0.9902\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 776us/step - loss: 0.1673 - f1_m: 0.9750 - val_loss: 0.1129 - val_f1_m: 0.9785\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 762us/step - loss: 0.1415 - f1_m: 0.9833 - val_loss: 0.0876 - val_f1_m: 0.9893\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 772us/step - loss: 0.1229 - f1_m: 0.9883 - val_loss: 0.0813 - val_f1_m: 0.9873\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 787us/step - loss: 0.1090 - f1_m: 0.9893 - val_loss: 0.0775 - val_f1_m: 0.9805\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 779us/step - loss: 0.0979 - f1_m: 0.9915 - val_loss: 0.0577 - val_f1_m: 0.9941\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 779us/step - loss: 0.0897 - f1_m: 0.9908 - val_loss: 0.0591 - val_f1_m: 0.9893\n",
      "0.9907500147819519\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5499 - f1_m: 0.7217 - val_loss: 0.2932 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 797us/step - loss: 0.3626 - f1_m: 0.8782 - val_loss: 0.1880 - val_f1_m: 0.9902\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 800us/step - loss: 0.2649 - f1_m: 0.9485 - val_loss: 0.1446 - val_f1_m: 0.9834\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 786us/step - loss: 0.2077 - f1_m: 0.9667 - val_loss: 0.1118 - val_f1_m: 0.9883\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 785us/step - loss: 0.1716 - f1_m: 0.9780 - val_loss: 0.0911 - val_f1_m: 0.9902\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 792us/step - loss: 0.1463 - f1_m: 0.9815 - val_loss: 0.0788 - val_f1_m: 0.9912\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 770us/step - loss: 0.1284 - f1_m: 0.9875 - val_loss: 0.0738 - val_f1_m: 0.9912\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 786us/step - loss: 0.1146 - f1_m: 0.9890 - val_loss: 0.0650 - val_f1_m: 0.9941\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 783us/step - loss: 0.1039 - f1_m: 0.9890 - val_loss: 0.0552 - val_f1_m: 1.0000\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 774us/step - loss: 0.0953 - f1_m: 0.9885 - val_loss: 0.0562 - val_f1_m: 0.9941\n",
      "0.9884999990463257\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.5484 - f1_m: 0.7200 - val_loss: 0.2864 - val_f1_m: 1.0000\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 787us/step - loss: 0.3591 - f1_m: 0.8750 - val_loss: 0.2084 - val_f1_m: 0.9873\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 778us/step - loss: 0.2589 - f1_m: 0.9532 - val_loss: 0.1633 - val_f1_m: 0.9746\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 760us/step - loss: 0.2015 - f1_m: 0.9712 - val_loss: 0.1276 - val_f1_m: 0.9854\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 777us/step - loss: 0.1647 - f1_m: 0.9783 - val_loss: 0.1222 - val_f1_m: 0.9688\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 775us/step - loss: 0.1402 - f1_m: 0.9843 - val_loss: 0.0930 - val_f1_m: 0.9893\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 782us/step - loss: 0.1224 - f1_m: 0.9887 - val_loss: 0.0776 - val_f1_m: 0.9922\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 788us/step - loss: 0.1087 - f1_m: 0.9890 - val_loss: 0.0762 - val_f1_m: 0.9854\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 784us/step - loss: 0.0986 - f1_m: 0.9898 - val_loss: 0.0631 - val_f1_m: 0.9971\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 767us/step - loss: 0.0901 - f1_m: 0.9912 - val_loss: 0.0544 - val_f1_m: 0.9980\n",
      "0.9912499785423279\n",
      "if any\n",
      "[100]\n",
      "[0]\n",
      "[array([[ 6.98341846e-01, -4.66521472e-01, -5.14196098e-01,\n",
      "         6.21552050e-01,  7.97312649e-04, -4.71439749e-01,\n",
      "        -4.00530249e-01, -5.85566100e-04,  6.51782870e-01,\n",
      "        -1.77507289e-02],\n",
      "       [-6.41932189e-01,  6.53515697e-01,  5.90939999e-01,\n",
      "        -7.15887427e-01,  1.98890604e-02,  6.38153911e-01,\n",
      "         5.86047292e-01,  7.06447521e-03, -6.33156836e-01,\n",
      "        -4.57099378e-02],\n",
      "       [ 1.82334721e-01,  1.68148547e-01,  6.10074364e-02,\n",
      "         1.72277942e-01, -4.26761732e-02,  1.18287235e-01,\n",
      "         1.46443784e-01, -3.66659686e-02,  1.22029476e-01,\n",
      "        -7.23978132e-02],\n",
      "       [ 1.68412417e-01,  1.80534437e-01,  1.30482867e-01,\n",
      "         2.24941373e-01, -5.37313893e-02,  6.61961213e-02,\n",
      "         6.67310134e-02, -8.71258974e-02,  1.94168791e-01,\n",
      "        -2.14454625e-02]], dtype=float32), array([ 0.23269072,  0.09688407,  0.16717963,  0.22844736, -0.0172952 ,\n",
      "        0.12888665,  0.10088346, -0.00952765,  0.22901164, -0.0209471 ],\n",
      "      dtype=float32), array([[ 0.5880805 , -1.4707072 ],\n",
      "       [-0.72320056,  0.96814716],\n",
      "       [-1.353991  ,  0.97655135],\n",
      "       [ 1.4385102 , -0.8222261 ],\n",
      "       [ 0.6791186 , -0.01383706],\n",
      "       [-0.6228315 ,  1.4622295 ],\n",
      "       [-1.3032447 ,  1.2333314 ],\n",
      "       [ 0.45420307,  0.31242555],\n",
      "       [ 1.2141849 , -1.1939576 ],\n",
      "       [ 0.35483494,  0.3016877 ]], dtype=float32), array([-0.07528379,  0.07528383], dtype=float32)]\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "yhn tk\n",
      "100\n",
      "Done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "   1/4375 [..............................] - ETA: 6:52 - loss: 0.3951 - accuracy: 0.9062"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ayushkv/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:123: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "/Users/ayushkv/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:124: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "/Users/ayushkv/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:130: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4375/4375 [==============================] - 2s 503us/step - loss: 0.2743 - accuracy: 0.9467\n",
      "1875/1875 [==============================] - 1s 508us/step - loss: 0.2768 - accuracy: 0.9464\n",
      "313/313 [==============================] - 0s 405us/step\n",
      "10000\n",
      "drift has been detecte models must be retrained\n",
      "finished\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 1s 1ms/step - loss: 0.6412 - f1_m: 0.7405 - val_loss: 0.6327 - val_f1_m: 0.7324\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 800us/step - loss: 0.4983 - f1_m: 0.9132 - val_loss: 0.4933 - val_f1_m: 0.8213\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 785us/step - loss: 0.3636 - f1_m: 0.9503 - val_loss: 0.3877 - val_f1_m: 0.8574\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 775us/step - loss: 0.2769 - f1_m: 0.9638 - val_loss: 0.2765 - val_f1_m: 0.9355\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 751us/step - loss: 0.2235 - f1_m: 0.9703 - val_loss: 0.2429 - val_f1_m: 0.9316\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 775us/step - loss: 0.1889 - f1_m: 0.9762 - val_loss: 0.2287 - val_f1_m: 0.9238\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 781us/step - loss: 0.1647 - f1_m: 0.9783 - val_loss: 0.1855 - val_f1_m: 0.9463\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 741us/step - loss: 0.1472 - f1_m: 0.9770 - val_loss: 0.1440 - val_f1_m: 0.9766\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 772us/step - loss: 0.1341 - f1_m: 0.9800 - val_loss: 0.1630 - val_f1_m: 0.9482\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 760us/step - loss: 0.1232 - f1_m: 0.9800 - val_loss: 0.1491 - val_f1_m: 0.9531\n",
      "0.9800000190734863\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.6449 - f1_m: 0.7562 - val_loss: 0.6946 - val_f1_m: 0.4561\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 788us/step - loss: 0.5236 - f1_m: 0.8667 - val_loss: 0.5708 - val_f1_m: 0.6885\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 794us/step - loss: 0.3929 - f1_m: 0.9302 - val_loss: 0.3653 - val_f1_m: 0.9170\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 807us/step - loss: 0.2988 - f1_m: 0.9612 - val_loss: 0.2928 - val_f1_m: 0.9248\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 794us/step - loss: 0.2399 - f1_m: 0.9705 - val_loss: 0.2463 - val_f1_m: 0.9346\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 788us/step - loss: 0.2014 - f1_m: 0.9715 - val_loss: 0.1982 - val_f1_m: 0.9619\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 770us/step - loss: 0.1755 - f1_m: 0.9775 - val_loss: 0.2087 - val_f1_m: 0.9248\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 772us/step - loss: 0.1562 - f1_m: 0.9795 - val_loss: 0.1664 - val_f1_m: 0.9590\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 772us/step - loss: 0.1415 - f1_m: 0.9822 - val_loss: 0.1303 - val_f1_m: 0.9844\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 784us/step - loss: 0.1297 - f1_m: 0.9827 - val_loss: 0.1458 - val_f1_m: 0.9590\n",
      "0.9827499985694885\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.6439 - f1_m: 0.7550 - val_loss: 0.6691 - val_f1_m: 0.5449\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 784us/step - loss: 0.5199 - f1_m: 0.8708 - val_loss: 0.5175 - val_f1_m: 0.8301\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 784us/step - loss: 0.3920 - f1_m: 0.9345 - val_loss: 0.4077 - val_f1_m: 0.8770\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 771us/step - loss: 0.2998 - f1_m: 0.9520 - val_loss: 0.3030 - val_f1_m: 0.9326\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 773us/step - loss: 0.2413 - f1_m: 0.9660 - val_loss: 0.2656 - val_f1_m: 0.9258\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 770us/step - loss: 0.2029 - f1_m: 0.9693 - val_loss: 0.2324 - val_f1_m: 0.9326\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 766us/step - loss: 0.1763 - f1_m: 0.9750 - val_loss: 0.1790 - val_f1_m: 0.9775\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 813us/step - loss: 0.1576 - f1_m: 0.9762 - val_loss: 0.1975 - val_f1_m: 0.9336\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 793us/step - loss: 0.1428 - f1_m: 0.9768 - val_loss: 0.1644 - val_f1_m: 0.9658\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 793us/step - loss: 0.1315 - f1_m: 0.9810 - val_loss: 0.1361 - val_f1_m: 0.9814\n",
      "0.9810000061988831\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.6436 - f1_m: 0.7320 - val_loss: 0.6623 - val_f1_m: 0.6064\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 778us/step - loss: 0.5166 - f1_m: 0.8802 - val_loss: 0.5193 - val_f1_m: 0.8076\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 781us/step - loss: 0.3824 - f1_m: 0.9383 - val_loss: 0.3549 - val_f1_m: 0.9326\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 784us/step - loss: 0.2893 - f1_m: 0.9635 - val_loss: 0.2891 - val_f1_m: 0.9326\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 758us/step - loss: 0.2308 - f1_m: 0.9682 - val_loss: 0.2152 - val_f1_m: 0.9639\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 781us/step - loss: 0.1931 - f1_m: 0.9753 - val_loss: 0.1972 - val_f1_m: 0.9541\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 780us/step - loss: 0.1670 - f1_m: 0.9790 - val_loss: 0.1984 - val_f1_m: 0.9336\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 790us/step - loss: 0.1483 - f1_m: 0.9778 - val_loss: 0.1674 - val_f1_m: 0.9512\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 761us/step - loss: 0.1335 - f1_m: 0.9835 - val_loss: 0.1708 - val_f1_m: 0.9365\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 766us/step - loss: 0.1221 - f1_m: 0.9808 - val_loss: 0.1266 - val_f1_m: 0.9707\n",
      "0.9807500243186951\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.6419 - f1_m: 0.7182 - val_loss: 0.6603 - val_f1_m: 0.5850\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 790us/step - loss: 0.5157 - f1_m: 0.8812 - val_loss: 0.4716 - val_f1_m: 0.9277\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 778us/step - loss: 0.3872 - f1_m: 0.9477 - val_loss: 0.3911 - val_f1_m: 0.9033\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 766us/step - loss: 0.2945 - f1_m: 0.9597 - val_loss: 0.2830 - val_f1_m: 0.9639\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 771us/step - loss: 0.2359 - f1_m: 0.9715 - val_loss: 0.2657 - val_f1_m: 0.9297\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 768us/step - loss: 0.1977 - f1_m: 0.9722 - val_loss: 0.2095 - val_f1_m: 0.9639\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 762us/step - loss: 0.1713 - f1_m: 0.9753 - val_loss: 0.1974 - val_f1_m: 0.9561\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 767us/step - loss: 0.1527 - f1_m: 0.9768 - val_loss: 0.1864 - val_f1_m: 0.9512\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 777us/step - loss: 0.1376 - f1_m: 0.9783 - val_loss: 0.1727 - val_f1_m: 0.9521\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 765us/step - loss: 0.1263 - f1_m: 0.9795 - val_loss: 0.1512 - val_f1_m: 0.9609\n",
      "0.9794999957084656\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.6454 - f1_m: 0.7345 - val_loss: 0.6690 - val_f1_m: 0.5742\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 781us/step - loss: 0.5253 - f1_m: 0.8732 - val_loss: 0.5164 - val_f1_m: 0.8291\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 792us/step - loss: 0.3970 - f1_m: 0.9400 - val_loss: 0.3975 - val_f1_m: 0.8896\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 773us/step - loss: 0.3022 - f1_m: 0.9607 - val_loss: 0.3463 - val_f1_m: 0.8770\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 788us/step - loss: 0.2422 - f1_m: 0.9630 - val_loss: 0.2468 - val_f1_m: 0.9629\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 789us/step - loss: 0.2030 - f1_m: 0.9697 - val_loss: 0.2126 - val_f1_m: 0.9639\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 790us/step - loss: 0.1762 - f1_m: 0.9728 - val_loss: 0.1829 - val_f1_m: 0.9678\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 768us/step - loss: 0.1571 - f1_m: 0.9772 - val_loss: 0.1668 - val_f1_m: 0.9678\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 790us/step - loss: 0.1425 - f1_m: 0.9775 - val_loss: 0.1531 - val_f1_m: 0.9678\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 791us/step - loss: 0.1311 - f1_m: 0.9783 - val_loss: 0.1568 - val_f1_m: 0.9639\n",
      "0.9782500267028809\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.6454 - f1_m: 0.7302 - val_loss: 0.6691 - val_f1_m: 0.5713\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 781us/step - loss: 0.5187 - f1_m: 0.8780 - val_loss: 0.4854 - val_f1_m: 0.8740\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 789us/step - loss: 0.3856 - f1_m: 0.9415 - val_loss: 0.3827 - val_f1_m: 0.8799\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 780us/step - loss: 0.2924 - f1_m: 0.9590 - val_loss: 0.2815 - val_f1_m: 0.9492\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 785us/step - loss: 0.2335 - f1_m: 0.9678 - val_loss: 0.2319 - val_f1_m: 0.9570\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 774us/step - loss: 0.1955 - f1_m: 0.9715 - val_loss: 0.2019 - val_f1_m: 0.9580\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 778us/step - loss: 0.1699 - f1_m: 0.9770 - val_loss: 0.1815 - val_f1_m: 0.9590\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 771us/step - loss: 0.1506 - f1_m: 0.9787 - val_loss: 0.1685 - val_f1_m: 0.9541\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 781us/step - loss: 0.1364 - f1_m: 0.9770 - val_loss: 0.1468 - val_f1_m: 0.9707\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 790us/step - loss: 0.1255 - f1_m: 0.9818 - val_loss: 0.1297 - val_f1_m: 0.9746\n",
      "0.9817500114440918\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.6448 - f1_m: 0.7330 - val_loss: 0.6642 - val_f1_m: 0.5859\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 779us/step - loss: 0.5117 - f1_m: 0.8797 - val_loss: 0.5380 - val_f1_m: 0.7646\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 788us/step - loss: 0.3754 - f1_m: 0.9362 - val_loss: 0.3602 - val_f1_m: 0.9297\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 770us/step - loss: 0.2824 - f1_m: 0.9660 - val_loss: 0.3149 - val_f1_m: 0.9004\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 772us/step - loss: 0.2252 - f1_m: 0.9703 - val_loss: 0.2205 - val_f1_m: 0.9658\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 770us/step - loss: 0.1885 - f1_m: 0.9745 - val_loss: 0.2031 - val_f1_m: 0.9561\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 779us/step - loss: 0.1635 - f1_m: 0.9803 - val_loss: 0.1992 - val_f1_m: 0.9346\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 778us/step - loss: 0.1455 - f1_m: 0.9795 - val_loss: 0.1871 - val_f1_m: 0.9316\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 769us/step - loss: 0.1315 - f1_m: 0.9805 - val_loss: 0.1613 - val_f1_m: 0.9512\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 753us/step - loss: 0.1204 - f1_m: 0.9827 - val_loss: 0.1406 - val_f1_m: 0.9609\n",
      "0.9827499985694885\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.6478 - f1_m: 0.7462 - val_loss: 0.6876 - val_f1_m: 0.4990\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 788us/step - loss: 0.5223 - f1_m: 0.8705 - val_loss: 0.5112 - val_f1_m: 0.8281\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 758us/step - loss: 0.3872 - f1_m: 0.9495 - val_loss: 0.4039 - val_f1_m: 0.8555\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 829us/step - loss: 0.2926 - f1_m: 0.9640 - val_loss: 0.2826 - val_f1_m: 0.9375\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 776us/step - loss: 0.2335 - f1_m: 0.9728 - val_loss: 0.2372 - val_f1_m: 0.9463\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 774us/step - loss: 0.1955 - f1_m: 0.9735 - val_loss: 0.1999 - val_f1_m: 0.9561\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 791us/step - loss: 0.1689 - f1_m: 0.9780 - val_loss: 0.1862 - val_f1_m: 0.9492\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 766us/step - loss: 0.1501 - f1_m: 0.9793 - val_loss: 0.1436 - val_f1_m: 0.9736\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 784us/step - loss: 0.1358 - f1_m: 0.9800 - val_loss: 0.1326 - val_f1_m: 0.9727\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 769us/step - loss: 0.1241 - f1_m: 0.9833 - val_loss: 0.1397 - val_f1_m: 0.9600\n",
      "0.9832500219345093\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.6421 - f1_m: 0.7410 - val_loss: 0.6293 - val_f1_m: 0.7529\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 806us/step - loss: 0.4999 - f1_m: 0.9150 - val_loss: 0.4759 - val_f1_m: 0.8594\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 802us/step - loss: 0.3622 - f1_m: 0.9507 - val_loss: 0.3384 - val_f1_m: 0.9238\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 776us/step - loss: 0.2728 - f1_m: 0.9665 - val_loss: 0.2625 - val_f1_m: 0.9453\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 782us/step - loss: 0.2191 - f1_m: 0.9732 - val_loss: 0.2366 - val_f1_m: 0.9238\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 767us/step - loss: 0.1847 - f1_m: 0.9760 - val_loss: 0.1934 - val_f1_m: 0.9551\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 754us/step - loss: 0.1611 - f1_m: 0.9765 - val_loss: 0.1602 - val_f1_m: 0.9717\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 770us/step - loss: 0.1438 - f1_m: 0.9795 - val_loss: 0.1570 - val_f1_m: 0.9629\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 765us/step - loss: 0.1302 - f1_m: 0.9803 - val_loss: 0.1247 - val_f1_m: 0.9824\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 773us/step - loss: 0.1202 - f1_m: 0.9800 - val_loss: 0.1242 - val_f1_m: 0.9736\n",
      "0.9800000190734863\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.6473 - f1_m: 0.7227 - val_loss: 0.6446 - val_f1_m: 0.6904\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 788us/step - loss: 0.5246 - f1_m: 0.8735 - val_loss: 0.5081 - val_f1_m: 0.8574\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 800us/step - loss: 0.3923 - f1_m: 0.9362 - val_loss: 0.3781 - val_f1_m: 0.9189\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 778us/step - loss: 0.2977 - f1_m: 0.9555 - val_loss: 0.3225 - val_f1_m: 0.9072\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 794us/step - loss: 0.2377 - f1_m: 0.9668 - val_loss: 0.2824 - val_f1_m: 0.9043\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 791us/step - loss: 0.1989 - f1_m: 0.9672 - val_loss: 0.1979 - val_f1_m: 0.9717\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 774us/step - loss: 0.1723 - f1_m: 0.9743 - val_loss: 0.2052 - val_f1_m: 0.9395\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 796us/step - loss: 0.1529 - f1_m: 0.9760 - val_loss: 0.1714 - val_f1_m: 0.9619\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 831us/step - loss: 0.1386 - f1_m: 0.9803 - val_loss: 0.1884 - val_f1_m: 0.9297\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 813us/step - loss: 0.1273 - f1_m: 0.9793 - val_loss: 0.1326 - val_f1_m: 0.9775\n",
      "0.9792500138282776\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.6408 - f1_m: 0.7400 - val_loss: 0.6870 - val_f1_m: 0.4932\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 780us/step - loss: 0.5102 - f1_m: 0.8750 - val_loss: 0.5147 - val_f1_m: 0.8066\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 786us/step - loss: 0.3784 - f1_m: 0.9492 - val_loss: 0.3795 - val_f1_m: 0.8906\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 772us/step - loss: 0.2869 - f1_m: 0.9625 - val_loss: 0.3051 - val_f1_m: 0.9053\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 778us/step - loss: 0.2293 - f1_m: 0.9707 - val_loss: 0.2687 - val_f1_m: 0.9033\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 1s 6ms/step - loss: 0.1922 - f1_m: 0.9747 - val_loss: 0.2250 - val_f1_m: 0.9238\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - 0s 822us/step - loss: 0.1665 - f1_m: 0.9780 - val_loss: 0.1744 - val_f1_m: 0.9600\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 797us/step - loss: 0.1478 - f1_m: 0.9790 - val_loss: 0.1832 - val_f1_m: 0.9297\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 784us/step - loss: 0.1339 - f1_m: 0.9793 - val_loss: 0.1693 - val_f1_m: 0.9336\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 794us/step - loss: 0.1222 - f1_m: 0.9808 - val_loss: 0.1285 - val_f1_m: 0.9736\n",
      "0.9807500243186951\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.6432 - f1_m: 0.7442 - val_loss: 0.6511 - val_f1_m: 0.6396\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 803us/step - loss: 0.5151 - f1_m: 0.8840 - val_loss: 0.4903 - val_f1_m: 0.8633\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 772us/step - loss: 0.3849 - f1_m: 0.9427 - val_loss: 0.3893 - val_f1_m: 0.8779\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 771us/step - loss: 0.2925 - f1_m: 0.9647 - val_loss: 0.3308 - val_f1_m: 0.8750\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 774us/step - loss: 0.2340 - f1_m: 0.9710 - val_loss: 0.2967 - val_f1_m: 0.8711\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 781us/step - loss: 0.1964 - f1_m: 0.9735 - val_loss: 0.2172 - val_f1_m: 0.9473\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 775us/step - loss: 0.1703 - f1_m: 0.9743 - val_loss: 0.1978 - val_f1_m: 0.9482\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 779us/step - loss: 0.1511 - f1_m: 0.9783 - val_loss: 0.1651 - val_f1_m: 0.9697\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 776us/step - loss: 0.1372 - f1_m: 0.9760 - val_loss: 0.1626 - val_f1_m: 0.9619\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 781us/step - loss: 0.1255 - f1_m: 0.9818 - val_loss: 0.1634 - val_f1_m: 0.9492\n",
      "0.9817500114440918\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.6432 - f1_m: 0.7327 - val_loss: 0.6600 - val_f1_m: 0.6094\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 793us/step - loss: 0.5161 - f1_m: 0.8697 - val_loss: 0.5088 - val_f1_m: 0.8252\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 822us/step - loss: 0.3849 - f1_m: 0.9425 - val_loss: 0.3821 - val_f1_m: 0.8828\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 761us/step - loss: 0.2927 - f1_m: 0.9600 - val_loss: 0.2994 - val_f1_m: 0.9082\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 773us/step - loss: 0.2342 - f1_m: 0.9725 - val_loss: 0.2590 - val_f1_m: 0.9082\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 777us/step - loss: 0.1960 - f1_m: 0.9720 - val_loss: 0.2206 - val_f1_m: 0.9258\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 785us/step - loss: 0.1695 - f1_m: 0.9780 - val_loss: 0.2285 - val_f1_m: 0.8926\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 775us/step - loss: 0.1501 - f1_m: 0.9790 - val_loss: 0.1692 - val_f1_m: 0.9512\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 775us/step - loss: 0.1357 - f1_m: 0.9787 - val_loss: 0.1600 - val_f1_m: 0.9473\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 772us/step - loss: 0.1239 - f1_m: 0.9820 - val_loss: 0.1283 - val_f1_m: 0.9746\n",
      "0.9819999933242798\n",
      "if any\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 1ms/step - loss: 0.6420 - f1_m: 0.7540 - val_loss: 0.6197 - val_f1_m: 0.7793\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 774us/step - loss: 0.4983 - f1_m: 0.9143 - val_loss: 0.4971 - val_f1_m: 0.8271\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 800us/step - loss: 0.3613 - f1_m: 0.9442 - val_loss: 0.3513 - val_f1_m: 0.9170\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 784us/step - loss: 0.2733 - f1_m: 0.9592 - val_loss: 0.2885 - val_f1_m: 0.9199\n",
      "Epoch 5/10\n",
      "  1/125 [..............................] - ETA: 0s - loss: 0.3430 - f1_m: 0.9062"
     ]
    }
   ],
   "source": [
    "#right now trying for mean models\n",
    "stream_mean_results=[]\n",
    "current_window = data_init.copy()\n",
    "delta=0.01\n",
    "adwin = ADWIN(delta)\n",
    "detected = False\n",
    "retraining_count=0\n",
    "data_window=data_init.copy()\n",
    "target_variable = 'y'\n",
    "num_models=[]\n",
    "Models_len=[]\n",
    "stream.restart()\n",
    "y_pred_total = []\n",
    "y_pred_uncertainty_total = []\n",
    "true_values = []\n",
    "while stream.n_remaining_samples()>1:\n",
    "    incoming_data = stream.next_sample(five_percent)\n",
    "    #rint(incoming_data)\n",
    "    data_incoming = pd.DataFrame(incoming_data[0], columns=data.columns[:-1])\n",
    "    data_incoming[target_variable]=incoming_data[1]\n",
    "    true_values.append(data_incoming[target_variable])\n",
    "    #print(data_incoming)\n",
    "    y_pred, y_pred_uncertainty = drift_detection(mean_model_weights, data_incoming.copy()) \n",
    "    #detected, evaluate, adwin = drift_detection_each_model(mean_model_weights, data_incoming.copy(), adwin)\n",
    "    y_pred_total.append(y_pred)\n",
    "    y_pred_uncertainty_total.append(y_pred_uncertainty)\n",
    "    print(len(y_pred_uncertainty))\n",
    "    for i in range(len(y_pred_uncertainty)):\n",
    "        adwin.add_element(y_pred_uncertainty[i])\n",
    "        if adwin.detected_change():\n",
    "            detected = True\n",
    "            break;\n",
    "    if detected:\n",
    "        print(\"drift has been detecte models must be retrained\")\n",
    "        data_window = update_train_data(data_window, data_incoming)\n",
    "        \n",
    "        retraining_count+=1\n",
    "        N = int(data_window.shape[0]*0.25)\n",
    "\n",
    "        samples = generate_samples(data_window, 100, N)\n",
    "        \"\"\"\n",
    "        df1=Positive.sample(positiveN)\n",
    "        Positive.drop(df1.index, inplace=True)\n",
    "        df2=Negative.sample(negativeN)\n",
    "        Negative.drop(df2.index, inplace=True)\n",
    "        test_data=df1.append(df2, ignore_index=True)\n",
    "        test_data=test_data.sample(frac = 1) #This is to shuffel the training and testing data\n",
    "        test_data=test_data.sample(frac = 1)\n",
    "        test_data=test_data.sample(frac = 1)\n",
    "        X_test=test_data.drop(columns=[target_variable])\n",
    "        y_test=to_categorical(test_data[target_variable])\n",
    "        \"\"\"\n",
    "\n",
    "        # adding dense layer\n",
    "        initial_model= get_initial_model(data.shape[1]-1, 2)\n",
    "        Models=[]\n",
    "        val_acc=[]\n",
    "        train_acc=[]\n",
    "        test_acc=[]\n",
    "        val_loss=[]\n",
    "        train_loss=[]\n",
    "        ind=0\n",
    "        add_weights=[]\n",
    "        while ind<len(samples):\n",
    "          \"\"\"\"\n",
    "          print(positiveN, negativeN)\n",
    "          df1=Positive.sample(min(positiveN, len(Positive)))\n",
    "          Positive.drop(df1.index, inplace=True)\n",
    "          df2=Negative.sample(min(negativeN, len(Negative)))\n",
    "          Negative.drop(df2.index, inplace=True)\n",
    "          train_data=df1.append(df2, ignore_index=True)\n",
    "          train_data=train_data.sample(frac = 1) #shuffel test data 3 times\n",
    "          train_data=train_data.sample(frac = 1) #shuffel test data 3 times\n",
    "          train_data=train_data.sample(frac = 1) #shuffel test data 3 times\n",
    "\n",
    "          #all models have different initialization\n",
    "          # define the sequential model\n",
    "          \"\"\"\n",
    "          train_data=samples[ind]\n",
    "          ind=ind+1\n",
    "          \"\"\"initial_model = keras.Sequential()\n",
    "\n",
    "            # adding dense layer\n",
    "          initial_model.add(Dense(5, input_dim=X_test.shape[1], kernel_initializer='normal', activation='relu'))\n",
    "          initial_model.add(Dense(10, activation='relu'))\n",
    "          initial_model.add(Dense(5, activation='relu'))\n",
    "\n",
    "            # adding dense layer with softmax activation/output layer\n",
    "          initial_model.add(Dense(2, activation='softmax'))\n",
    "          #initial_model.summary()\"\"\"\n",
    "          ann_model=get_initial_model(data.shape[1]-1, 2) #same intial weights\n",
    "          ann_model.set_weights(initial_model.get_weights())\n",
    "          X_train=train_data.drop(columns=[target_variable])\n",
    "          #train_data[target_variable]=train_data[target_variable]-1 #only for skin_nonskin dataset\n",
    "          y_train=to_categorical(train_data[target_variable])\n",
    "          #print(y_train)\n",
    "          ann_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[f1_m]) # metrics=['accuracy']\n",
    "          history = ann_model.fit(X_train, y_train, epochs=10, validation_split=0.2, verbose=1)\n",
    "          print(history.history['f1_m'][-1])\n",
    "          #ann_model.set_weights(update_weights(ann_model.get_weights()))\n",
    "          #pred_test=ann_model.predict(X_test)\n",
    "          present=False\n",
    "          for i in range(len(Models)):\n",
    "            if (check_models(Models[i][0], ann_model.get_weights())):\n",
    "              print(\"if any\")\n",
    "              Models[i][1]=Models[i][1]+1\n",
    "              add_weights[i].append(ann_model.get_weights())\n",
    "              val_acc[i].append(history.history['val_f1_m'])\n",
    "              train_acc[i].append(history.history['f1_m'])\n",
    "              #test_acc[i].append(f1_m(y_test, pred_test).numpy())\n",
    "              val_loss[i].append(history.history['val_loss'])\n",
    "              train_loss[i].append(history.history['loss'])\n",
    "              present=True\n",
    "              break;\n",
    "          if present==False:\n",
    "            add_weights.append([ann_model.get_weights()])\n",
    "            Models.append([ann_model.get_weights(), 1])\n",
    "            val_acc.append([history.history['val_f1_m']])\n",
    "            train_acc.append([history.history['f1_m']])\n",
    "            #test_acc.append([f1_m(y_test, pred_test).numpy()])\n",
    "            val_loss.append([history.history['val_loss']])\n",
    "            train_loss.append([history.history['loss']])\n",
    "        A=np.argsort(np.array(Models).T[1])[::-1][:5]\n",
    "        print(np.array(Models).T[1])\n",
    "        print(A)\n",
    "        recommended_models=[]\n",
    "        for i in range(len(A)):\n",
    "            recommended_models.append(add_weights[A[i]])\n",
    "        num_models.append(len(A))\n",
    "        Models_len.append(np.array(Models).T[1])\n",
    "        #add_weights=add_weights[A]\n",
    "        # Now trying to generate Streaming settings for the dataset\n",
    "        # lets find the outputs from all the \n",
    "        mean_model_weights, mean_model_acc, mean_model_loss, mean_model_test_acc, mean_model_test_loss = epsilon_mean_recommendation(recommended_models, data)\n",
    "    else:\n",
    "        print(\"One lap done.\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e4ed75",
   "metadata": {},
   "outputs": [],
   "source": [
    "retraining_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0964b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=[]\n",
    "for i in y_pred_total:\n",
    "    predictions.append(list(np.argmax(i,axis=1)))\n",
    "predictions = list(np.concatenate(predictions))\n",
    "y_pred_total[0], len(predictions)\n",
    "true_values = list(np.concatenate(true_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455c398c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#here for computing metrics \n",
    "#retraining count = no. of drift detected\n",
    "#accuracy score, mean_absolute error for regressiom, f1_score, matthews_score \n",
    "from sklearn.metrics import log_loss, mean_squared_error, accuracy_score, mean_absolute_error, f1_score, matthews_corrcoef, mean_squared_error, mean_squared_log_error, roc_auc_score\n",
    "\n",
    "# this is for classification\n",
    "acc_score = accuracy_score(true_values, predictions)\n",
    "print(acc_score)\n",
    "mcc = matthews_corrcoef(true_values, predictions)\n",
    "f1_score = f1_score(true_values, predictions)\n",
    "auc_score = roc_auc_score(true_values, predictions)\n",
    "\n",
    "\n",
    "\n",
    "print(mcc)\n",
    "print(f1_score)\n",
    "print(auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3badc180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 5, 3, 5, 4, 5, 1, 5, 5, 5, 5, 1, 5, 5, 3, 5, 5]\n",
      "[array([100], dtype=object), array([100], dtype=object), array([22, 73, 2, 1, 1, 1], dtype=object), array([89, 8, 3], dtype=object), array([9, 14, 11, 5, 8, 1, 16, 1, 4, 8, 8, 1, 2, 1, 3, 3, 3, 1, 1],\n",
      "      dtype=object), array([79, 18, 2, 1], dtype=object), array([3, 49, 10, 4, 19, 1, 1, 2, 2, 1, 1, 1, 1, 1, 2, 1, 1], dtype=object), array([100], dtype=object), array([45, 20, 15, 17, 1, 1, 1], dtype=object), array([12, 39, 3, 3, 13, 4, 1, 2, 15, 3, 1, 1, 1, 1, 1], dtype=object), array([43, 1, 44, 2, 1, 3, 1, 1, 1, 1, 1, 1], dtype=object), array([45, 30, 1, 7, 1, 6, 2, 4, 1, 3], dtype=object), array([100], dtype=object), array([13, 36, 6, 27, 6, 1, 1, 1, 2, 1, 2, 1, 1, 2], dtype=object), array([65, 15, 10, 1, 2, 2, 2, 1, 1, 1], dtype=object), array([95, 4, 1], dtype=object), array([93, 1, 3, 1, 1, 1], dtype=object), array([30, 24, 5, 2, 19, 16, 1, 1, 1, 1], dtype=object)]\n"
     ]
    }
   ],
   "source": [
    "print(num_models)\n",
    "print(Models_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a973cba4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#here's the fig for k-anonymity with increasing initialization parameters\n",
    "diff_in=[1,2,3,4,5,6,7,8,9,10]\n",
    "k_anonymous=[4,4,4,5,7,6,6,7,7,8]\n",
    "plt.xlabel(\"Number of different initialization\")\n",
    "plt.ylabel(\"k\")\n",
    "plt.plot(diff_in, k_anonymous, label = \"k-anonymous integral privacy\")\n",
    "plt.legend()\n",
    "plt.savefig(\"k-anonymous Integral privacy.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "09c2e980",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#here's the fig for k-anonymity with increasing initialization parameters\n",
    "diff_in=[1,2,3,4,5,6,7,8,9,10]\n",
    "k_anonymity=[59,51,43,29,20,16,16,15,13,11]\n",
    "plt.xlabel(\"Number of different initialization\")\n",
    "plt.ylabel(\"k\")\n",
    "plt.plot(diff_in, k_anonymity, label = \"k-anonymity integral privacy\")\n",
    "plt.legend()\n",
    "plt.savefig(\"k-anonymity Integral privacy.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e29e35d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#now increasing epsilon for k-anonymity\n",
    "epsilon_list=[0.001,0.005,0.008,0.01, 0.05,0.1, 0.25,0.5,0.75]\n",
    "anonymity_list=[2,3,6,27,43,62,82,100,100]\n",
    "plt.xlabel(\"Different epsilon in integral privacy\", size=\"15\")\n",
    "plt.ylabel(\"k\", size=\"15\")\n",
    "plt.plot(epsilon_list,anonymity_list, label=\"k-anonymity against each epsilon\")\n",
    "plt.legend()\n",
    "plt.savefig(\"k-anonymity against each epsilon.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "852c80db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#here's the image for increasing m for k-anonymous IP\n",
    "num_samples=[10,50,100,200,500,750,1000]\n",
    "k_anon=[2,4,6,8,11,12,14]\n",
    "plt.xlabel(\"Number of samples\")\n",
    "plt.ylabel(\"k\")\n",
    "plt.plot(num_samples,k_anon, label=\"k-anonymous integral privacy\")\n",
    "plt.legend()\n",
    "plt.savefig(\"k-anonymous IP against number of samples.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b1602e6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#here's the image for increasing m for k-anonymity IP\n",
    "num_samples=[10,50,100,200,500,750,1000]\n",
    "k_anon=[4,8,19,38,46,59,73]\n",
    "plt.xlabel(\"Number of samples\")\n",
    "plt.ylabel(\"k\")\n",
    "plt.plot(num_samples,k_anon, label=\"k-anonymity integral privacy\")\n",
    "plt.legend()\n",
    "plt.savefig(\"k-anonymity IP against number of samples.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2545a94c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement pickel (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for pickel\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1af250d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0, 0.0, 0.001762114537444934, 0.001762114537444934, 0.001762114537444934, 0.001762114537444934, 0.001762114537444934, 0.001762114537444934, 0.001762114537444934, 0.001762114537444934, 0.001762114537444934, 0.001762114537444934, 0.001762114537444934, 0.001762114537444934, 0.001762114537444934, 0.001762114537444934, 0.001762114537444934, 0.001762114537444934, 0.001762114537444934, 0.001762114537444934, 0.001762114537444934, 0.001762114537444934, 0.001762114537444934, 0.000881057268722467, 0.000881057268722467, 0.000881057268722467, 0.000881057268722467, 0.000881057268722467]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "file = open('posionacc_IP_noniid_0.005_0.01_29_1.p', 'rb')\n",
    "A=pickle.load(file)\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182c6ecd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
